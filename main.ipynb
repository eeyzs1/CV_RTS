{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c297172",
   "metadata": {},
   "source": [
    "Multi-level Perturbed Unit Gradient Descent, MPUGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data  import Subset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from types import SimpleNamespace \n",
    "import matplotlib.pyplot  as plt\n",
    "import numpy as np \n",
    "\n",
    "from optimizers import *\n",
    "from upanets import UPANets\n",
    "from torchsummary import summary\n",
    "import time, copy,timm\n",
    "import json\n",
    "import random \n",
    "import os\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa60ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args = SimpleNamespace(\n",
    "    datasets='cifar_10',\n",
    "    batch_size = 500,\n",
    "    seed = 42,\n",
    "    lr=0.1, \n",
    "    momentum=0.9,\n",
    "    wd = 0.0005,\n",
    "    blocks = 1,\n",
    "    filters = 16,\n",
    "    epochs = 400,\n",
    "    start_epochs = 8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdda934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed=42):\n",
    "    # Python原生随机 \n",
    "    random.seed(seed) \n",
    "    # NumPy随机 \n",
    "    np.random.seed(seed) \n",
    "    # PyTorch随机 \n",
    "    torch.manual_seed(seed) \n",
    "    # CUDA随机（GPU相关）\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    # CUDNN确定性模式 \n",
    "    torch.backends.cudnn.deterministic  = True \n",
    "    torch.backends.cudnn.benchmark  = False \n",
    " \n",
    "set_all_seeds(args.seed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e50ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 32 # default image size for Cifar-10\n",
    "im_dimention = 32\n",
    "cifar_10_mean = [0.4914, 0.4822, 0.4465] \n",
    "cifar_10_std = [0.2023, 0.1994, 0.2010]\n",
    "cifar_100_mean = [0.5071, 0.4867, 0.4408]\n",
    "cifar_100_std = [0.2673, 0.2564, 0.2762]\n",
    "\n",
    "if args.datasets == 'cifar_10':\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((im_dimention,im_dimention)),\n",
    "            transforms.RandomRotation(15,),\n",
    "            transforms.RandomCrop(im_dimention),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cifar_10_mean, std=cifar_10_std)\n",
    "            # transforms.Lambda(lambda x: x.to(torch.float16))    # 最终输出FP16\n",
    "        ]),\n",
    "        # 'valid': transforms.Compose([\n",
    "        #     transforms.Resize((im_dimention,im_dimention)),\n",
    "        #     transforms.ToTensor(),\n",
    "        #     transforms.Normalize(mean=cifar_10_mean, std=cifar_10_std)\n",
    "        # ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((im_dimention,im_dimention)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cifar_10_mean, std=cifar_10_std)\n",
    "        ]),\n",
    "    }\n",
    " \n",
    "    full_trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data/cifar_10', train=True, download=True, transform=data_transforms['train'])\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data/cifar_10', train=False, download=True, transform=data_transforms['test'])\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "    Num_class = 10\n",
    "\n",
    "if args.datasets == 'cifar_100':\n",
    "    data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((im_dimention,im_dimention)),\n",
    "        transforms.RandomRotation(15,),\n",
    "        transforms.RandomCrop(im_dimention),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar_100_mean, std=cifar_100_std)\n",
    "    ]),\n",
    "    # 'valid': transforms.Compose([\n",
    "    #     transforms.Resize((im_dimention,im_dimention)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=cifar_100_mean, std=cifar_100_std)\n",
    "    # ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((im_dimention,im_dimention)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar_100_mean, std=cifar_100_std)\n",
    "    ]),\n",
    "    }\n",
    "    full_trainset = torchvision.datasets.CIFAR100(\n",
    "        root='./data/cifar_100', train=True, download=True, transform=data_transforms['train'])\n",
    "    testset = torchvision.datasets.CIFAR100(\n",
    "        root='./data/cifar_100', train=False, download=True, transform=data_transforms['test'])\n",
    "    testloader = DataLoader(\n",
    "        testset, batch_size=args.batch_size, shuffle=False,sampler=torch.utils.data.SequentialSampler(testset),  num_workers=0)\n",
    "    Num_class = 100\n",
    "\n",
    "# # 获取所有样本的标签 \n",
    "# labels = [full_trainset[i][1] for i in range(len(full_trainset))]\n",
    "\n",
    "# # 分层划分（stratify参数确保比例）\n",
    "# train_idx, val_idx = train_test_split(\n",
    "#     range(len(full_trainset)),\n",
    "#     test_size=0.2,\n",
    "#     shuffle=True,\n",
    "#     stratify=labels,\n",
    "#     random_state=args.seed  \n",
    "# )\n",
    "\n",
    "# train_data = np.stack([full_trainset.data[i]  for i in train_idx]) \n",
    "# train_targets = [full_trainset.targets[i] for i in train_idx] \n",
    "# val_data = np.stack([full_trainset.data[i]  for i in val_idx]) \n",
    "# val_targets = [full_trainset.targets[i] for i in val_idx] \n",
    "\n",
    "# valset = full_trainset\n",
    "# valset.data = val_data\n",
    "# valset.targets = val_targets\n",
    "# valset.transform = data_transforms['valid']\n",
    "\n",
    "# trainset = copy.deepcopy(valset)\n",
    "# trainset.data = train_data\n",
    "# trainset.targets = train_targets\n",
    "# trainset.transform = data_transforms['train']\n",
    "\n",
    "# trainloader = {\n",
    "#     'train':DataLoader(\n",
    "#     trainset, batch_size=args.batch_size, shuffle=False, sampler=torch.utils.data.SequentialSampler(trainset), num_workers=0),\n",
    "#     'valid':DataLoader(\n",
    "#     valset, batch_size=args.batch_size, shuffle=False, sampler=torch.utils.data.SequentialSampler(valset), num_workers=0)}\n",
    "\n",
    "# dataset_sizes = {\n",
    "#     'train': len(trainset),\n",
    "#     'valid': len(valset),            \n",
    "                #  }\n",
    "\n",
    "trainloader = {\n",
    "    'train':DataLoader(\n",
    "    full_trainset, batch_size=args.batch_size, shuffle=False, sampler=torch.utils.data.SequentialSampler(full_trainset), num_workers=0),\n",
    "    'valid':testloader\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(full_trainset),\n",
    "    'valid': len(testset),      \n",
    "}\n",
    "print(dataset_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timm.list_models('*vit_tiny_patch16_224*')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms.functional  import to_pil_image \n",
    "# 反归一化转换（需与transform中的参数对应）\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.4914/0.2023, -0.4822/0.1994, -0.4465/0.2010],\n",
    "    std=[1/0.2023, 1/0.1994, 1/0.2010]\n",
    ")\n",
    " \n",
    "def show_images(loader, num_images=4):\n",
    "    # 获取一个batch的数据 \n",
    "    # images, labels = next(iter(loader))\n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        if batch_idx == 0:  # 只取第一个batch \n",
    "            break \n",
    "\n",
    "    # 创建子图 \n",
    "    fig, axes = plt.subplots(1,  num_images, figsize=(15, 3))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # 反归一化+通道顺序调整 \n",
    "        img = inv_normalize(images[i]).permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img,  0, 1)  # 处理浮点误差 \n",
    "        \n",
    "        # 显示图像及标签 \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Label: {full_trainset.classes[labels[i]]}\") \n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show() \n",
    " \n",
    "show_images(trainloader['train'])\n",
    "show_images(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/\" + args.datasets + \"/benckmark.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    net_benckmark_data = torch.load(model_path,  map_location='cpu')\n",
    "    benckmark_state_dict = net_benckmark_data['model_state_dict'] \n",
    "else:\n",
    "    net_benchmark = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "    torch.save({\n",
    "        'model_state_dict': net_benchmark.state_dict()\n",
    "    }, model_path)\n",
    "    benckmark_state_dict = net_benchmark.state_dict()\n",
    "\n",
    "def tensor_to_serializable(obj):\n",
    "    if isinstance(obj, (np.float32,  np.float64)):   # 处理NumPy浮点数\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.integer):               # 处理NumPy整数 \n",
    "        return int(obj)\n",
    "    elif isinstance(obj, torch.Tensor):            # 处理PyTorch Tensor \n",
    "        return obj.item()  if obj.numel()  == 1 else obj.tolist() \n",
    "    elif isinstance(obj, (np.ndarray)):             # 处理NumPy数组 \n",
    "        return obj.tolist() \n",
    "    elif hasattr(obj, '__dict__'):                 # 处理自定义对象（可选）\n",
    "        return obj.__dict__\n",
    "    return obj \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdt_delta = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugdt_delta.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdt_delta.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugdt_delta = torch.nn.DataParallel(net_pugdt_delta)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXT(net_pugdt_delta.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugdt_delta, metricst_delta = train_model_timing_delta(net_pugdt_delta, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes, int(args.epochs/10), 0.01, 10) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdt_delta_\" + str(args.epochs) + \"xi\" + str(args.epochs/10) + \"mu0.01_t10.pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdt_delta.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdt_delta_\" + str(args.epochs) + \"xi\" + str(args.epochs/10) + \"mu0.01_t10.json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricst_delta,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb925b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdt_var = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugdt_var.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdt_var.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugdt_var = torch.nn.DataParallel(net_pugdt_var)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXT(net_pugdt_var.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugdt_var, metricst_var = train_model_timing_var(net_pugdt_var, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes, int(args.epochs/10), 0.015, 10) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdt_var_\" + str(args.epochs) + \"init_t\" + str(args.epochs/10) + \"gamma0.015_k10.pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdt_var.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdt_var_\" + str(args.epochs) + \"init_t\" + str(args.epochs/10) + \"gamma0.015_k10.json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricst_var,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "# net.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net = torch.nn.DataParallel(net)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(),\n",
    "#                 lr=args.lr,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net, metrics_org = train_model_org(net, criterion, optimizer, scheduler, args.epochs * 2, trainloader, device, dataset_sizes) \n",
    "\n",
    "# # 保存模型架构+参数+优化器状态（完整恢复训练）\n",
    "# model_path = \"./model/\"+args.datasets+\"/org\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/org_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metrics_org, f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n",
    " \n",
    "# # 加载 \n",
    "\n",
    "# # checkpoint = torch.load('full_model_checkpoint.pth',  map_location='cpu')  # 先加载到CPU避免设备冲突 \n",
    "# # 模型结构需提前定义（需与保存时一致）\n",
    "# # model = YourModelClass()  \n",
    "# # model.load_state_dict(checkpoint['model_state_dict']) \n",
    " \n",
    "# # # 恢复优化器和训练状态 \n",
    "# # optimizer = torch.optim.Adam(model.parameters())  \n",
    "# # optimizer.load_state_dict(checkpoint['optimizer_state_dict']) \n",
    "# # with open('data.json',  'r', encoding='utf-8') as f:\n",
    "# #     loaded_dict = json.load(f) \n",
    "\n",
    "\n",
    "# # summary(net, (3, img_size, img_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_pugdr_sin = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "net_pugdr_sin.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "net_pugdr_sin.to(device)\n",
    "\n",
    "if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "    model_ft_org = torch.nn.DataParallel(net_pugdr_sin)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "base_optimizer = optim.SGD\n",
    "optimizer = PUGDXR(net_pugdr_sin.parameters(),\n",
    "                base_optimizer,\n",
    "                lr=args.lr,\n",
    "                max_epochs= args.epochs,\n",
    "                momentum=args.momentum,\n",
    "                weight_decay=args.wd,\n",
    "                min_beta = 0.0, \n",
    "                max_beta = 2.0, \n",
    "                method = 'sin',\n",
    "                dampening=0,   # 必须设置为0才能完全固定 \n",
    "                nesterov=False # 禁用Nesterov动量 \n",
    "                )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "net_pugdr_sin, metricsr_sin = train_model_alpha(net_pugdr_sin, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "model_path = \"./model/\"+args.datasets+\"/pugdr_sin\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': net_pugdr_sin.state_dict(), \n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, model_path) \n",
    "\n",
    "name = \"./results/\"+args.datasets+\"/pugdr_sin_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "with open(name,  'w', encoding='utf-8') as f:\n",
    "    json.dump(metricsr_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82cb0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_pugdr_cos = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "net_pugdr_cos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "net_pugdr_cos.to(device)\n",
    "\n",
    "if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "    model_ft_org = torch.nn.DataParallel(net_pugdr_cos)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "base_optimizer = optim.SGD\n",
    "optimizer = PUGDXR(net_pugdr_cos.parameters(),\n",
    "                base_optimizer,\n",
    "                lr=args.lr,\n",
    "                max_epochs= args.epochs,\n",
    "                momentum=args.momentum,\n",
    "                weight_decay=args.wd,\n",
    "                min_beta = 0.0, \n",
    "                max_beta = 1.0, \n",
    "                method = 'cos',\n",
    "                dampening=0,   # 必须设置为0才能完全固定 \n",
    "                nesterov=False # 禁用Nesterov动量 \n",
    "                )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "net_pugdr_cos, metricsr_sin = train_model_alpha(net_pugdr_cos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "model_path = \"./model/\"+args.datasets+\"/pugdr_cos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': net_pugdr_cos.state_dict(), \n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, model_path) \n",
    "\n",
    "name = \"./results/\"+args.datasets+\"/pugdr_cos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "with open(name,  'w', encoding='utf-8') as f:\n",
    "    json.dump(metricsr_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_pugds_cos = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "net_pugds_cos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "net_pugds_cos.to(device)\n",
    "\n",
    "if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "    net_pugds_cos = torch.nn.DataParallel(net_pugds_cos)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "base_optimizer = optim.SGD\n",
    "optimizer = PUGDXS(net_pugds_cos.parameters(),\n",
    "                base_optimizer,\n",
    "                lr=args.lr,\n",
    "                max_epochs= args.epochs,\n",
    "                momentum=args.momentum,\n",
    "                weight_decay=args.wd,\n",
    "                min_beta = 0.5, \n",
    "                max_beta = 1.0,\n",
    "                method = 'cos',\n",
    "                dampening=0,   # 必须设置为0才能完全固定 \n",
    "                nesterov=False # 禁用Nesterov动量 \n",
    "                 )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "net_pugds_cos, metricss_cos = train_model_alpha(net_pugds_cos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "model_path = \"./model/\"+args.datasets+\"/pugds_cos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': net_pugds_cos.state_dict(), \n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, model_path) \n",
    "\n",
    "name = \"./results/\"+args.datasets+\"/pugds_cos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "with open(name,  'w', encoding='utf-8') as f:\n",
    "    json.dump(metricss_cos,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e79fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugds_sin = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugds_sin.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugds_sin.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugds_sin = torch.nn.DataParallel(net_pugds_sin)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXS(net_pugds_sin.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.0, \n",
    "#                 max_beta = 2.0,\n",
    "#                 method = 'sin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugds_sin, metricss_sin = train_model_alpha(net_pugds_sin, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugds_sin\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugds_sin.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugds_sin_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricss_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugd = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugd.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugd.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugd)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDX(net_pugd.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugd, metrics0 = train_model(net_pugd, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugd\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugd.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugd_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c195537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdr_cos = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugdr_cos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdr_cos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugdr_cos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(net_pugdr_cos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.1, \n",
    "#                 max_beta = 2, \n",
    "#                 method = 'icos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugdr_cos, metricsr_cos = train_model_alpha(net_pugdr_cos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdr_icos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdr_cos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdr_icos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsr_cos,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdr_sin = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "# net_pugdr_sin.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdr_sin.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugdr_sin)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(net_pugdr_sin.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.1, \n",
    "#                 max_beta = 2, \n",
    "#                 method = 'isin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugdr_sin, metricsr_sin = train_model_alpha(net_pugdr_sin, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdr_isin\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdr_sin.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdr_isin_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsr_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b645d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugds_icos = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugds_icos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugds_icos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugds_icos = torch.nn.DataParallel(net_pugds_icos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXS(net_pugds_icos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 1, \n",
    "#                 max_beta = 2, \n",
    "#                 method = 'icos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugds_icos, metricss_icos = train_model_alpha(net_pugds_icos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugds_icos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugds_icos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugds_icos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricss_icos,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214741d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugds_isin = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugds_isin.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugds_isin.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugds_isin = torch.nn.DataParallel(net_pugds_isin)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXS(net_pugds_isin.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.8, \n",
    "#                 max_beta = 3, \n",
    "#                 method = 'isin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugds_isin, metricss_isin = train_model_alpha(net_pugds_isin, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugds_isin\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugds_isin.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugds_isin_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricss_isin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_pugdrs = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "net_pugdrs.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "net_pugdrs.to(device)\n",
    "\n",
    "if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "    net_pugdrs = torch.nn.DataParallel(net_pugdrs)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "base_optimizer = optim.SGD\n",
    "optimizer = PUGDXRS(net_pugdrs.parameters(),\n",
    "                base_optimizer,\n",
    "                lr=args.lr,\n",
    "                max_epochs= args.epochs,\n",
    "                momentum=args.momentum,\n",
    "                weight_decay=args.wd,\n",
    "                min_beta_r = 0.01, \n",
    "                max_beta_r = 2, \n",
    "                method_r = 'icos',\n",
    "                min_beta_s = 1, \n",
    "                max_beta_s = 2, \n",
    "                method_s = 'icos',\n",
    "                dampening=0,   # 必须设置为0才能完全固定 \n",
    "                nesterov=False # 禁用Nesterov动量 \n",
    "                 )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "net_pugdrs, metricsrs = train_model_alpha(net_pugdrs, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "model_path = \"./model/\"+args.datasets+\"/pugdrs_\" + str(optimizer.method_r) + str(optimizer.max_beta_r) + \"_\" + str(optimizer.min_beta_r) + \"_\" + str(optimizer.method_s) + str(optimizer.max_beta_s) + \"_\" + str(optimizer.min_beta_s) + \"_\" + str(args.epochs) + \".pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': net_pugdrs.state_dict(), \n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, model_path) \n",
    "\n",
    "name = \"./results/\"+args.datasets+\"/pugdrs_\" + str(optimizer.method_r) + str(optimizer.max_beta_r) + \"_\" + str(optimizer.min_beta_r) + \"_\" + str(optimizer.method_s) + str(optimizer.max_beta_s) + \"_\" + str(optimizer.min_beta_s) + \"_\" + str(args.epochs) + \".json\"\n",
    "with open(name,  'w', encoding='utf-8') as f:\n",
    "    json.dump(metricsrs,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG-16 ResNet-18 DenseNet-121* growth rate in 16 UPANet-16 Overall Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## finetune\n",
    "\n",
    "# from transformers import ViTForImageClassification, DeiTForImageClassification \n",
    " \n",
    "# # 加载预训练模型 \n",
    "# vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", num_labels=10, ignore_mismatched_sizes=True, device_map=\"auto\", resume_download=True) \n",
    "# deit_model = DeiTForImageClassification.from_pretrained(\"facebook/deit-base-patch16-224\", num_labels=10, ignore_mismatched_sizes=True, device_map=\"auto\", resume_download=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b0f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
    "#     optimizer,  # 绑定的优化器对象 \n",
    "#     mode='max',  # 监测指标模式 \n",
    "#     factor=0.5,  # 学习率衰减系数 \n",
    "#     patience=3   # 等待周期数 \n",
    "# )\n",
    "# for name, param in vit_model.named_parameters(): \n",
    "#     if 'encoder.layer.0'  in name:  # 冻结前N层 \n",
    "#         param.requires_grad  = False \n",
    "# # 修改分类头以适应CIFAR-10的10类 \n",
    "# vit_model.classifier  = torch.nn.Linear(vit_model.config.hidden_size,  10)\n",
    "# deit_model.classifier  = torch.nn.Linear(deit_model.config.hidden_size,  10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8923cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(new_head.__dict__)\n",
    "# print(vars(new_head))\n",
    "# import optimizers\n",
    "# import importlib \n",
    "# importlib.reload(optimizers)   \n",
    "# from optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c21966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft2 = timm.create_model('mobilenetv3_small_100', pretrained=True, num_classes=Num_class)\n",
    "# original_head = model_ft2.classifier   # MobileNetV3的分类头名为classifier \n",
    "# new_head = nn.Linear(original_head.in_features,  Num_class)\n",
    "# model_ft2 = model_ft2.to(device)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# base_optimizer2 = optim.SGD\n",
    "# optimizer2 = PUGD2(model_ft2.parameters(),\n",
    "#                  base_optimizer2,\n",
    "#                  lr=args.lr,\n",
    "#                  momentum=args.momentum,\n",
    "#                  weight_decay=args.wd,\n",
    "#                  )\n",
    "\n",
    "\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer2, step_size=7, gamma=0.1)\n",
    "\n",
    "# model_ft2 = train_model2(model_ft2, criterion, optimizer2, exp_lr_scheduler, num_epochs=20) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
