{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c297172",
   "metadata": {},
   "source": [
    "Multi-level Perturbed Unit Gradient Descent, MPUGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51d2b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data  import Subset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models  as models\n",
    "from types import SimpleNamespace \n",
    "import matplotlib.pyplot  as plt\n",
    "import numpy as np \n",
    "\n",
    "from optimizers import *\n",
    "from upanets import UPANets\n",
    "from torchsummary import summary\n",
    "import time, copy,timm\n",
    "import json\n",
    "import random \n",
    "import os\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eaa60ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args = SimpleNamespace(\n",
    "    datasets='cifar_10',\n",
    "    batch_size = 500,\n",
    "    seed = 42,\n",
    "    lr=0.1, \n",
    "    momentum=0.9,\n",
    "    wd = 0.0005,\n",
    "    blocks = 1,\n",
    "    filters = 16,\n",
    "    epochs = 400,\n",
    "    start_epochs = 8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bdda934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed=42):\n",
    "    # Python原生随机 \n",
    "    random.seed(seed) \n",
    "    # NumPy随机 \n",
    "    np.random.seed(seed) \n",
    "    # PyTorch随机 \n",
    "    torch.manual_seed(seed) \n",
    "    # CUDA随机（GPU相关）\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    # CUDNN确定性模式 \n",
    "    torch.backends.cudnn.deterministic  = True \n",
    "    torch.backends.cudnn.benchmark  = False \n",
    " \n",
    "set_all_seeds(args.seed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca2f2d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "557e50ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 50000, 'valid': 10000}\n"
     ]
    }
   ],
   "source": [
    "img_size = 32 # default image size for Cifar-10\n",
    "im_dimention = 32\n",
    "cifar_10_mean = [0.4914, 0.4822, 0.4465] \n",
    "cifar_10_std = [0.2023, 0.1994, 0.2010]\n",
    "cifar_100_mean = [0.5071, 0.4867, 0.4408]\n",
    "cifar_100_std = [0.2673, 0.2564, 0.2762]\n",
    "\n",
    "if args.datasets == 'cifar_10':\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((im_dimention,im_dimention)),\n",
    "            transforms.RandomRotation(15,),\n",
    "            transforms.RandomCrop(im_dimention),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cifar_10_mean, std=cifar_10_std)\n",
    "            # transforms.Lambda(lambda x: x.to(torch.float16))    # 最终输出FP16\n",
    "        ]),\n",
    "        # 'valid': transforms.Compose([\n",
    "        #     transforms.Resize((im_dimention,im_dimention)),\n",
    "        #     transforms.ToTensor(),\n",
    "        #     transforms.Normalize(mean=cifar_10_mean, std=cifar_10_std)\n",
    "        # ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((im_dimention,im_dimention)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cifar_10_mean, std=cifar_10_std)\n",
    "        ]),\n",
    "    }\n",
    " \n",
    "    full_trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data/cifar_10', train=True, download=True, transform=data_transforms['train'])\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data/cifar_10', train=False, download=True, transform=data_transforms['test'])\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "    Num_class = 10\n",
    "\n",
    "if args.datasets == 'cifar_100':\n",
    "    data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((im_dimention,im_dimention)),\n",
    "        transforms.RandomRotation(15,),\n",
    "        transforms.RandomCrop(im_dimention),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar_100_mean, std=cifar_100_std)\n",
    "    ]),\n",
    "    # 'valid': transforms.Compose([\n",
    "    #     transforms.Resize((im_dimention,im_dimention)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=cifar_100_mean, std=cifar_100_std)\n",
    "    # ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((im_dimention,im_dimention)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar_100_mean, std=cifar_100_std)\n",
    "    ]),\n",
    "    }\n",
    "    full_trainset = torchvision.datasets.CIFAR100(\n",
    "        root='./data/cifar_100', train=True, download=True, transform=data_transforms['train'])\n",
    "    testset = torchvision.datasets.CIFAR100(\n",
    "        root='./data/cifar_100', train=False, download=True, transform=data_transforms['test'])\n",
    "    testloader = DataLoader(\n",
    "        testset, batch_size=args.batch_size, shuffle=False,sampler=torch.utils.data.SequentialSampler(testset),  num_workers=0)\n",
    "    Num_class = 100\n",
    "\n",
    "# # 获取所有样本的标签 \n",
    "# labels = [full_trainset[i][1] for i in range(len(full_trainset))]\n",
    "\n",
    "# # 分层划分（stratify参数确保比例）\n",
    "# train_idx, val_idx = train_test_split(\n",
    "#     range(len(full_trainset)),\n",
    "#     test_size=0.2,\n",
    "#     shuffle=True,\n",
    "#     stratify=labels,\n",
    "#     random_state=args.seed  \n",
    "# )\n",
    "\n",
    "# train_data = np.stack([full_trainset.data[i]  for i in train_idx]) \n",
    "# train_targets = [full_trainset.targets[i] for i in train_idx] \n",
    "# val_data = np.stack([full_trainset.data[i]  for i in val_idx]) \n",
    "# val_targets = [full_trainset.targets[i] for i in val_idx] \n",
    "\n",
    "# valset = full_trainset\n",
    "# valset.data = val_data\n",
    "# valset.targets = val_targets\n",
    "# valset.transform = data_transforms['valid']\n",
    "\n",
    "# trainset = copy.deepcopy(valset)\n",
    "# trainset.data = train_data\n",
    "# trainset.targets = train_targets\n",
    "# trainset.transform = data_transforms['train']\n",
    "\n",
    "# trainloader = {\n",
    "#     'train':DataLoader(\n",
    "#     trainset, batch_size=args.batch_size, shuffle=False, sampler=torch.utils.data.SequentialSampler(trainset), num_workers=0),\n",
    "#     'valid':DataLoader(\n",
    "#     valset, batch_size=args.batch_size, shuffle=False, sampler=torch.utils.data.SequentialSampler(valset), num_workers=0)}\n",
    "\n",
    "# dataset_sizes = {\n",
    "#     'train': len(trainset),\n",
    "#     'valid': len(valset),            \n",
    "                #  }\n",
    "\n",
    "trainloader = {\n",
    "    'train':DataLoader(\n",
    "    full_trainset, batch_size=args.batch_size, shuffle=False, sampler=torch.utils.data.SequentialSampler(full_trainset), num_workers=0),\n",
    "    'valid':testloader\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(full_trainset),\n",
    "    'valid': len(testset),      \n",
    "}\n",
    "print(dataset_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "840b5037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vit_tiny_patch16_224']\n"
     ]
    }
   ],
   "source": [
    "print(timm.list_models('*vit_tiny_patch16_224*')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ab5b31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from torchvision.transforms.functional  import to_pil_image \n",
    "# 反归一化转换（需与transform中的参数对应）\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.4914/0.2023, -0.4822/0.1994, -0.4465/0.2010],\n",
    "    std=[1/0.2023, 1/0.1994, 1/0.2010]\n",
    ")\n",
    " \n",
    "def show_images(loader, num_images=4):\n",
    "    # 获取一个batch的数据 \n",
    "    # images, labels = next(iter(loader))\n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        if batch_idx == 0:  # 只取第一个batch \n",
    "            break \n",
    "\n",
    "    # 创建子图 \n",
    "    fig, axes = plt.subplots(1,  num_images, figsize=(15, 3))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # 反归一化+通道顺序调整 \n",
    "        img = inv_normalize(images[i]).permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img,  0, 1)  # 处理浮点误差 \n",
    "        \n",
    "        # 显示图像及标签 \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Label: {full_trainset.classes[labels[i]]}\") \n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show() \n",
    " \n",
    "show_images(trainloader['train'])\n",
    "show_images(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5aa47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/\" + args.datasets + \"/benchmark.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    net_benckmark_data = torch.load(model_path,  map_location='cpu')\n",
    "    benckmark_state_dict = net_benckmark_data['model_state_dict'] \n",
    "else:\n",
    "    net_benchmark = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "    torch.save({\n",
    "        'model_state_dict': net_benchmark.state_dict()\n",
    "    }, model_path)\n",
    "    benckmark_state_dict = net_benchmark.state_dict()\n",
    "\n",
    "def tensor_to_serializable(obj):\n",
    "    if isinstance(obj, (np.float32,  np.float64)):   # 处理NumPy浮点数\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.integer):               # 处理NumPy整数 \n",
    "        return int(obj)\n",
    "    elif isinstance(obj, torch.Tensor):            # 处理PyTorch Tensor \n",
    "        return obj.item()  if obj.numel()  == 1 else obj.tolist() \n",
    "    elif isinstance(obj, (np.ndarray)):             # 处理NumPy数组 \n",
    "        return obj.tolist() \n",
    "    elif hasattr(obj, '__dict__'):                 # 处理自定义对象（可选）\n",
    "        return obj.__dict__\n",
    "    return obj \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3ec9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdt_delta = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugdt_delta.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdt_delta.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugdt_delta = torch.nn.DataParallel(net_pugdt_delta)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXT(net_pugdt_delta.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugdt_delta, metricst_delta = train_model_timing_delta(net_pugdt_delta, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes, int(args.epochs/10), 0.01, 10) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdt_delta_\" + str(args.epochs) + \"xi\" + str(args.epochs/10) + \"mu0.01_t10.pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdt_delta.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdt_delta_\" + str(args.epochs) + \"xi\" + str(args.epochs/10) + \"mu0.01_t10.json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricst_delta,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "efb925b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdt_var = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugdt_var.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdt_var.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugdt_var = torch.nn.DataParallel(net_pugdt_var)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXT(net_pugdt_var.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugdt_var, metricst_var = train_model_timing_var(net_pugdt_var, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes, int(args.epochs/10), 0.015, 10) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdt_var_\" + str(args.epochs) + \"init_t\" + str(args.epochs/10) + \"gamma0.015_k10.pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdt_var.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdt_var_\" + str(args.epochs) + \"init_t\" + str(args.epochs/10) + \"gamma0.015_k10.json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricst_var,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a42d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "# net.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net = torch.nn.DataParallel(net)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(),\n",
    "#                 lr=args.lr,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net, metrics_org = train_model_org(net, criterion, optimizer, scheduler, args.epochs * 2, trainloader, device, dataset_sizes) \n",
    "\n",
    "# # 保存模型架构+参数+优化器状态（完整恢复训练）\n",
    "# model_path = \"./model/\"+args.datasets+\"/org\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/org_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metrics_org, f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n",
    " \n",
    "# # 加载 \n",
    "\n",
    "# # checkpoint = torch.load('full_model_checkpoint.pth',  map_location='cpu')  # 先加载到CPU避免设备冲突 \n",
    "# # 模型结构需提前定义（需与保存时一致）\n",
    "# # model = YourModelClass()  \n",
    "# # model.load_state_dict(checkpoint['model_state_dict']) \n",
    " \n",
    "# # # 恢复优化器和训练状态 \n",
    "# # optimizer = torch.optim.Adam(model.parameters())  \n",
    "# # optimizer.load_state_dict(checkpoint['optimizer_state_dict']) \n",
    "# # with open('data.json',  'r', encoding='utf-8') as f:\n",
    "# #     loaded_dict = json.load(f) \n",
    "\n",
    "\n",
    "# # summary(net, (3, img_size, img_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0db0a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdr_sin = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "# net_pugdr_sin.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdr_sin.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugdr_sin)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(net_pugdr_sin.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.2, \n",
    "#                 max_beta = 2.0, \n",
    "#                 method = 'sin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugdr_sin, metricsr_sin = train_model_alpha(net_pugdr_sin, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdr_sin\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdr_sin.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdr_sin_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsr_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c82cb0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdr_cos = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "# net_pugdr_cos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdr_cos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugdr_cos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(net_pugdr_cos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.001, \n",
    "#                 max_beta = 2.0, \n",
    "#                 method = 'cos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugdr_cos, metricsr_sin = train_model_alpha(net_pugdr_cos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdr_cos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdr_cos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdr_cos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsr_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db4abe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     if metricsr_sin['bast_acc'] < 0.7234:\n",
    "#         net_pugdr_cos = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "#         net_pugdr_cos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "#         criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#         net_pugdr_cos.to(device)\n",
    "\n",
    "#         if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#             model_ft_org = torch.nn.DataParallel(net_pugdr_cos)\n",
    "#             cudnn.benchmark = True\n",
    "\n",
    "#         base_optimizer = optim.SGD\n",
    "#         optimizer = PUGDXR(net_pugdr_cos.parameters(),\n",
    "#                         base_optimizer,\n",
    "#                         lr=args.lr,\n",
    "#                         max_epochs= args.epochs,\n",
    "#                         momentum=args.momentum,\n",
    "#                         weight_decay=args.wd,\n",
    "#                         min_beta = 0.0, \n",
    "#                         max_beta = 2.0, \n",
    "#                         method = 'cos',\n",
    "#                         dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                         nesterov=False # 禁用Nesterov动量 \n",
    "#                         )\n",
    "\n",
    "#         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "#         # Decay LR by a factor of 0.1 every 7 epochs\n",
    "#         # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "#         net_pugdr_cos, metricsr_sin = train_model_alpha(net_pugdr_cos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "#         model_path = \"./model/\"+args.datasets+\"/pugdr_cos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "#         torch.save({\n",
    "#             'model_state_dict': net_pugdr_cos.state_dict(), \n",
    "#             'optimizer_state_dict': optimizer.state_dict()\n",
    "#         }, model_path) \n",
    "\n",
    "#         name = \"./results/\"+args.datasets+\"/pugdr_cos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "#         with open(name,  'w', encoding='utf-8') as f:\n",
    "#             json.dump(metricsr_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a514712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdr_cos = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "# net_pugdr_cos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdr_cos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugdr_cos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(net_pugdr_cos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.0, \n",
    "#                 max_beta = 3.0, \n",
    "#                 method = 'cos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugdr_cos, metricsr_sin = train_model_alpha(net_pugdr_cos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdr_cos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdr_cos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdr_cos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsr_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be3e5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugds_cos = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugds_cos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugds_cos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugds_cos = torch.nn.DataParallel(net_pugds_cos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXS(net_pugds_cos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 1.2, \n",
    "#                 max_beta = 3.0,\n",
    "#                 method = 'cos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugds_cos, metricss_cos = train_model_alpha(net_pugds_cos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugds_cos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugds_cos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugds_cos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricss_cos,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14e79fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugds_sin = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugds_sin.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugds_sin.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugds_sin = torch.nn.DataParallel(net_pugds_sin)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXS(net_pugds_sin.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 1.8, \n",
    "#                 max_beta = 2.0,\n",
    "#                 method = 'sin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugds_sin, metricss_sin = train_model_alpha(net_pugds_sin, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugds_sin\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugds_sin.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugds_sin_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricss_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1d2ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugd = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugd.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugd.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugd)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDX(net_pugd.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugd, metrics0 = train_model(net_pugd, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugd\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugd.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugd_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c195537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdr_cos = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugdr_cos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdr_cos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugdr_cos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(net_pugdr_cos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.3, \n",
    "#                 max_beta = 1, \n",
    "#                 method = 'icos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugdr_cos, metricsr_cos = train_model_alpha(net_pugdr_cos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdr_icos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdr_cos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdr_icos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsr_cos,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10cf6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdr_sin = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "# net_pugdr_sin.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdr_sin.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugdr_sin)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(net_pugdr_sin.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.1, \n",
    "#                 max_beta = 1, \n",
    "#                 method = 'isin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugdr_sin, metricsr_sin = train_model_alpha(net_pugdr_sin, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdr_isin\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdr_sin.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdr_isin_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsr_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6b645d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugds_icos = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugds_icos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugds_icos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugds_icos = torch.nn.DataParallel(net_pugds_icos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXS(net_pugds_icos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.01, \n",
    "#                 max_beta = 0.5, \n",
    "#                 method = 'icos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugds_icos, metricss_icos = train_model_alpha(net_pugds_icos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugds_icos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugds_icos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugds_icos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricss_icos,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9575fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugds_icos = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugds_icos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugds_icos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugds_icos = torch.nn.DataParallel(net_pugds_icos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXS(net_pugds_icos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.05, \n",
    "#                 max_beta = 1.0, \n",
    "#                 method = 'icos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugds_icos, metricss_icos = train_model_alpha(net_pugds_icos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugds_icos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugds_icos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugds_icos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricss_icos,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "214741d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugds_isin = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugds_isin.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugds_isin.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugds_isin = torch.nn.DataParallel(net_pugds_isin)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXS(net_pugds_isin.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.1, \n",
    "#                 max_beta = 2,\n",
    "#                 method = 'isin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugds_isin, metricss_isin = train_model_alpha(net_pugds_isin, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugds_isin\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugds_isin.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugds_isin_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricss_isin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "156b0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdrs = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugdrs.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdrs.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugdrs = torch.nn.DataParallel(net_pugdrs)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXRS(net_pugdrs.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta_r = 0.01, \n",
    "#                 max_beta_r = 2, \n",
    "#                 method_r = 'isin',\n",
    "#                 min_beta_s = 0, \n",
    "#                 max_beta_s = 1.5,\n",
    "#                 method_s = 'cos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugdrs, metricsrs = train_model_alpha(net_pugdrs, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdrs_\" + str(optimizer.method_r) + str(optimizer.max_beta_r) + \"_\" + str(optimizer.min_beta_r) + \"_\" + str(optimizer.method_s) + str(optimizer.max_beta_s) + \"_\" + str(optimizer.min_beta_s) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdrs.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdrs_\" + str(optimizer.method_r) + str(optimizer.max_beta_r) + \"_\" + str(optimizer.min_beta_r) + \"_\" + str(optimizer.method_s) + str(optimizer.max_beta_s) + \"_\" + str(optimizer.min_beta_s) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsrs,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f20f6280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdrs = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugdrs.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdrs.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugdrs = torch.nn.DataParallel(net_pugdrs)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXRS(net_pugdrs.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta_r = 0.0,\n",
    "#                 max_beta_r = 2.0,\n",
    "#                 method_r = 'sin',\n",
    "#                 min_beta_s = 0.0, \n",
    "#                 max_beta_s = 2.0,\n",
    "#                 method_s = 'sin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugdrs, metricsrs = train_model_alpha(net_pugdrs, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdrs_\" + str(optimizer.method_r) + str(optimizer.max_beta_r) + \"_\" + str(optimizer.min_beta_r) + \"_\" + str(optimizer.method_s) + str(optimizer.max_beta_s) + \"_\" + str(optimizer.min_beta_s) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdrs.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdrs_\" + str(optimizer.method_r) + str(optimizer.max_beta_r) + \"_\" + str(optimizer.min_beta_r) + \"_\" + str(optimizer.method_s) + str(optimizer.max_beta_s) + \"_\" + str(optimizer.min_beta_s) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsrs,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "672b77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG-16 ResNet-18 DenseNet-121* growth rate in 16 UPANet-16 Overall Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "088d3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## finetune\n",
    "\n",
    "# from transformers import ViTForImageClassification, DeiTForImageClassification \n",
    " \n",
    "# # 加载预训练模型 \n",
    "# vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", num_labels=10, ignore_mismatched_sizes=True, device_map=\"auto\", resume_download=True) \n",
    "# deit_model = DeiTForImageClassification.from_pretrained(\"facebook/deit-base-patch16-224\", num_labels=10, ignore_mismatched_sizes=True, device_map=\"auto\", resume_download=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd6b0f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
    "#     optimizer,  # 绑定的优化器对象 \n",
    "#     mode='max',  # 监测指标模式 \n",
    "#     factor=0.5,  # 学习率衰减系数 \n",
    "#     patience=3   # 等待周期数 \n",
    "# )\n",
    "# for name, param in vit_model.named_parameters(): \n",
    "#     if 'encoder.layer.0'  in name:  # 冻结前N层 \n",
    "#         param.requires_grad  = False \n",
    "# # 修改分类头以适应CIFAR-10的10类 \n",
    "# vit_model.classifier  = torch.nn.Linear(vit_model.config.hidden_size,  10)\n",
    "# deit_model.classifier  = torch.nn.Linear(deit_model.config.hidden_size,  10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8923cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(new_head.__dict__)\n",
    "# print(vars(new_head))\n",
    "# import optimizers\n",
    "# import importlib \n",
    "# importlib.reload(optimizers)   \n",
    "# from optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04c21966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft2 = timm.create_model('mobilenetv3_small_100', pretrained=True, num_classes=Num_class)\n",
    "# original_head = model_ft2.classifier   # MobileNetV3的分类头名为classifier \n",
    "# new_head = nn.Linear(original_head.in_features,  Num_class)\n",
    "# model_ft2 = model_ft2.to(device)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# base_optimizer2 = optim.SGD\n",
    "# optimizer2 = PUGD2(model_ft2.parameters(),\n",
    "#                  base_optimizer2,\n",
    "#                  lr=args.lr,\n",
    "#                  momentum=args.momentum,\n",
    "#                  weight_decay=args.wd,\n",
    "#                  )\n",
    "\n",
    "\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer2, step_size=7, gamma=0.1)\n",
    "\n",
    "# model_ft2 = train_model2(model_ft2, criterion, optimizer2, exp_lr_scheduler, num_epochs=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5bd8ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_epochs = 300\n",
    "# os.environ.update({ \n",
    "#     \"HF_ENDPOINT\": \"https://hf-mirror.com\", \n",
    "#     \"HF_HUB_OFFLINE\": \"0\",  # 确保非离线模式 \n",
    "#     \"HF_HUB_DISABLE_TELEMETRY\": \"1\",  # 禁用检测请求 \n",
    "#     \"HF_CDN_DOMAIN\": \"hf-mirror.com\", \n",
    "#     \"HF_S3_ENDPOINT\": \"hf-mirror.com\", \n",
    "#     \"TRANSFORMERS_OFFLINE\": \"0\",       # transformers专用设置 \n",
    "#     \"HF_DATASETS_OFFLINE\": \"0\"         # datasets专用设置 \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0696aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests \n",
    "# endpoints = [\"https://hf-mirror.com\",  \"https://huggingface.co\"] \n",
    "# for url in endpoints:\n",
    "#     try:\n",
    "#         r = requests.get(f\"{url}/api/models\",  timeout=5)\n",
    "#         print(f\"{url} 响应状态: {r.status_code}\") \n",
    "#     except Exception as e:\n",
    "#         print(f\"{url} 连接失败: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "01616389",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_epochs = 300\n",
    "# 加载预训练ResNet18\n",
    "resnet18 = models.resnet18(weights=(\"pretrained\", models.ResNet18_Weights.IMAGENET1K_V1))\n",
    "\n",
    "# 冻结所有卷积层\n",
    "# for name, param in resnet18.named_parameters(): \n",
    "#     if 'fc' not in name:  # 排除最后的全连接层\n",
    "#         param.requires_grad  = False\n",
    "\n",
    "resnet18.fc  = torch.nn.Linear(resnet18.fc.in_features,  Num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9179dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "7.791322708129883\n",
      "Epoch 1/299\n",
      "7.588793039321899\n",
      "Epoch 2/299\n",
      "7.86086106300354\n",
      "Epoch 3/299\n",
      "7.656046152114868\n",
      "Epoch 4/299\n",
      "7.701139211654663\n",
      "Epoch 5/299\n",
      "7.6561877727508545\n",
      "Epoch 6/299\n",
      "7.627509355545044\n",
      "Epoch 7/299\n",
      "7.6600117683410645\n",
      "Epoch 8/299\n",
      "7.603913068771362\n",
      "Epoch 9/299\n",
      "7.647732496261597\n",
      "Epoch 10/299\n",
      "7.548749923706055\n",
      "Epoch 11/299\n",
      "7.618535995483398\n",
      "Epoch 12/299\n",
      "7.694027900695801\n",
      "Epoch 13/299\n",
      "7.605334043502808\n",
      "Epoch 14/299\n",
      "7.52229905128479\n",
      "Epoch 15/299\n",
      "7.59897255897522\n",
      "Epoch 16/299\n",
      "7.799351930618286\n",
      "Epoch 17/299\n",
      "7.612516641616821\n",
      "Epoch 18/299\n",
      "7.60531759262085\n",
      "Epoch 19/299\n",
      "7.658989667892456\n",
      "Epoch 20/299\n",
      "7.687312364578247\n",
      "Epoch 21/299\n",
      "7.633037567138672\n",
      "Epoch 22/299\n",
      "7.77065634727478\n",
      "Epoch 23/299\n",
      "7.622539281845093\n",
      "Epoch 24/299\n",
      "7.588796377182007\n",
      "Epoch 25/299\n",
      "7.576785564422607\n",
      "Epoch 26/299\n",
      "7.660423040390015\n",
      "Epoch 27/299\n",
      "7.572794675827026\n",
      "Epoch 28/299\n",
      "7.606537580490112\n",
      "Epoch 29/299\n",
      "7.745173692703247\n",
      "Epoch 30/299\n",
      "7.568406820297241\n",
      "Epoch 31/299\n",
      "8.088048219680786\n",
      "Epoch 32/299\n",
      "8.33944320678711\n",
      "Epoch 33/299\n",
      "8.328998565673828\n",
      "Epoch 34/299\n",
      "8.212611436843872\n",
      "Epoch 35/299\n",
      "8.264007329940796\n",
      "Epoch 36/299\n",
      "8.48286771774292\n",
      "Epoch 37/299\n",
      "8.37537956237793\n",
      "Epoch 38/299\n",
      "8.297266960144043\n",
      "Epoch 39/299\n",
      "8.380316019058228\n",
      "Epoch 40/299\n",
      "8.375350713729858\n",
      "Epoch 41/299\n",
      "8.30964469909668\n",
      "Epoch 42/299\n",
      "8.499539375305176\n",
      "Epoch 43/299\n",
      "8.367855072021484\n",
      "Epoch 44/299\n",
      "8.242942571640015\n",
      "Epoch 45/299\n",
      "8.360944509506226\n",
      "Epoch 46/299\n",
      "8.302732467651367\n",
      "Epoch 47/299\n",
      "8.3616361618042\n",
      "Epoch 48/299\n",
      "8.366809844970703\n",
      "Epoch 49/299\n",
      "8.484721422195435\n",
      "Epoch 50/299\n",
      "8.34449815750122\n",
      "Epoch 51/299\n",
      "8.33224892616272\n",
      "Epoch 52/299\n",
      "8.342705249786377\n",
      "Epoch 53/299\n",
      "8.246558904647827\n",
      "Epoch 54/299\n",
      "8.218313694000244\n",
      "Epoch 55/299\n",
      "8.283518075942993\n",
      "Epoch 56/299\n",
      "8.552980184555054\n",
      "Epoch 57/299\n",
      "8.327293634414673\n",
      "Epoch 58/299\n",
      "8.393731594085693\n",
      "Epoch 59/299\n",
      "8.314536333084106\n",
      "Epoch 60/299\n",
      "8.267433643341064\n",
      "Epoch 61/299\n",
      "8.358411073684692\n",
      "Epoch 62/299\n",
      "8.48964238166809\n",
      "Epoch 63/299\n",
      "8.307433366775513\n",
      "Epoch 64/299\n",
      "8.35660195350647\n",
      "Epoch 65/299\n",
      "8.338833808898926\n",
      "Epoch 66/299\n",
      "8.37553596496582\n",
      "Epoch 67/299\n",
      "8.321571350097656\n",
      "Epoch 68/299\n",
      "8.21337890625\n",
      "Epoch 69/299\n",
      "8.394639015197754\n",
      "Epoch 70/299\n",
      "8.319962501525879\n",
      "Epoch 71/299\n",
      "8.377849817276001\n",
      "Epoch 72/299\n",
      "8.366032600402832\n",
      "Epoch 73/299\n",
      "8.348807096481323\n",
      "Epoch 74/299\n",
      "8.40157151222229\n",
      "Epoch 75/299\n",
      "8.352617502212524\n",
      "Epoch 76/299\n",
      "8.466840982437134\n",
      "Epoch 77/299\n",
      "8.327626705169678\n",
      "Epoch 78/299\n",
      "8.343213081359863\n",
      "Epoch 79/299\n",
      "8.370655536651611\n",
      "Epoch 80/299\n",
      "8.361448049545288\n",
      "Epoch 81/299\n",
      "8.408079862594604\n",
      "Epoch 82/299\n",
      "8.494428157806396\n",
      "Epoch 83/299\n",
      "8.371738195419312\n",
      "Epoch 84/299\n",
      "8.301911354064941\n",
      "Epoch 85/299\n",
      "8.426700830459595\n",
      "Epoch 86/299\n",
      "8.491490840911865\n",
      "Epoch 87/299\n",
      "8.483744382858276\n",
      "Epoch 88/299\n",
      "8.38049602508545\n",
      "Epoch 89/299\n",
      "8.60366702079773\n",
      "Epoch 90/299\n",
      "8.406635999679565\n",
      "Epoch 91/299\n",
      "8.34634017944336\n",
      "Epoch 92/299\n",
      "8.369470834732056\n",
      "Epoch 93/299\n",
      "8.331920385360718\n",
      "Epoch 94/299\n",
      "8.337868690490723\n",
      "Epoch 95/299\n",
      "8.337197542190552\n",
      "Epoch 96/299\n",
      "8.624333381652832\n",
      "Epoch 97/299\n",
      "8.315239429473877\n",
      "Epoch 98/299\n",
      "8.360949277877808\n",
      "Epoch 99/299\n",
      "8.435818433761597\n",
      "Epoch 100/299\n",
      "8.482118368148804\n",
      "Epoch 101/299\n",
      "8.441395282745361\n",
      "Epoch 102/299\n",
      "8.43013310432434\n",
      "Epoch 103/299\n",
      "8.612900018692017\n",
      "Epoch 104/299\n",
      "8.309120655059814\n",
      "Epoch 105/299\n",
      "8.3882577419281\n",
      "Epoch 106/299\n",
      "8.369332075119019\n",
      "Epoch 107/299\n",
      "8.399863481521606\n",
      "Epoch 108/299\n",
      "8.392916917800903\n",
      "Epoch 109/299\n",
      "8.4653799533844\n",
      "Epoch 110/299\n",
      "8.3755202293396\n",
      "Epoch 111/299\n",
      "8.387262344360352\n",
      "Epoch 112/299\n",
      "8.350892066955566\n",
      "Epoch 113/299\n",
      "8.323544979095459\n",
      "Epoch 114/299\n",
      "8.353723287582397\n",
      "Epoch 115/299\n",
      "8.324170589447021\n",
      "Epoch 116/299\n",
      "8.480441093444824\n",
      "Epoch 117/299\n",
      "8.357532024383545\n",
      "Epoch 118/299\n",
      "8.342251777648926\n",
      "Epoch 119/299\n",
      "8.412453413009644\n",
      "Epoch 120/299\n",
      "8.336187362670898\n",
      "Epoch 121/299\n",
      "8.387833595275879\n",
      "Epoch 122/299\n",
      "8.364731788635254\n",
      "Epoch 123/299\n",
      "8.58626413345337\n",
      "Epoch 124/299\n",
      "8.389123439788818\n",
      "Epoch 125/299\n",
      "8.37368392944336\n",
      "Epoch 126/299\n",
      "8.455528259277344\n",
      "Epoch 127/299\n",
      "8.470501184463501\n",
      "Epoch 128/299\n",
      "8.419025897979736\n",
      "Epoch 129/299\n",
      "8.515272855758667\n",
      "Epoch 130/299\n",
      "8.367053985595703\n",
      "Epoch 131/299\n",
      "8.38613247871399\n",
      "Epoch 132/299\n",
      "8.370842695236206\n",
      "Epoch 133/299\n",
      "8.467306137084961\n",
      "Epoch 134/299\n",
      "8.371735334396362\n",
      "Epoch 135/299\n",
      "8.421645164489746\n",
      "Epoch 136/299\n",
      "8.496279954910278\n",
      "Epoch 137/299\n",
      "8.399925231933594\n",
      "Epoch 138/299\n",
      "8.366533041000366\n",
      "Epoch 139/299\n",
      "8.38944959640503\n",
      "Epoch 140/299\n",
      "8.42304515838623\n",
      "Epoch 141/299\n",
      "8.429336309432983\n",
      "Epoch 142/299\n",
      "8.50661587715149\n",
      "Epoch 143/299\n",
      "8.606539011001587\n",
      "Epoch 144/299\n",
      "8.392654180526733\n",
      "Epoch 145/299\n",
      "8.38490629196167\n",
      "Epoch 146/299\n",
      "8.379243850708008\n",
      "Epoch 147/299\n",
      "8.429492235183716\n",
      "Epoch 148/299\n",
      "8.385804176330566\n",
      "Epoch 149/299\n",
      "8.516397953033447\n",
      "Epoch 150/299\n",
      "8.38413953781128\n",
      "Epoch 151/299\n",
      "8.379303932189941\n",
      "Epoch 152/299\n",
      "8.371403455734253\n",
      "Epoch 153/299\n",
      "8.399173259735107\n",
      "Epoch 154/299\n",
      "8.406611204147339\n",
      "Epoch 155/299\n",
      "8.366915225982666\n",
      "Epoch 156/299\n",
      "8.508818626403809\n",
      "Epoch 157/299\n",
      "8.381311893463135\n",
      "Epoch 158/299\n",
      "8.317887783050537\n",
      "Epoch 159/299\n",
      "8.478866577148438\n",
      "Epoch 160/299\n",
      "8.409808158874512\n",
      "Epoch 161/299\n",
      "8.400216341018677\n",
      "Epoch 162/299\n",
      "8.443854570388794\n",
      "Epoch 163/299\n",
      "8.589007139205933\n",
      "Epoch 164/299\n",
      "8.451894283294678\n",
      "Epoch 165/299\n",
      "8.409203290939331\n",
      "Epoch 166/299\n",
      "8.448469400405884\n",
      "Epoch 167/299\n",
      "8.420157670974731\n",
      "Epoch 168/299\n",
      "8.388672590255737\n",
      "Epoch 169/299\n",
      "8.591021537780762\n",
      "Epoch 170/299\n",
      "8.413943767547607\n",
      "Epoch 171/299\n",
      "8.464599370956421\n",
      "Epoch 172/299\n",
      "8.413406610488892\n",
      "Epoch 173/299\n",
      "8.438665628433228\n",
      "Epoch 174/299\n",
      "8.468492031097412\n",
      "Epoch 175/299\n",
      "8.451304912567139\n",
      "Epoch 176/299\n",
      "8.56885814666748\n",
      "Epoch 177/299\n",
      "8.358891010284424\n",
      "Epoch 178/299\n",
      "8.419687986373901\n",
      "Epoch 179/299\n",
      "8.382131576538086\n",
      "Epoch 180/299\n",
      "8.40651535987854\n",
      "Epoch 181/299\n",
      "8.391660213470459\n",
      "Epoch 182/299\n",
      "8.421594858169556\n",
      "Epoch 183/299\n",
      "8.527839183807373\n",
      "Epoch 184/299\n",
      "8.44236445426941\n",
      "Epoch 185/299\n",
      "8.409994840621948\n",
      "Epoch 186/299\n",
      "8.422786235809326\n",
      "Epoch 187/299\n",
      "8.456690311431885\n",
      "Epoch 188/299\n",
      "8.455923080444336\n",
      "Epoch 189/299\n",
      "8.566057682037354\n",
      "Epoch 190/299\n",
      "8.39806580543518\n",
      "Epoch 191/299\n",
      "8.355838298797607\n",
      "Epoch 192/299\n",
      "8.384098052978516\n",
      "Epoch 193/299\n",
      "8.380868196487427\n",
      "Epoch 194/299\n",
      "8.387133121490479\n",
      "Epoch 195/299\n",
      "8.424950122833252\n",
      "Epoch 196/299\n",
      "8.672983407974243\n",
      "Epoch 197/299\n",
      "8.378175497055054\n",
      "Epoch 198/299\n",
      "8.428950548171997\n",
      "Epoch 199/299\n",
      "8.391257286071777\n",
      "Epoch 200/299\n",
      "8.394881963729858\n",
      "Epoch 201/299\n",
      "8.386117458343506\n",
      "Epoch 202/299\n",
      "8.37248969078064\n",
      "Epoch 203/299\n",
      "8.478072166442871\n",
      "Epoch 204/299\n",
      "8.349856615066528\n",
      "Epoch 205/299\n",
      "8.374884366989136\n",
      "Epoch 206/299\n",
      "8.315617084503174\n",
      "Epoch 207/299\n",
      "8.320769548416138\n",
      "Epoch 208/299\n",
      "8.401796579360962\n",
      "Epoch 209/299\n",
      "8.53974723815918\n",
      "Epoch 210/299\n",
      "8.377132177352905\n",
      "Epoch 211/299\n",
      "8.411155939102173\n",
      "Epoch 212/299\n",
      "8.379321575164795\n",
      "Epoch 213/299\n",
      "8.40476393699646\n",
      "Epoch 214/299\n",
      "8.358093023300171\n",
      "Epoch 215/299\n",
      "8.395983219146729\n",
      "Epoch 216/299\n",
      "8.56187105178833\n",
      "Epoch 217/299\n",
      "8.396137952804565\n",
      "Epoch 218/299\n",
      "8.437994003295898\n",
      "Epoch 219/299\n",
      "8.408305406570435\n",
      "Epoch 220/299\n",
      "8.343080043792725\n",
      "Epoch 221/299\n",
      "8.462401866912842\n",
      "Epoch 222/299\n",
      "8.371463775634766\n",
      "Epoch 223/299\n",
      "8.498793125152588\n",
      "Epoch 224/299\n",
      "8.358842611312866\n",
      "Epoch 225/299\n",
      "8.377400636672974\n",
      "Epoch 226/299\n",
      "8.419246673583984\n",
      "Epoch 227/299\n",
      "8.407389640808105\n",
      "Epoch 228/299\n",
      "8.348152875900269\n",
      "Epoch 229/299\n",
      "8.512654781341553\n",
      "Epoch 230/299\n",
      "8.456451654434204\n",
      "Epoch 231/299\n",
      "8.406323194503784\n",
      "Epoch 232/299\n",
      "8.401827812194824\n",
      "Epoch 233/299\n",
      "8.45762825012207\n",
      "Epoch 234/299\n",
      "8.343953609466553\n",
      "Epoch 235/299\n",
      "8.420233249664307\n",
      "Epoch 236/299\n",
      "8.494600534439087\n",
      "Epoch 237/299\n",
      "8.38308072090149\n",
      "Epoch 238/299\n",
      "8.429548263549805\n",
      "Epoch 239/299\n",
      "8.361638069152832\n",
      "Epoch 240/299\n",
      "8.378830909729004\n",
      "Epoch 241/299\n",
      "8.44658088684082\n",
      "Epoch 242/299\n",
      "8.449960708618164\n",
      "Epoch 243/299\n",
      "8.559671640396118\n",
      "Epoch 244/299\n",
      "8.402773380279541\n",
      "Epoch 245/299\n",
      "8.388842821121216\n",
      "Epoch 246/299\n",
      "8.369954109191895\n",
      "Epoch 247/299\n",
      "8.380436897277832\n",
      "Epoch 248/299\n",
      "8.399224996566772\n",
      "Epoch 249/299\n",
      "8.34698224067688\n",
      "Epoch 250/299\n",
      "8.501802206039429\n",
      "Epoch 251/299\n",
      "8.37069296836853\n",
      "Epoch 252/299\n",
      "8.404179811477661\n",
      "Epoch 253/299\n",
      "8.379420042037964\n",
      "Epoch 254/299\n",
      "8.407094478607178\n",
      "Epoch 255/299\n",
      "8.484704732894897\n",
      "Epoch 256/299\n",
      "8.604674100875854\n",
      "Epoch 257/299\n",
      "8.290185451507568\n",
      "Epoch 258/299\n",
      "8.458812475204468\n",
      "Epoch 259/299\n",
      "8.417757749557495\n",
      "Epoch 260/299\n",
      "8.447781562805176\n",
      "Epoch 261/299\n",
      "8.425088882446289\n",
      "Epoch 262/299\n",
      "8.419907331466675\n",
      "Epoch 263/299\n",
      "8.568982362747192\n",
      "Epoch 264/299\n",
      "8.361760377883911\n",
      "Epoch 265/299\n",
      "8.41367769241333\n",
      "Epoch 266/299\n",
      "8.39839792251587\n",
      "Epoch 267/299\n",
      "8.586332321166992\n",
      "Epoch 268/299\n",
      "8.580607414245605\n",
      "Epoch 269/299\n",
      "8.541772842407227\n",
      "Epoch 270/299\n",
      "8.483591079711914\n",
      "Epoch 271/299\n",
      "8.279321193695068\n",
      "Epoch 272/299\n",
      "8.449520826339722\n",
      "Epoch 273/299\n",
      "8.439420223236084\n",
      "Epoch 274/299\n",
      "8.452941417694092\n",
      "Epoch 275/299\n",
      "8.407615184783936\n",
      "Epoch 276/299\n",
      "8.513264656066895\n",
      "Epoch 277/299\n",
      "8.344570636749268\n",
      "Epoch 278/299\n",
      "8.430214643478394\n",
      "Epoch 279/299\n",
      "8.384832620620728\n",
      "Epoch 280/299\n",
      "8.411019563674927\n",
      "Epoch 281/299\n",
      "8.454060554504395\n",
      "Epoch 282/299\n",
      "8.362057209014893\n",
      "Epoch 283/299\n",
      "8.565035104751587\n",
      "Epoch 284/299\n",
      "8.400970935821533\n",
      "Epoch 285/299\n",
      "8.420435190200806\n",
      "Epoch 286/299\n",
      "8.460625886917114\n",
      "Epoch 287/299\n",
      "8.512541770935059\n",
      "Epoch 288/299\n",
      "8.48018217086792\n",
      "Epoch 289/299\n",
      "8.332088947296143\n",
      "Epoch 290/299\n",
      "8.605616569519043\n",
      "Epoch 291/299\n",
      "8.378664493560791\n",
      "Epoch 292/299\n",
      "8.454509735107422\n",
      "Epoch 293/299\n",
      "8.4286527633667\n",
      "Epoch 294/299\n",
      "8.468492269515991\n",
      "Epoch 295/299\n",
      "8.436838150024414\n",
      "Epoch 296/299\n",
      "8.564253568649292\n",
      "Epoch 297/299\n",
      "8.440773487091064\n",
      "Epoch 298/299\n",
      "8.362153053283691\n",
      "Epoch 299/299\n",
      "8.467800378799438\n",
      "Training complete in 41m 39s\n",
      "Best val Acc: 0.863300\n"
     ]
    }
   ],
   "source": [
    "resnet18_copy1 = copy.deepcopy(resnet18)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "resnet18_copy1.to(device)\n",
    "\n",
    "if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "    model_ft_org = torch.nn.DataParallel(resnet18_copy1)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "base_optimizer = optim.SGD\n",
    "optimizer = PUGDX(resnet18_copy1.parameters(),\n",
    "                base_optimizer,\n",
    "                lr=0.01,\n",
    "                momentum=args.momentum,\n",
    "                weight_decay=args.wd,\n",
    "                dampening=0,   # 必须设置为0才能完全固定 \n",
    "                nesterov=False # 禁用Nesterov动量 \n",
    "                 )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ft_epochs)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "resnet18_copy1, metrics0 = train_model(resnet18_copy1, criterion, optimizer, scheduler, ft_epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "model_path = \"./model/lfs/\"+args.datasets+\"/resnet18_pugd\" + str(ft_epochs) + \".pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': resnet18_copy1.state_dict(), \n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, model_path) \n",
    "\n",
    "name = \"./results/\"+args.datasets+\"/resnet18_pugd_\" + str(ft_epochs) + \".json\"\n",
    "with open(name,  'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86811ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "8.614818096160889\n",
      "Epoch 1/299\n",
      "8.484698057174683\n",
      "Epoch 2/299\n",
      "8.480557203292847\n",
      "Epoch 3/299\n",
      "8.60815143585205\n",
      "Epoch 4/299\n",
      "8.467992544174194\n",
      "Epoch 5/299\n",
      "8.501445293426514\n",
      "Epoch 6/299\n",
      "8.441718339920044\n",
      "Epoch 7/299\n",
      "8.481943845748901\n",
      "Epoch 8/299\n",
      "8.574955224990845\n",
      "Epoch 9/299\n",
      "8.59570574760437\n",
      "Epoch 10/299\n",
      "8.650724172592163\n",
      "Epoch 11/299\n",
      "8.547104597091675\n",
      "Epoch 12/299\n",
      "8.509070873260498\n",
      "Epoch 13/299\n",
      "8.433101415634155\n",
      "Epoch 14/299\n",
      "8.485283374786377\n",
      "Epoch 15/299\n",
      "8.47225284576416\n",
      "Epoch 16/299\n",
      "8.572571992874146\n",
      "Epoch 17/299\n",
      "8.42900276184082\n",
      "Epoch 18/299\n",
      "8.509242057800293\n",
      "Epoch 19/299\n",
      "8.580318450927734\n",
      "Epoch 20/299\n",
      "8.459119081497192\n",
      "Epoch 21/299\n",
      "8.548299789428711\n",
      "Epoch 22/299\n",
      "8.572463750839233\n",
      "Epoch 23/299\n",
      "8.702510595321655\n",
      "Epoch 24/299\n",
      "8.485719680786133\n",
      "Epoch 25/299\n",
      "8.448039293289185\n",
      "Epoch 26/299\n",
      "8.460539817810059\n",
      "Epoch 27/299\n",
      "8.445878982543945\n",
      "Epoch 28/299\n",
      "8.458104610443115\n",
      "Epoch 29/299\n",
      "8.526161432266235\n",
      "Epoch 30/299\n",
      "8.653056621551514\n",
      "Epoch 31/299\n",
      "8.476912021636963\n",
      "Epoch 32/299\n",
      "8.392372846603394\n",
      "Epoch 33/299\n",
      "8.471147775650024\n",
      "Epoch 34/299\n",
      "8.499444723129272\n",
      "Epoch 35/299\n",
      "8.45622205734253\n",
      "Epoch 36/299\n",
      "8.440365552902222\n",
      "Epoch 37/299\n",
      "8.616575241088867\n",
      "Epoch 38/299\n",
      "8.44497275352478\n",
      "Epoch 39/299\n",
      "8.511159181594849\n",
      "Epoch 40/299\n",
      "8.578059434890747\n",
      "Epoch 41/299\n",
      "8.431946039199829\n",
      "Epoch 42/299\n",
      "8.576977968215942\n",
      "Epoch 43/299\n",
      "8.57037091255188\n",
      "Epoch 44/299\n",
      "8.477928876876831\n",
      "Epoch 45/299\n",
      "8.427461862564087\n",
      "Epoch 46/299\n",
      "8.444629192352295\n",
      "Epoch 47/299\n",
      "8.535542726516724\n",
      "Epoch 48/299\n",
      "8.47614598274231\n",
      "Epoch 49/299\n",
      "8.448747158050537\n",
      "Epoch 50/299\n",
      "8.571189880371094\n",
      "Epoch 51/299\n",
      "8.45315957069397\n",
      "Epoch 52/299\n",
      "8.436950445175171\n",
      "Epoch 53/299\n",
      "8.46274471282959\n",
      "Epoch 54/299\n",
      "8.508414506912231\n",
      "Epoch 55/299\n",
      "8.506072044372559\n",
      "Epoch 56/299\n",
      "8.427997827529907\n",
      "Epoch 57/299\n",
      "8.60804295539856\n",
      "Epoch 58/299\n",
      "8.456764221191406\n",
      "Epoch 59/299\n",
      "8.51274037361145\n",
      "Epoch 60/299\n",
      "8.53838586807251\n",
      "Epoch 61/299\n",
      "8.582100868225098\n",
      "Epoch 62/299\n",
      "8.395929336547852\n",
      "Epoch 63/299\n",
      "8.682814836502075\n",
      "Epoch 64/299\n",
      "8.55513310432434\n",
      "Epoch 65/299\n",
      "8.570188283920288\n",
      "Epoch 66/299\n",
      "8.526281118392944\n",
      "Epoch 67/299\n",
      "8.474730014801025\n",
      "Epoch 68/299\n",
      "8.420505046844482\n",
      "Epoch 69/299\n",
      "8.411872386932373\n",
      "Epoch 70/299\n",
      "8.598761558532715\n",
      "Epoch 71/299\n",
      "8.45816683769226\n",
      "Epoch 72/299\n",
      "8.47029995918274\n",
      "Epoch 73/299\n",
      "8.436768293380737\n",
      "Epoch 74/299\n",
      "8.493939638137817\n",
      "Epoch 75/299\n",
      "8.477367877960205\n",
      "Epoch 76/299\n",
      "8.429712295532227\n",
      "Epoch 77/299\n",
      "8.568355083465576\n",
      "Epoch 78/299\n",
      "8.405944585800171\n",
      "Epoch 79/299\n",
      "8.351570129394531\n",
      "Epoch 80/299\n",
      "8.466415643692017\n",
      "Epoch 81/299\n",
      "8.43756103515625\n",
      "Epoch 82/299\n",
      "8.468437910079956\n",
      "Epoch 83/299\n",
      "8.592813491821289\n",
      "Epoch 84/299\n",
      "8.455891609191895\n",
      "Epoch 85/299\n",
      "8.466102361679077\n",
      "Epoch 86/299\n",
      "8.452258825302124\n",
      "Epoch 87/299\n",
      "8.443086624145508\n",
      "Epoch 88/299\n",
      "8.422110795974731\n",
      "Epoch 89/299\n",
      "8.461982488632202\n",
      "Epoch 90/299\n",
      "8.574517011642456\n",
      "Epoch 91/299\n",
      "8.412883996963501\n",
      "Epoch 92/299\n",
      "8.457160711288452\n",
      "Epoch 93/299\n",
      "8.438674449920654\n",
      "Epoch 94/299\n",
      "8.441222190856934\n",
      "Epoch 95/299\n",
      "8.442874670028687\n",
      "Epoch 96/299\n",
      "8.437358856201172\n",
      "Epoch 97/299\n",
      "8.592923641204834\n",
      "Epoch 98/299\n",
      "8.391707181930542\n",
      "Epoch 99/299\n",
      "8.49091362953186\n",
      "Epoch 100/299\n",
      "8.479520320892334\n",
      "Epoch 101/299\n",
      "8.432125568389893\n",
      "Epoch 102/299\n",
      "8.47052550315857\n",
      "Epoch 103/299\n",
      "8.660680055618286\n",
      "Epoch 104/299\n",
      "8.396898031234741\n",
      "Epoch 105/299\n",
      "8.421698808670044\n",
      "Epoch 106/299\n",
      "8.490579605102539\n",
      "Epoch 107/299\n",
      "8.372887134552002\n",
      "Epoch 108/299\n",
      "8.395015954971313\n",
      "Epoch 109/299\n",
      "8.46736192703247\n",
      "Epoch 110/299\n",
      "8.558165073394775\n",
      "Epoch 111/299\n",
      "8.338948011398315\n",
      "Epoch 112/299\n",
      "8.383495330810547\n",
      "Epoch 113/299\n",
      "8.475124835968018\n",
      "Epoch 114/299\n",
      "8.423306226730347\n",
      "Epoch 115/299\n",
      "8.397950649261475\n",
      "Epoch 116/299\n",
      "8.415238857269287\n",
      "Epoch 117/299\n",
      "8.50666880607605\n",
      "Epoch 118/299\n",
      "8.418310165405273\n",
      "Epoch 119/299\n",
      "8.3833749294281\n",
      "Epoch 120/299\n",
      "8.401665687561035\n",
      "Epoch 121/299\n",
      "8.491253852844238\n",
      "Epoch 122/299\n",
      "8.42184853553772\n",
      "Epoch 123/299\n",
      "8.548391819000244\n",
      "Epoch 124/299\n",
      "8.513187408447266\n",
      "Epoch 125/299\n",
      "8.517452716827393\n",
      "Epoch 126/299\n",
      "8.410244226455688\n",
      "Epoch 127/299\n",
      "8.452034950256348\n",
      "Epoch 128/299\n",
      "8.436192750930786\n",
      "Epoch 129/299\n",
      "8.432554244995117\n",
      "Epoch 130/299\n",
      "8.565629005432129\n",
      "Epoch 131/299\n",
      "8.432174682617188\n",
      "Epoch 132/299\n",
      "8.420953035354614\n",
      "Epoch 133/299\n",
      "8.386529445648193\n",
      "Epoch 134/299\n",
      "8.460885524749756\n",
      "Epoch 135/299\n",
      "8.469115257263184\n",
      "Epoch 136/299\n",
      "8.371962785720825\n",
      "Epoch 137/299\n",
      "8.528393745422363\n",
      "Epoch 138/299\n",
      "8.487236738204956\n",
      "Epoch 139/299\n",
      "8.409935474395752\n",
      "Epoch 140/299\n",
      "8.451090574264526\n",
      "Epoch 141/299\n",
      "8.400906085968018\n",
      "Epoch 142/299\n",
      "8.413792133331299\n",
      "Epoch 143/299\n",
      "8.505051136016846\n",
      "Epoch 144/299\n",
      "8.451472043991089\n",
      "Epoch 145/299\n",
      "8.450971841812134\n",
      "Epoch 146/299\n",
      "8.382720947265625\n",
      "Epoch 147/299\n",
      "8.440107583999634\n",
      "Epoch 148/299\n",
      "8.402661561965942\n",
      "Epoch 149/299\n",
      "8.409749984741211\n",
      "Epoch 150/299\n",
      "8.625021696090698\n",
      "Epoch 151/299\n",
      "8.397338151931763\n",
      "Epoch 152/299\n",
      "8.459457159042358\n",
      "Epoch 153/299\n",
      "8.424128293991089\n",
      "Epoch 154/299\n",
      "8.408003807067871\n",
      "Epoch 155/299\n",
      "8.396421194076538\n",
      "Epoch 156/299\n",
      "8.465418577194214\n",
      "Epoch 157/299\n",
      "8.567631006240845\n",
      "Epoch 158/299\n",
      "8.40260934829712\n",
      "Epoch 159/299\n",
      "8.39090347290039\n",
      "Epoch 160/299\n",
      "8.391581535339355\n",
      "Epoch 161/299\n",
      "8.432419776916504\n",
      "Epoch 162/299\n",
      "8.420001745223999\n",
      "Epoch 163/299\n",
      "8.562354564666748\n",
      "Epoch 164/299\n",
      "8.42955470085144\n",
      "Epoch 165/299\n",
      "8.447344064712524\n",
      "Epoch 166/299\n",
      "8.434310674667358\n",
      "Epoch 167/299\n",
      "8.455782890319824\n",
      "Epoch 168/299\n",
      "8.441160917282104\n",
      "Epoch 169/299\n",
      "8.435143232345581\n",
      "Epoch 170/299\n",
      "8.553581476211548\n",
      "Epoch 171/299\n",
      "8.47072958946228\n",
      "Epoch 172/299\n",
      "8.428704977035522\n",
      "Epoch 173/299\n",
      "8.421733856201172\n",
      "Epoch 174/299\n",
      "8.46176791191101\n",
      "Epoch 175/299\n",
      "8.537432432174683\n",
      "Epoch 176/299\n",
      "8.632812023162842\n",
      "Epoch 177/299\n",
      "8.688867092132568\n",
      "Epoch 178/299\n",
      "8.483569622039795\n",
      "Epoch 179/299\n",
      "8.38124418258667\n",
      "Epoch 180/299\n",
      "8.435189723968506\n",
      "Epoch 181/299\n",
      "8.43799901008606\n",
      "Epoch 182/299\n",
      "8.369828939437866\n",
      "Epoch 183/299\n",
      "8.535743951797485\n",
      "Epoch 184/299\n",
      "8.525318622589111\n",
      "Epoch 185/299\n",
      "8.446803092956543\n",
      "Epoch 186/299\n",
      "8.304755449295044\n",
      "Epoch 187/299\n",
      "8.4457848072052\n",
      "Epoch 188/299\n",
      "8.321313858032227\n",
      "Epoch 189/299\n",
      "8.551830291748047\n",
      "Epoch 190/299\n",
      "8.63621211051941\n",
      "Epoch 191/299\n",
      "8.465386390686035\n",
      "Epoch 192/299\n",
      "8.459248781204224\n",
      "Epoch 193/299\n",
      "8.418407440185547\n",
      "Epoch 194/299\n",
      "8.450059652328491\n",
      "Epoch 195/299\n",
      "8.470132827758789\n",
      "Epoch 196/299\n",
      "8.375951051712036\n",
      "Epoch 197/299\n",
      "8.57844352722168\n",
      "Epoch 198/299\n",
      "8.46571397781372\n",
      "Epoch 199/299\n",
      "8.40108036994934\n",
      "Epoch 200/299\n",
      "8.414094924926758\n",
      "Epoch 201/299\n",
      "8.39433240890503\n",
      "Epoch 202/299\n",
      "8.457950830459595\n",
      "Epoch 203/299\n",
      "8.531099557876587\n",
      "Epoch 204/299\n",
      "8.437516212463379\n",
      "Epoch 205/299\n",
      "8.38152003288269\n",
      "Epoch 206/299\n",
      "8.43604302406311\n",
      "Epoch 207/299\n",
      "8.545052766799927\n",
      "Epoch 208/299\n",
      "8.446675777435303\n",
      "Epoch 209/299\n",
      "8.518803358078003\n",
      "Epoch 210/299\n",
      "8.630459308624268\n",
      "Epoch 211/299\n",
      "8.478723049163818\n",
      "Epoch 212/299\n",
      "8.422898530960083\n",
      "Epoch 213/299\n",
      "8.490736484527588\n",
      "Epoch 214/299\n",
      "8.490539789199829\n",
      "Epoch 215/299\n",
      "8.441177129745483\n",
      "Epoch 216/299\n",
      "8.426965236663818\n",
      "Epoch 217/299\n",
      "8.63628888130188\n",
      "Epoch 218/299\n",
      "8.530917167663574\n",
      "Epoch 219/299\n",
      "8.503923416137695\n",
      "Epoch 220/299\n",
      "8.397292613983154\n",
      "Epoch 221/299\n",
      "8.426296710968018\n",
      "Epoch 222/299\n",
      "8.487200736999512\n",
      "Epoch 223/299\n",
      "8.449383020401001\n",
      "Epoch 224/299\n",
      "8.577025413513184\n",
      "Epoch 225/299\n",
      "8.425815343856812\n",
      "Epoch 226/299\n",
      "8.573493719100952\n",
      "Epoch 227/299\n",
      "8.430267333984375\n",
      "Epoch 228/299\n",
      "8.48043417930603\n",
      "Epoch 229/299\n",
      "8.52910852432251\n",
      "Epoch 230/299\n",
      "8.703733205795288\n",
      "Epoch 231/299\n",
      "8.439891815185547\n",
      "Epoch 232/299\n",
      "8.538763046264648\n",
      "Epoch 233/299\n",
      "8.59956669807434\n",
      "Epoch 234/299\n",
      "8.540185928344727\n",
      "Epoch 235/299\n",
      "8.50251293182373\n",
      "Epoch 236/299\n",
      "8.428352355957031\n",
      "Epoch 237/299\n",
      "8.664562225341797\n",
      "Epoch 238/299\n",
      "8.603581666946411\n",
      "Epoch 239/299\n",
      "8.489309310913086\n",
      "Epoch 240/299\n",
      "8.526196479797363\n",
      "Epoch 241/299\n",
      "8.44516897201538\n",
      "Epoch 242/299\n",
      "8.45783019065857\n",
      "Epoch 243/299\n",
      "8.470898389816284\n",
      "Epoch 244/299\n",
      "8.620383262634277\n",
      "Epoch 245/299\n",
      "8.451860189437866\n",
      "Epoch 246/299\n",
      "8.527515411376953\n",
      "Epoch 247/299\n",
      "8.519108057022095\n",
      "Epoch 248/299\n",
      "8.508530616760254\n",
      "Epoch 249/299\n",
      "8.517984867095947\n",
      "Epoch 250/299\n",
      "8.475000381469727\n",
      "Epoch 251/299\n",
      "8.575700759887695\n",
      "Epoch 252/299\n",
      "8.57480263710022\n",
      "Epoch 253/299\n",
      "8.601155757904053\n",
      "Epoch 254/299\n",
      "8.50295090675354\n",
      "Epoch 255/299\n",
      "8.510103940963745\n",
      "Epoch 256/299\n",
      "8.510547637939453\n",
      "Epoch 257/299\n",
      "8.66890263557434\n",
      "Epoch 258/299\n",
      "8.58674168586731\n",
      "Epoch 259/299\n",
      "8.487408638000488\n",
      "Epoch 260/299\n",
      "8.438868761062622\n",
      "Epoch 261/299\n",
      "8.538028240203857\n",
      "Epoch 262/299\n",
      "8.481088638305664\n",
      "Epoch 263/299\n",
      "8.54840612411499\n",
      "Epoch 264/299\n",
      "8.543859958648682\n",
      "Epoch 265/299\n",
      "8.514560461044312\n",
      "Epoch 266/299\n",
      "8.473464727401733\n",
      "Epoch 267/299\n",
      "8.450524806976318\n",
      "Epoch 268/299\n",
      "8.479752779006958\n",
      "Epoch 269/299\n",
      "8.536696434020996\n",
      "Epoch 270/299\n",
      "8.48167610168457\n",
      "Epoch 271/299\n",
      "8.604267835617065\n",
      "Epoch 272/299\n",
      "8.475974082946777\n",
      "Epoch 273/299\n",
      "8.477075338363647\n",
      "Epoch 274/299\n",
      "8.434277772903442\n",
      "Epoch 275/299\n",
      "8.472134351730347\n",
      "Epoch 276/299\n",
      "8.481239080429077\n",
      "Epoch 277/299\n",
      "8.537453413009644\n",
      "Epoch 278/299\n",
      "8.610564231872559\n",
      "Epoch 279/299\n",
      "8.47462272644043\n",
      "Epoch 280/299\n",
      "8.498581647872925\n",
      "Epoch 281/299\n",
      "8.516761064529419\n",
      "Epoch 282/299\n",
      "8.480824947357178\n",
      "Epoch 283/299\n",
      "8.486296653747559\n",
      "Epoch 284/299\n",
      "8.650433778762817\n",
      "Epoch 285/299\n",
      "8.444534540176392\n",
      "Epoch 286/299\n",
      "8.532931804656982\n",
      "Epoch 287/299\n",
      "8.5216646194458\n",
      "Epoch 288/299\n",
      "8.540068864822388\n",
      "Epoch 289/299\n",
      "8.558780193328857\n",
      "Epoch 290/299\n",
      "8.637847900390625\n",
      "Epoch 291/299\n",
      "8.459765672683716\n",
      "Epoch 292/299\n",
      "8.51137375831604\n",
      "Epoch 293/299\n",
      "8.440912961959839\n",
      "Epoch 294/299\n",
      "8.471157312393188\n",
      "Epoch 295/299\n",
      "8.589210033416748\n",
      "Epoch 296/299\n",
      "8.450524091720581\n",
      "Epoch 297/299\n",
      "8.49841022491455\n",
      "Epoch 298/299\n",
      "8.709190368652344\n",
      "Epoch 299/299\n",
      "8.452986717224121\n",
      "Training complete in 42m 26s\n",
      "Best val Acc: 0.866000\n"
     ]
    }
   ],
   "source": [
    "resnet18_copy1 = copy.deepcopy(resnet18)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "resnet18_copy1.to(device)\n",
    "\n",
    "if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "    model_ft_org = torch.nn.DataParallel(resnet18_copy1)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "base_optimizer = optim.SGD\n",
    "optimizer = PUGDXR(resnet18_copy1.parameters(),\n",
    "                base_optimizer,\n",
    "                lr=0.01,\n",
    "                max_epochs= args.epochs,\n",
    "                momentum=args.momentum,\n",
    "                weight_decay=args.wd,\n",
    "                min_beta = 0.5, \n",
    "                max_beta = 1, \n",
    "                method = 'isin',\n",
    "                dampening=0,   # 必须设置为0才能完全固定 \n",
    "                nesterov=False # 禁用Nesterov动量 \n",
    "                )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ft_epochs)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "resnet18_copy1, metrics0 = train_model(resnet18_copy1, criterion, optimizer, scheduler, ft_epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "model_path = \"./model/lfs/\"+args.datasets+\"/resnet18_pugdr_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(ft_epochs) + \".pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': resnet18_copy1.state_dict(), \n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, model_path) \n",
    "\n",
    "name = \"./results/\"+args.datasets+\"/resnet18_pugdr_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" +  str(ft_epochs) + \".json\"\n",
    "with open(name,  'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1eab5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "ft_epochs = 200\n",
    "# 加载预训练ViT-B/16\n",
    "vit = timm.create_model('google/vit_base_patch16_224',  pretrained=True, \n",
    "                        img_size = 32, patch_size = 8, num_classes = Num_class)\n",
    "\n",
    "# vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224',\n",
    "#                                                 img_size = 32, patch_size = 8, num_classes = Num_class)\n",
    "# 冻结除head外的所有层\n",
    "# for name, param in vit.named_parameters(): \n",
    "#     if 'heads.head'  not in name:  # 仅保留分类头可训练\n",
    "#         param.requires_grad  = False\n",
    "\n",
    "# vit.heads.head  = torch.nn.Linear(vit.heads.head.in_features,  Num_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b21be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "87.27649283409119\n",
      "Epoch 1/199\n",
      "87.01743006706238\n",
      "Epoch 2/199\n",
      "86.75871515274048\n",
      "Epoch 3/199\n",
      "86.80527639389038\n",
      "Epoch 4/199\n",
      "86.87925100326538\n",
      "Epoch 5/199\n",
      "87.11076617240906\n",
      "Epoch 6/199\n",
      "87.28419828414917\n",
      "Epoch 7/199\n",
      "87.24719214439392\n",
      "Epoch 8/199\n",
      "87.49161767959595\n",
      "Epoch 9/199\n",
      "87.58965992927551\n",
      "Epoch 10/199\n",
      "87.84839344024658\n",
      "Epoch 11/199\n",
      "87.69825434684753\n",
      "Epoch 12/199\n",
      "87.65998148918152\n",
      "Epoch 13/199\n",
      "87.66978287696838\n",
      "Epoch 14/199\n",
      "87.75122427940369\n",
      "Epoch 15/199\n",
      "87.65101051330566\n",
      "Epoch 16/199\n",
      "87.6350781917572\n",
      "Epoch 17/199\n",
      "87.27856397628784\n",
      "Epoch 18/199\n",
      "87.18223071098328\n",
      "Epoch 19/199\n",
      "87.21493172645569\n",
      "Epoch 20/199\n",
      "87.33242893218994\n",
      "Epoch 21/199\n",
      "87.37593650817871\n",
      "Epoch 22/199\n",
      "87.49761199951172\n",
      "Epoch 23/199\n",
      "87.66464948654175\n",
      "Epoch 24/199\n",
      "87.74119424819946\n",
      "Epoch 25/199\n",
      "87.80505013465881\n",
      "Epoch 26/199\n",
      "87.68211770057678\n",
      "Epoch 27/199\n",
      "87.76371812820435\n",
      "Epoch 28/199\n",
      "87.77299094200134\n",
      "Epoch 29/199\n",
      "88.03620386123657\n",
      "Epoch 30/199\n",
      "87.74037075042725\n",
      "Epoch 31/199\n",
      "87.69756507873535\n",
      "Epoch 32/199\n",
      "87.83534598350525\n",
      "Epoch 33/199\n",
      "87.83723187446594\n",
      "Epoch 34/199\n",
      "87.5058822631836\n",
      "Epoch 35/199\n",
      "87.39952540397644\n",
      "Epoch 36/199\n",
      "87.35906386375427\n",
      "Epoch 37/199\n",
      "87.16549634933472\n",
      "Epoch 38/199\n",
      "87.17731857299805\n",
      "Epoch 39/199\n",
      "87.06703281402588\n",
      "Epoch 40/199\n",
      "86.97811722755432\n",
      "Epoch 41/199\n",
      "87.01388478279114\n",
      "Epoch 42/199\n",
      "86.96571660041809\n",
      "Epoch 43/199\n",
      "87.08133220672607\n",
      "Epoch 44/199\n",
      "87.2497923374176\n",
      "Epoch 45/199\n",
      "87.4693374633789\n",
      "Epoch 46/199\n",
      "87.48901653289795\n",
      "Epoch 47/199\n",
      "87.52120614051819\n",
      "Epoch 48/199\n",
      "87.49032020568848\n",
      "Epoch 49/199\n",
      "87.7910795211792\n",
      "Epoch 50/199\n",
      "87.66893792152405\n",
      "Epoch 51/199\n",
      "87.75001096725464\n",
      "Epoch 52/199\n",
      "87.71255207061768\n",
      "Epoch 53/199\n",
      "87.73725986480713\n",
      "Epoch 54/199\n",
      "87.77704572677612\n",
      "Epoch 55/199\n",
      "87.90090942382812\n",
      "Epoch 56/199\n",
      "87.83429026603699\n",
      "Epoch 57/199\n",
      "87.75368571281433\n",
      "Epoch 58/199\n",
      "87.86356782913208\n",
      "Epoch 59/199\n",
      "87.93364405632019\n",
      "Epoch 60/199\n",
      "87.87471508979797\n",
      "Epoch 61/199\n",
      "87.84390592575073\n",
      "Epoch 62/199\n",
      "88.00743007659912\n",
      "Epoch 63/199\n",
      "87.8807954788208\n",
      "Epoch 64/199\n",
      "87.88547372817993\n",
      "Epoch 65/199\n",
      "87.83382177352905\n",
      "Epoch 66/199\n",
      "87.85556960105896\n",
      "Epoch 67/199\n",
      "87.97417569160461\n",
      "Epoch 68/199\n",
      "87.99849128723145\n",
      "Epoch 69/199\n",
      "87.94142031669617\n",
      "Epoch 70/199\n",
      "88.02337288856506\n",
      "Epoch 71/199\n",
      "87.93319153785706\n",
      "Epoch 72/199\n",
      "87.84404063224792\n",
      "Epoch 73/199\n",
      "87.76922655105591\n",
      "Epoch 74/199\n",
      "87.57170176506042\n",
      "Epoch 75/199\n",
      "87.34566283226013\n",
      "Epoch 76/199\n",
      "87.17343592643738\n",
      "Epoch 77/199\n",
      "87.16712999343872\n",
      "Epoch 78/199\n",
      "87.16485643386841\n",
      "Epoch 79/199\n",
      "87.28541612625122\n",
      "Epoch 80/199\n",
      "87.26789426803589\n",
      "Epoch 81/199\n",
      "87.58060908317566\n",
      "Epoch 82/199\n",
      "87.68125796318054\n",
      "Epoch 83/199\n",
      "87.57932591438293\n",
      "Epoch 84/199\n",
      "87.77438521385193\n",
      "Epoch 85/199\n",
      "87.81331181526184\n",
      "Epoch 86/199\n",
      "87.76998901367188\n",
      "Epoch 87/199\n",
      "87.89696764945984\n",
      "Epoch 88/199\n",
      "87.84129524230957\n",
      "Epoch 89/199\n",
      "87.82355880737305\n",
      "Epoch 90/199\n",
      "87.96495723724365\n",
      "Epoch 91/199\n",
      "87.86050200462341\n",
      "Epoch 92/199\n",
      "87.89247727394104\n",
      "Epoch 93/199\n",
      "87.84362006187439\n",
      "Epoch 94/199\n",
      "88.0434308052063\n",
      "Epoch 95/199\n",
      "87.8845579624176\n",
      "Epoch 96/199\n",
      "87.88333010673523\n",
      "Epoch 97/199\n",
      "87.97942662239075\n",
      "Epoch 98/199\n",
      "87.92117428779602\n",
      "Epoch 99/199\n",
      "87.95312404632568\n",
      "Epoch 100/199\n",
      "88.12631034851074\n",
      "Epoch 101/199\n",
      "87.94098687171936\n",
      "Epoch 102/199\n",
      "88.06141710281372\n",
      "Epoch 103/199\n",
      "88.06953883171082\n",
      "Epoch 104/199\n",
      "87.9589033126831\n",
      "Epoch 105/199\n",
      "87.96891355514526\n",
      "Epoch 106/199\n",
      "87.91544198989868\n",
      "Epoch 107/199\n",
      "88.01849865913391\n",
      "Epoch 108/199\n",
      "87.67993211746216\n",
      "Epoch 109/199\n",
      "87.55977201461792\n",
      "Epoch 110/199\n",
      "87.50406861305237\n",
      "Epoch 111/199\n",
      "87.54646921157837\n",
      "Epoch 112/199\n",
      "87.5801682472229\n",
      "Epoch 113/199\n",
      "87.88410663604736\n",
      "Epoch 114/199\n",
      "87.76731848716736\n",
      "Epoch 115/199\n",
      "87.79195427894592\n",
      "Epoch 116/199\n",
      "87.69853019714355\n",
      "Epoch 117/199\n",
      "87.84974980354309\n",
      "Epoch 118/199\n",
      "87.93372225761414\n",
      "Epoch 119/199\n",
      "87.95399689674377\n",
      "Epoch 120/199\n",
      "88.14449906349182\n",
      "Epoch 121/199\n",
      "87.80575633049011\n",
      "Epoch 122/199\n",
      "87.61425042152405\n",
      "Epoch 123/199\n",
      "87.36675143241882\n",
      "Epoch 124/199\n",
      "87.37908101081848\n",
      "Epoch 125/199\n",
      "87.13893055915833\n",
      "Epoch 126/199\n",
      "87.22003555297852\n",
      "Epoch 127/199\n",
      "86.98252534866333\n",
      "Epoch 128/199\n",
      "86.92519497871399\n",
      "Epoch 129/199\n",
      "86.95142579078674\n",
      "Epoch 130/199\n",
      "86.92893767356873\n",
      "Epoch 131/199\n",
      "86.86859226226807\n",
      "Epoch 132/199\n",
      "86.79566979408264\n",
      "Epoch 133/199\n",
      "86.96283459663391\n",
      "Epoch 134/199\n",
      "86.85492825508118\n",
      "Epoch 135/199\n",
      "86.68226552009583\n",
      "Epoch 136/199\n",
      "86.74158334732056\n",
      "Epoch 137/199\n",
      "86.760751247406\n",
      "Epoch 138/199\n",
      "86.72211933135986\n",
      "Epoch 139/199\n",
      "86.91567206382751\n",
      "Epoch 140/199\n",
      "86.65126705169678\n",
      "Epoch 141/199\n",
      "86.69571661949158\n",
      "Epoch 142/199\n",
      "86.6828396320343\n",
      "Epoch 143/199\n",
      "86.64152550697327\n",
      "Epoch 144/199\n",
      "86.67836236953735\n",
      "Epoch 145/199\n",
      "86.48497343063354\n",
      "Epoch 146/199\n",
      "86.69780254364014\n",
      "Epoch 147/199\n",
      "86.506023645401\n",
      "Epoch 148/199\n",
      "86.59839105606079\n",
      "Epoch 149/199\n",
      "86.5363998413086\n",
      "Epoch 150/199\n",
      "86.57808709144592\n",
      "Epoch 151/199\n",
      "86.60988759994507\n",
      "Epoch 152/199\n",
      "86.76178216934204\n",
      "Epoch 153/199\n",
      "86.58195114135742\n",
      "Epoch 154/199\n",
      "86.5796709060669\n",
      "Epoch 155/199\n",
      "86.62379431724548\n",
      "Epoch 156/199\n",
      "86.65799617767334\n",
      "Epoch 157/199\n",
      "86.74140119552612\n",
      "Epoch 158/199\n",
      "86.62070059776306\n",
      "Epoch 159/199\n",
      "86.77770352363586\n",
      "Epoch 160/199\n",
      "86.58494162559509\n",
      "Epoch 161/199\n",
      "86.50411534309387\n",
      "Epoch 162/199\n",
      "86.57752251625061\n",
      "Epoch 163/199\n",
      "86.56176090240479\n",
      "Epoch 164/199\n",
      "86.52112436294556\n",
      "Epoch 165/199\n",
      "86.70176005363464\n",
      "Epoch 166/199\n",
      "86.4009530544281\n",
      "Epoch 167/199\n",
      "86.50311636924744\n",
      "Epoch 168/199\n",
      "86.88035607337952\n",
      "Epoch 169/199\n",
      "86.99847364425659\n",
      "Epoch 170/199\n",
      "87.15336513519287\n",
      "Epoch 171/199\n",
      "87.25081658363342\n",
      "Epoch 172/199\n",
      "87.32582974433899\n",
      "Epoch 173/199\n",
      "87.27591609954834\n",
      "Epoch 174/199\n",
      "87.41045188903809\n",
      "Epoch 175/199\n",
      "87.41480112075806\n",
      "Epoch 176/199\n",
      "87.36042928695679\n",
      "Epoch 177/199\n",
      "87.38495898246765\n",
      "Epoch 178/199\n",
      "87.6235146522522\n",
      "Epoch 179/199\n",
      "87.41081166267395\n",
      "Epoch 180/199\n",
      "87.0907928943634\n",
      "Epoch 181/199\n",
      "87.079185962677\n",
      "Epoch 182/199\n",
      "86.84051847457886\n",
      "Epoch 183/199\n",
      "86.79991555213928\n",
      "Epoch 184/199\n",
      "86.76067566871643\n",
      "Epoch 185/199\n",
      "86.87975263595581\n",
      "Epoch 186/199\n",
      "86.72466611862183\n",
      "Epoch 187/199\n",
      "86.76260662078857\n",
      "Epoch 188/199\n",
      "86.62003135681152\n",
      "Epoch 189/199\n",
      "86.62945938110352\n",
      "Epoch 190/199\n",
      "86.59097695350647\n",
      "Epoch 191/199\n",
      "86.68155550956726\n",
      "Epoch 192/199\n",
      "86.58247900009155\n",
      "Epoch 193/199\n",
      "86.54246187210083\n",
      "Epoch 194/199\n",
      "86.56243228912354\n",
      "Epoch 195/199\n",
      "86.80064153671265\n",
      "Epoch 196/199\n",
      "86.88280963897705\n",
      "Epoch 197/199\n",
      "87.12177920341492\n",
      "Epoch 198/199\n",
      "87.42708277702332\n",
      "Epoch 199/199\n",
      "87.2952835559845\n",
      "Training complete in 291m 8s\n",
      "Best val Acc: 0.965900\n"
     ]
    }
   ],
   "source": [
    "vit_copy1 = copy.deepcopy(vit)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "vit_copy1.to(device)\n",
    "\n",
    "if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "    model_ft_org = torch.nn.DataParallel(vit_copy1)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "base_optimizer = optim.SGD\n",
    "optimizer = PUGDX(vit_copy1.parameters(),\n",
    "                base_optimizer,\n",
    "                lr=0.001,\n",
    "                momentum=args.momentum,\n",
    "                weight_decay=args.wd,\n",
    "                dampening=0,   # 必须设置为0才能完全固定 \n",
    "                nesterov=False # 禁用Nesterov动量 \n",
    "                 )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ft_epochs)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "vit_copy1, metrics0 = train_model(vit_copy1, criterion, optimizer, scheduler, ft_epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "model_path = \"./model/lfs/\"+args.datasets+\"/vit_pugd\" + str(ft_epochs) + \".pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': vit_copy1.state_dict(), \n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, model_path) \n",
    "\n",
    "name = \"./results/\"+args.datasets+\"/vit_pugd_\" + str(ft_epochs) + \".json\"\n",
    "with open(name,  'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "87.43074703216553\n",
      "Epoch 1/199\n",
      "87.50903058052063\n",
      "Epoch 2/199\n",
      "87.60423803329468\n",
      "Epoch 3/199\n",
      "87.55445718765259\n",
      "Epoch 4/199\n",
      "87.74158883094788\n",
      "Epoch 5/199\n",
      "87.70942878723145\n",
      "Epoch 6/199\n",
      "87.8306143283844\n",
      "Epoch 7/199\n",
      "87.64871406555176\n",
      "Epoch 8/199\n",
      "87.71636486053467\n",
      "Epoch 9/199\n",
      "87.82065057754517\n",
      "Epoch 10/199\n",
      "87.91743326187134\n",
      "Epoch 11/199\n",
      "87.72715997695923\n",
      "Epoch 12/199\n",
      "87.8803722858429\n",
      "Epoch 13/199\n",
      "87.81188583374023\n",
      "Epoch 14/199\n",
      "87.8195013999939\n",
      "Epoch 15/199\n",
      "87.93416428565979\n",
      "Epoch 16/199\n",
      "87.93970823287964\n",
      "Epoch 17/199\n",
      "87.94647550582886\n",
      "Epoch 18/199\n",
      "87.90225553512573\n",
      "Epoch 19/199\n",
      "87.79440569877625\n",
      "Epoch 20/199\n",
      "87.80140924453735\n",
      "Epoch 21/199\n",
      "87.73399066925049\n",
      "Epoch 22/199\n",
      "87.70495939254761\n",
      "Epoch 23/199\n",
      "87.96953701972961\n",
      "Epoch 24/199\n",
      "87.89654684066772\n",
      "Epoch 25/199\n",
      "87.89340591430664\n",
      "Epoch 26/199\n",
      "87.87587666511536\n",
      "Epoch 27/199\n",
      "87.9213433265686\n",
      "Epoch 28/199\n",
      "87.7792444229126\n",
      "Epoch 29/199\n",
      "87.87321782112122\n",
      "Epoch 30/199\n",
      "87.28893280029297\n",
      "Epoch 31/199\n",
      "87.11319947242737\n",
      "Epoch 32/199\n",
      "87.18629837036133\n",
      "Epoch 33/199\n",
      "86.98304224014282\n",
      "Epoch 34/199\n",
      "87.006911277771\n",
      "Epoch 35/199\n",
      "87.01739931106567\n",
      "Epoch 36/199\n",
      "87.08023238182068\n",
      "Epoch 37/199\n",
      "86.88517665863037\n",
      "Epoch 38/199\n",
      "86.8801896572113\n",
      "Epoch 39/199\n",
      "86.9027316570282\n",
      "Epoch 40/199\n",
      "86.85859370231628\n",
      "Epoch 41/199\n",
      "86.84663987159729\n",
      "Epoch 42/199\n",
      "86.95524263381958\n",
      "Epoch 43/199\n",
      "86.79569411277771\n",
      "Epoch 44/199\n",
      "86.76314973831177\n",
      "Epoch 45/199\n",
      "86.80384135246277\n",
      "Epoch 46/199\n",
      "86.7793664932251\n",
      "Epoch 47/199\n",
      "86.78458285331726\n",
      "Epoch 48/199\n",
      "86.719233751297\n",
      "Epoch 49/199\n",
      "87.07156658172607\n",
      "Epoch 50/199\n",
      "87.0303041934967\n",
      "Epoch 51/199\n",
      "87.1984715461731\n",
      "Epoch 52/199\n",
      "87.28658175468445\n",
      "Epoch 53/199\n",
      "87.47789406776428\n",
      "Epoch 54/199\n",
      "87.36718416213989\n",
      "Epoch 55/199\n",
      "87.71647810935974\n",
      "Epoch 56/199\n",
      "87.54435729980469\n",
      "Epoch 57/199\n",
      "87.46136355400085\n",
      "Epoch 58/199\n",
      "87.15969324111938\n",
      "Epoch 59/199\n",
      "86.98767042160034\n",
      "Epoch 60/199\n",
      "86.91836810112\n",
      "Epoch 61/199\n",
      "86.88453793525696\n",
      "Epoch 62/199\n",
      "87.1905128955841\n",
      "Epoch 63/199\n",
      "87.2260046005249\n",
      "Epoch 64/199\n",
      "87.41723728179932\n",
      "Epoch 65/199\n",
      "87.38988208770752\n",
      "Epoch 66/199\n",
      "87.43697786331177\n",
      "Epoch 67/199\n",
      "87.52000951766968\n",
      "Epoch 68/199\n",
      "87.74732899665833\n",
      "Epoch 69/199\n",
      "87.65973806381226\n",
      "Epoch 70/199\n",
      "87.56113171577454\n",
      "Epoch 71/199\n",
      "87.68014335632324\n",
      "Epoch 72/199\n",
      "87.56213474273682\n",
      "Epoch 73/199\n",
      "87.69574642181396\n",
      "Epoch 74/199\n",
      "87.67122840881348\n",
      "Epoch 75/199\n",
      "87.35110282897949\n",
      "Epoch 76/199\n",
      "87.06427216529846\n",
      "Epoch 77/199\n",
      "86.96449494361877\n",
      "Epoch 78/199\n",
      "86.95531606674194\n",
      "Epoch 79/199\n",
      "86.93219304084778\n",
      "Epoch 80/199\n",
      "86.9700243473053\n",
      "Epoch 81/199\n",
      "87.30390977859497\n",
      "Epoch 82/199\n",
      "87.33471775054932\n",
      "Epoch 83/199\n",
      "87.40890979766846\n",
      "Epoch 84/199\n",
      "87.4968843460083\n",
      "Epoch 85/199\n",
      "87.49933505058289\n",
      "Epoch 86/199\n",
      "87.55982685089111\n",
      "Epoch 87/199\n",
      "87.85072040557861\n",
      "Epoch 88/199\n",
      "87.6235339641571\n",
      "Epoch 89/199\n",
      "87.67674899101257\n",
      "Epoch 90/199\n",
      "87.74591827392578\n",
      "Epoch 91/199\n",
      "87.66545748710632\n",
      "Epoch 92/199\n",
      "87.73141312599182\n",
      "Epoch 93/199\n",
      "87.6867322921753\n",
      "Epoch 94/199\n",
      "87.88666009902954\n",
      "Epoch 95/199\n",
      "87.69554567337036\n",
      "Epoch 96/199\n",
      "87.49850845336914\n",
      "Epoch 97/199\n",
      "87.2006573677063\n",
      "Epoch 98/199\n",
      "87.1449887752533\n",
      "Epoch 99/199\n",
      "87.03591704368591\n",
      "Epoch 100/199\n",
      "87.05822134017944\n",
      "Epoch 101/199\n",
      "86.8444995880127\n",
      "Epoch 102/199\n",
      "86.84388661384583\n",
      "Epoch 103/199\n",
      "86.81951832771301\n",
      "Epoch 104/199\n",
      "86.80506253242493\n",
      "Epoch 105/199\n",
      "86.81265568733215\n",
      "Epoch 106/199\n",
      "86.86118030548096\n",
      "Epoch 107/199\n",
      "86.94123816490173\n",
      "Epoch 108/199\n",
      "87.02369952201843\n",
      "Epoch 109/199\n",
      "87.24026799201965\n",
      "Epoch 110/199\n",
      "87.14431810379028\n",
      "Epoch 111/199\n",
      "87.34967827796936\n",
      "Epoch 112/199\n",
      "87.43493795394897\n",
      "Epoch 113/199\n",
      "87.5224142074585\n",
      "Epoch 114/199\n",
      "87.48399043083191\n",
      "Epoch 115/199\n",
      "87.58485436439514\n",
      "Epoch 116/199\n",
      "87.71415424346924\n",
      "Epoch 117/199\n",
      "87.69057965278625\n",
      "Epoch 118/199\n",
      "87.62368845939636\n",
      "Epoch 119/199\n",
      "87.70929002761841\n",
      "Epoch 120/199\n",
      "87.9043242931366\n",
      "Epoch 121/199\n",
      "87.75399231910706\n",
      "Epoch 122/199\n",
      "87.73124194145203\n",
      "Epoch 123/199\n",
      "87.72015643119812\n",
      "Epoch 124/199\n",
      "87.71663093566895\n",
      "Epoch 125/199\n",
      "87.7780122756958\n",
      "Epoch 126/199\n",
      "87.94798254966736\n",
      "Epoch 127/199\n",
      "87.70740914344788\n",
      "Epoch 128/199\n",
      "87.74695682525635\n",
      "Epoch 129/199\n",
      "87.76596641540527\n",
      "Epoch 130/199\n",
      "87.83249545097351\n",
      "Epoch 131/199\n",
      "87.81582498550415\n",
      "Epoch 132/199\n",
      "87.65644574165344\n",
      "Epoch 133/199\n",
      "87.89801287651062\n",
      "Epoch 134/199\n",
      "87.87061619758606\n",
      "Epoch 135/199\n",
      "87.84858560562134\n",
      "Epoch 136/199\n",
      "87.85325121879578\n",
      "Epoch 137/199\n",
      "87.81505346298218\n",
      "Epoch 138/199\n",
      "87.81289863586426\n",
      "Epoch 139/199\n",
      "87.99623680114746\n",
      "Epoch 140/199\n",
      "87.83352541923523\n",
      "Epoch 141/199\n",
      "87.8453962802887\n",
      "Epoch 142/199\n",
      "87.77601051330566\n",
      "Epoch 143/199\n",
      "87.78355503082275\n",
      "Epoch 144/199\n",
      "87.83298683166504\n",
      "Epoch 145/199\n",
      "87.78714442253113\n",
      "Epoch 146/199\n",
      "88.02333950996399\n",
      "Epoch 147/199\n",
      "87.81217002868652\n",
      "Epoch 148/199\n",
      "87.8116192817688\n",
      "Epoch 149/199\n",
      "87.94942593574524\n",
      "Epoch 150/199\n",
      "87.88198399543762\n",
      "Epoch 151/199\n",
      "87.79231834411621\n",
      "Epoch 152/199\n",
      "88.01340126991272\n",
      "Epoch 153/199\n",
      "87.79191994667053\n",
      "Epoch 154/199\n",
      "87.8231680393219\n",
      "Epoch 155/199\n",
      "87.8633303642273\n",
      "Epoch 156/199\n",
      "87.97532749176025\n",
      "Epoch 157/199\n",
      "87.9245958328247\n",
      "Epoch 158/199\n",
      "87.85992217063904\n",
      "Epoch 159/199\n",
      "88.05627536773682\n",
      "Epoch 160/199\n",
      "87.81340169906616\n",
      "Epoch 161/199\n",
      "87.83489727973938\n",
      "Epoch 162/199\n",
      "87.79193210601807\n",
      "Epoch 163/199\n",
      "87.83148527145386\n",
      "Epoch 164/199\n",
      "87.76203608512878\n",
      "Epoch 165/199\n",
      "87.95912504196167\n",
      "Epoch 166/199\n",
      "87.91634964942932\n",
      "Epoch 167/199\n",
      "87.91532111167908\n",
      "Epoch 168/199\n",
      "87.88095188140869\n",
      "Epoch 169/199\n",
      "87.9828155040741\n",
      "Epoch 170/199\n",
      "87.94766354560852\n",
      "Epoch 171/199\n",
      "87.78580784797668\n",
      "Epoch 172/199\n",
      "88.04437065124512\n",
      "Epoch 173/199\n",
      "87.91701030731201\n",
      "Epoch 174/199\n",
      "87.94607830047607\n",
      "Epoch 175/199\n",
      "87.85471940040588\n",
      "Epoch 176/199\n",
      "87.87036681175232\n",
      "Epoch 177/199\n",
      "87.99127984046936\n",
      "Epoch 178/199\n",
      "87.98329949378967\n",
      "Epoch 179/199\n",
      "87.90900182723999\n",
      "Epoch 180/199\n",
      "87.97355937957764\n",
      "Epoch 181/199\n",
      "87.88621640205383\n",
      "Epoch 182/199\n",
      "87.89387464523315\n",
      "Epoch 183/199\n",
      "87.99358034133911\n",
      "Epoch 184/199\n",
      "87.95051431655884\n",
      "Epoch 185/199\n",
      "88.1285502910614\n",
      "Epoch 186/199\n",
      "87.84090232849121\n",
      "Epoch 187/199\n",
      "87.9038143157959\n",
      "Epoch 188/199\n",
      "87.88832592964172\n",
      "Epoch 189/199\n",
      "87.97050881385803\n",
      "Epoch 190/199\n",
      "87.91747617721558\n",
      "Epoch 191/199\n",
      "88.13847970962524\n",
      "Epoch 192/199\n",
      "87.86182808876038\n",
      "Epoch 193/199\n",
      "88.04898476600647\n",
      "Epoch 194/199\n",
      "87.97942185401917\n",
      "Epoch 195/199\n",
      "88.02038526535034\n",
      "Epoch 196/199\n",
      "87.99164319038391\n",
      "Epoch 197/199\n",
      "87.88918948173523\n",
      "Epoch 198/199\n",
      "88.12368702888489\n",
      "Epoch 199/199\n",
      "87.92181706428528\n",
      "Training complete in 291m 58s\n",
      "Best val Acc: 0.966900\n"
     ]
    }
   ],
   "source": [
    "vit_copy1 = copy.deepcopy(vit)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "vit_copy1.to(device)\n",
    "\n",
    "if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "    model_ft_org = torch.nn.DataParallel(vit_copy1)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "base_optimizer = optim.SGD\n",
    "optimizer = PUGDXR(vit_copy1.parameters(),\n",
    "                base_optimizer,\n",
    "                lr=0.001,\n",
    "                momentum=args.momentum,\n",
    "                weight_decay=args.wd,\n",
    "                min_beta = 0.1, \n",
    "                max_beta = 2, \n",
    "                method = 'icos',\n",
    "                dampening=0,   # 必须设置为0才能完全固定 \n",
    "                nesterov=False # 禁用Nesterov动量 \n",
    "                 )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ft_epochs)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "vit_copy1, metrics0 = train_model(vit_copy1, criterion, optimizer, scheduler, ft_epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "model_path = \"./model/lfs/\"+args.datasets+\"/vit_pugd_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(ft_epochs) + \".pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': vit_copy1.state_dict(), \n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, model_path) \n",
    "\n",
    "name = \"./results/\"+args.datasets+\"/vit_pugd_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(ft_epochs) + \".json\"\n",
    "with open(name,  'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2b1a7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 加载预训练DeiT-B/16\n",
    "deit = timm.create_model('deit_base_patch16_224', pretrained=True, \n",
    "                        img_size = 32, patch_size = 8, num_classes = Num_class)\n",
    "ft_epochs = 300\n",
    "# # 冻结所有层除了分类token和head\n",
    "# for name, param in deit.named_parameters(): \n",
    "#     if not name.startswith(('cls_token',  'pos_embed', 'head')):\n",
    "#         param.requires_grad  = False\n",
    "\n",
    "# # 修改分类头（20类任务示例）\n",
    "# deit.head  = torch.nn.Linear(deit.head.in_features,  Num_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bff32bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "87.9381730556488\n",
      "Epoch 1/299\n",
      "87.79314637184143\n",
      "Epoch 2/299\n",
      "87.83815598487854\n",
      "Epoch 3/299\n",
      "87.89583826065063\n",
      "Epoch 4/299\n",
      "87.96307325363159\n",
      "Epoch 5/299\n",
      "87.88075923919678\n",
      "Epoch 6/299\n",
      "87.871258020401\n",
      "Epoch 7/299\n",
      "87.90223145484924\n",
      "Epoch 8/299\n",
      "87.81615900993347\n",
      "Epoch 9/299\n",
      "87.87571978569031\n",
      "Epoch 10/299\n",
      "87.95237350463867\n",
      "Epoch 11/299\n",
      "87.80326414108276\n",
      "Epoch 12/299\n",
      "87.85398173332214\n",
      "Epoch 13/299\n",
      "87.85771036148071\n",
      "Epoch 14/299\n",
      "87.8468725681305\n",
      "Epoch 15/299\n",
      "87.79708290100098\n",
      "Epoch 16/299\n",
      "87.80381846427917\n",
      "Epoch 17/299\n",
      "87.93544459342957\n",
      "Epoch 18/299\n",
      "87.77240204811096\n",
      "Epoch 19/299\n",
      "87.93308997154236\n",
      "Epoch 20/299\n",
      "87.90068578720093\n",
      "Epoch 21/299\n",
      "87.86560845375061\n",
      "Epoch 22/299\n",
      "87.85792851448059\n",
      "Epoch 23/299\n",
      "87.97219848632812\n",
      "Epoch 24/299\n",
      "87.8548629283905\n",
      "Epoch 25/299\n",
      "87.82375597953796\n",
      "Epoch 26/299\n",
      "87.9475109577179\n",
      "Epoch 27/299\n",
      "87.99560785293579\n",
      "Epoch 28/299\n",
      "87.92789936065674\n",
      "Epoch 29/299\n",
      "87.90910935401917\n",
      "Epoch 30/299\n",
      "88.076895236969\n",
      "Epoch 31/299\n",
      "87.99578213691711\n",
      "Epoch 32/299\n",
      "87.98622298240662\n",
      "Epoch 33/299\n",
      "87.99468278884888\n",
      "Epoch 34/299\n",
      "87.87264013290405\n",
      "Epoch 35/299\n",
      "87.92059278488159\n",
      "Epoch 36/299\n",
      "87.89389991760254\n",
      "Epoch 37/299\n",
      "88.01271963119507\n",
      "Epoch 38/299\n",
      "87.91550779342651\n",
      "Epoch 39/299\n",
      "87.97657108306885\n",
      "Epoch 40/299\n",
      "87.9897620677948\n",
      "Epoch 41/299\n",
      "87.91896843910217\n",
      "Epoch 42/299\n",
      "87.87233328819275\n",
      "Epoch 43/299\n",
      "88.05875730514526\n",
      "Epoch 44/299\n",
      "87.92958354949951\n",
      "Epoch 45/299\n",
      "88.01233434677124\n",
      "Epoch 46/299\n",
      "87.89944911003113\n",
      "Epoch 47/299\n",
      "87.72607469558716\n",
      "Epoch 48/299\n",
      "87.81631326675415\n",
      "Epoch 49/299\n",
      "87.80278015136719\n",
      "Epoch 50/299\n",
      "87.90247344970703\n",
      "Epoch 51/299\n",
      "87.76841139793396\n",
      "Epoch 52/299\n",
      "87.71312642097473\n",
      "Epoch 53/299\n",
      "87.74395561218262\n",
      "Epoch 54/299\n",
      "87.79604649543762\n",
      "Epoch 55/299\n",
      "87.82743859291077\n",
      "Epoch 56/299\n",
      "88.05482482910156\n",
      "Epoch 57/299\n",
      "87.86391067504883\n",
      "Epoch 58/299\n",
      "87.83260655403137\n",
      "Epoch 59/299\n",
      "87.85685062408447\n",
      "Epoch 60/299\n",
      "87.77247667312622\n",
      "Epoch 61/299\n",
      "87.84725022315979\n",
      "Epoch 62/299\n",
      "87.82064318656921\n",
      "Epoch 63/299\n",
      "87.93438744544983\n",
      "Epoch 64/299\n",
      "87.84022760391235\n",
      "Epoch 65/299\n",
      "87.82665944099426\n",
      "Epoch 66/299\n",
      "87.82044529914856\n",
      "Epoch 67/299\n",
      "87.84533715248108\n",
      "Epoch 68/299\n",
      "87.99324154853821\n",
      "Epoch 69/299\n",
      "88.09978246688843\n",
      "Epoch 70/299\n",
      "87.91496443748474\n",
      "Epoch 71/299\n",
      "87.79706716537476\n",
      "Epoch 72/299\n",
      "87.80405306816101\n",
      "Epoch 73/299\n",
      "87.88473558425903\n",
      "Epoch 74/299\n",
      "87.9633150100708\n",
      "Epoch 75/299\n",
      "87.94857573509216\n",
      "Epoch 76/299\n",
      "88.11084175109863\n",
      "Epoch 77/299\n",
      "87.90226936340332\n",
      "Epoch 78/299\n",
      "87.7873125076294\n",
      "Epoch 79/299\n",
      "87.92399668693542\n",
      "Epoch 80/299\n",
      "87.88589072227478\n",
      "Epoch 81/299\n",
      "87.85300970077515\n",
      "Epoch 82/299\n",
      "87.93280339241028\n",
      "Epoch 83/299\n",
      "87.83222675323486\n",
      "Epoch 84/299\n",
      "87.86925292015076\n",
      "Epoch 85/299\n",
      "87.81172275543213\n",
      "Epoch 86/299\n",
      "87.89094114303589\n",
      "Epoch 87/299\n",
      "87.8014178276062\n",
      "Epoch 88/299\n",
      "87.80443215370178\n",
      "Epoch 89/299\n",
      "88.03304982185364\n",
      "Epoch 90/299\n",
      "87.82593131065369\n",
      "Epoch 91/299\n",
      "87.81864190101624\n",
      "Epoch 92/299\n",
      "87.80820178985596\n",
      "Epoch 93/299\n",
      "87.8505027294159\n",
      "Epoch 94/299\n",
      "87.80471873283386\n",
      "Epoch 95/299\n",
      "87.8219850063324\n",
      "Epoch 96/299\n",
      "88.086580991745\n",
      "Epoch 97/299\n",
      "87.85407710075378\n",
      "Epoch 98/299\n",
      "87.86220645904541\n",
      "Epoch 99/299\n",
      "87.90844535827637\n",
      "Epoch 100/299\n",
      "87.82971572875977\n",
      "Epoch 101/299\n",
      "87.82131910324097\n",
      "Epoch 102/299\n",
      "87.89024472236633\n",
      "Epoch 103/299\n",
      "87.85342788696289\n",
      "Epoch 104/299\n",
      "87.87575697898865\n",
      "Epoch 105/299\n",
      "87.8284204006195\n",
      "Epoch 106/299\n",
      "87.8824896812439\n",
      "Epoch 107/299\n",
      "87.92242980003357\n",
      "Epoch 108/299\n",
      "87.92235779762268\n",
      "Epoch 109/299\n",
      "88.19627714157104\n",
      "Epoch 110/299\n",
      "87.91237783432007\n",
      "Epoch 111/299\n",
      "87.99241781234741\n",
      "Epoch 112/299\n",
      "87.771000623703\n",
      "Epoch 113/299\n",
      "87.89235830307007\n",
      "Epoch 114/299\n",
      "87.88300490379333\n",
      "Epoch 115/299\n",
      "88.0627293586731\n",
      "Epoch 116/299\n",
      "87.87851071357727\n",
      "Epoch 117/299\n",
      "87.89171934127808\n",
      "Epoch 118/299\n",
      "87.81138253211975\n",
      "Epoch 119/299\n",
      "87.87170910835266\n",
      "Epoch 120/299\n",
      "87.76759672164917\n",
      "Epoch 121/299\n",
      "87.89272570610046\n",
      "Epoch 122/299\n",
      "87.97075152397156\n",
      "Epoch 123/299\n",
      "87.83312916755676\n",
      "Epoch 124/299\n",
      "87.86976599693298\n",
      "Epoch 125/299\n",
      "87.88276267051697\n",
      "Epoch 126/299\n",
      "87.94782447814941\n",
      "Epoch 127/299\n",
      "87.86536026000977\n",
      "Epoch 128/299\n",
      "88.0915949344635\n",
      "Epoch 129/299\n",
      "87.9750440120697\n",
      "Epoch 130/299\n",
      "87.90361428260803\n",
      "Epoch 131/299\n",
      "87.90444135665894\n",
      "Epoch 132/299\n",
      "87.91115832328796\n",
      "Epoch 133/299\n",
      "87.96302270889282\n",
      "Epoch 134/299\n",
      "87.87009024620056\n",
      "Epoch 135/299\n",
      "88.07384252548218\n",
      "Epoch 136/299\n",
      "87.82055377960205\n",
      "Epoch 137/299\n",
      "87.9390652179718\n",
      "Epoch 138/299\n",
      "88.03416967391968\n",
      "Epoch 139/299\n",
      "87.95905041694641\n",
      "Epoch 140/299\n",
      "87.87110137939453\n",
      "Epoch 141/299\n",
      "87.89312076568604\n",
      "Epoch 142/299\n",
      "87.9638922214508\n",
      "Epoch 143/299\n",
      "87.91628789901733\n",
      "Epoch 144/299\n",
      "87.92719054222107\n",
      "Epoch 145/299\n",
      "87.91333651542664\n",
      "Epoch 146/299\n",
      "87.88581705093384\n",
      "Epoch 147/299\n",
      "87.85194993019104\n",
      "Epoch 148/299\n",
      "88.01050877571106\n",
      "Epoch 149/299\n",
      "88.02542304992676\n",
      "Epoch 150/299\n",
      "87.79040241241455\n",
      "Epoch 151/299\n",
      "87.89157390594482\n",
      "Epoch 152/299\n",
      "87.86236691474915\n",
      "Epoch 153/299\n",
      "87.88406872749329\n",
      "Epoch 154/299\n",
      "87.92073035240173\n",
      "Epoch 155/299\n",
      "87.99627590179443\n",
      "Epoch 156/299\n",
      "87.9666383266449\n",
      "Epoch 157/299\n",
      "87.92133498191833\n",
      "Epoch 158/299\n",
      "87.96203517913818\n",
      "Epoch 159/299\n",
      "87.91913890838623\n",
      "Epoch 160/299\n",
      "87.89787006378174\n",
      "Epoch 161/299\n",
      "88.0037260055542\n",
      "Epoch 162/299\n",
      "87.77012133598328\n",
      "Epoch 163/299\n",
      "87.81937837600708\n",
      "Epoch 164/299\n",
      "87.78292679786682\n",
      "Epoch 165/299\n",
      "87.81526970863342\n",
      "Epoch 166/299\n",
      "87.89357042312622\n",
      "Epoch 167/299\n",
      "87.89673066139221\n",
      "Epoch 168/299\n",
      "88.06704235076904\n",
      "Epoch 169/299\n",
      "87.91579151153564\n",
      "Epoch 170/299\n",
      "87.87631106376648\n",
      "Epoch 171/299\n",
      "87.93325138092041\n",
      "Epoch 172/299\n",
      "87.99113059043884\n",
      "Epoch 173/299\n",
      "87.93369269371033\n",
      "Epoch 174/299\n",
      "88.07434272766113\n",
      "Epoch 175/299\n",
      "87.86744379997253\n",
      "Epoch 176/299\n",
      "87.85445427894592\n",
      "Epoch 177/299\n",
      "87.94520306587219\n",
      "Epoch 178/299\n",
      "88.00911831855774\n",
      "Epoch 179/299\n",
      "88.04350876808167\n",
      "Epoch 180/299\n",
      "87.9361469745636\n",
      "Epoch 181/299\n",
      "87.86965274810791\n",
      "Epoch 182/299\n",
      "87.70394229888916\n",
      "Epoch 183/299\n",
      "87.51988458633423\n",
      "Epoch 184/299\n",
      "87.3861677646637\n",
      "Epoch 185/299\n",
      "87.30071926116943\n",
      "Epoch 186/299\n",
      "87.43878507614136\n",
      "Epoch 187/299\n",
      "87.55866551399231\n",
      "Epoch 188/299\n",
      "87.76244688034058\n",
      "Epoch 189/299\n",
      "87.78113555908203\n",
      "Epoch 190/299\n",
      "87.93043375015259\n",
      "Epoch 191/299\n",
      "87.79356646537781\n",
      "Epoch 192/299\n",
      "87.79395270347595\n",
      "Epoch 193/299\n",
      "87.86410570144653\n",
      "Epoch 194/299\n",
      "88.1403443813324\n",
      "Epoch 195/299\n",
      "87.88687753677368\n",
      "Epoch 196/299\n",
      "87.95553946495056\n",
      "Epoch 197/299\n",
      "88.01066327095032\n",
      "Epoch 198/299\n",
      "87.83974289894104\n",
      "Epoch 199/299\n",
      "88.04301047325134\n",
      "Epoch 200/299\n",
      "87.95662498474121\n",
      "Epoch 201/299\n",
      "88.02336812019348\n",
      "Epoch 202/299\n",
      "87.90386509895325\n",
      "Epoch 203/299\n",
      "87.87380242347717\n",
      "Epoch 204/299\n",
      "87.88479900360107\n",
      "Epoch 205/299\n",
      "87.90210318565369\n",
      "Epoch 206/299\n",
      "87.94215869903564\n",
      "Epoch 207/299\n",
      "88.1383273601532\n",
      "Epoch 208/299\n",
      "87.94541835784912\n",
      "Epoch 209/299\n",
      "87.95949125289917\n",
      "Epoch 210/299\n",
      "87.93325662612915\n",
      "Epoch 211/299\n",
      "88.00726175308228\n",
      "Epoch 212/299\n",
      "88.03296637535095\n",
      "Epoch 213/299\n",
      "88.00863099098206\n",
      "Epoch 214/299\n",
      "88.04407787322998\n",
      "Epoch 215/299\n",
      "87.87179017066956\n",
      "Epoch 216/299\n",
      "87.91073966026306\n",
      "Epoch 217/299\n",
      "88.00111126899719\n",
      "Epoch 218/299\n",
      "87.97972416877747\n",
      "Epoch 219/299\n",
      "88.10656595230103\n",
      "Epoch 220/299\n",
      "88.0774474143982\n",
      "Epoch 221/299\n",
      "88.12966346740723\n",
      "Epoch 222/299\n",
      "87.95456290245056\n",
      "Epoch 223/299\n",
      "87.98534917831421\n",
      "Epoch 224/299\n",
      "87.92286539077759\n",
      "Epoch 225/299\n",
      "87.93265724182129\n",
      "Epoch 226/299\n",
      "87.90663814544678\n",
      "Epoch 227/299\n",
      "88.15016341209412\n",
      "Epoch 228/299\n",
      "87.91368222236633\n",
      "Epoch 229/299\n",
      "87.96887469291687\n",
      "Epoch 230/299\n",
      "88.01026248931885\n",
      "Epoch 231/299\n",
      "88.03251552581787\n",
      "Epoch 232/299\n",
      "87.98737025260925\n",
      "Epoch 233/299\n",
      "88.05449104309082\n",
      "Epoch 234/299\n",
      "88.18859624862671\n",
      "Epoch 235/299\n",
      "87.97038745880127\n",
      "Epoch 236/299\n",
      "87.95095181465149\n",
      "Epoch 237/299\n",
      "88.04912877082825\n",
      "Epoch 238/299\n",
      "88.0223958492279\n",
      "Epoch 239/299\n",
      "88.03972959518433\n",
      "Epoch 240/299\n",
      "88.1919424533844\n",
      "Epoch 241/299\n",
      "88.03647041320801\n",
      "Epoch 242/299\n",
      "88.05964517593384\n",
      "Epoch 243/299\n",
      "87.92249178886414\n",
      "Epoch 244/299\n",
      "88.00217032432556\n",
      "Epoch 245/299\n",
      "87.99931502342224\n",
      "Epoch 246/299\n",
      "88.05049085617065\n",
      "Epoch 247/299\n",
      "88.16433501243591\n",
      "Epoch 248/299\n",
      "87.940274477005\n",
      "Epoch 249/299\n",
      "88.0831298828125\n",
      "Epoch 250/299\n",
      "87.9723310470581\n",
      "Epoch 251/299\n",
      "87.9902732372284\n",
      "Epoch 252/299\n",
      "87.76791763305664\n",
      "Epoch 253/299\n",
      "87.81573057174683\n",
      "Epoch 254/299\n",
      "87.45077848434448\n",
      "Epoch 255/299\n",
      "87.26955771446228\n",
      "Epoch 256/299\n",
      "87.20804953575134\n",
      "Epoch 257/299\n",
      "87.21485805511475\n",
      "Epoch 258/299\n",
      "87.01999759674072\n",
      "Epoch 259/299\n",
      "87.14202308654785\n",
      "Epoch 260/299\n",
      "87.43894362449646\n",
      "Epoch 261/299\n",
      "87.01958799362183\n",
      "Epoch 262/299\n",
      "87.00605654716492\n",
      "Epoch 263/299\n",
      "86.97877073287964\n",
      "Epoch 264/299\n",
      "87.02208590507507\n",
      "Epoch 265/299\n",
      "87.1684947013855\n",
      "Epoch 266/299\n",
      "87.50019598007202\n",
      "Epoch 267/299\n",
      "87.75638628005981\n",
      "Epoch 268/299\n",
      "87.21645927429199\n",
      "Epoch 269/299\n",
      "86.97345399856567\n",
      "Epoch 270/299\n",
      "87.15084409713745\n",
      "Epoch 271/299\n",
      "87.67911887168884\n",
      "Epoch 272/299\n",
      "87.98137521743774\n",
      "Epoch 273/299\n",
      "88.22337317466736\n",
      "Epoch 274/299\n",
      "88.03034710884094\n",
      "Epoch 275/299\n",
      "88.09401369094849\n",
      "Epoch 276/299\n",
      "88.13322949409485\n",
      "Epoch 277/299\n",
      "88.079509973526\n",
      "Epoch 278/299\n",
      "88.1094241142273\n",
      "Epoch 279/299\n",
      "88.00367426872253\n",
      "Epoch 280/299\n",
      "88.2489242553711\n",
      "Epoch 281/299\n",
      "88.15101075172424\n",
      "Epoch 282/299\n",
      "88.05304551124573\n",
      "Epoch 283/299\n",
      "87.8170280456543\n",
      "Epoch 284/299\n",
      "87.74728631973267\n",
      "Epoch 285/299\n",
      "87.50837182998657\n",
      "Epoch 286/299\n",
      "87.33235144615173\n",
      "Epoch 287/299\n",
      "87.42763352394104\n",
      "Epoch 288/299\n",
      "87.29854559898376\n",
      "Epoch 289/299\n",
      "87.21550679206848\n",
      "Epoch 290/299\n",
      "87.26453018188477\n",
      "Epoch 291/299\n",
      "87.16458821296692\n",
      "Epoch 292/299\n",
      "87.08248209953308\n",
      "Epoch 293/299\n",
      "87.24529886245728\n",
      "Epoch 294/299\n",
      "87.15225911140442\n",
      "Epoch 295/299\n",
      "87.25405812263489\n",
      "Epoch 296/299\n",
      "87.41125464439392\n",
      "Epoch 297/299\n",
      "87.59748196601868\n",
      "Epoch 298/299\n",
      "87.72372460365295\n",
      "Epoch 299/299\n",
      "87.80421853065491\n",
      "Training complete in 439m 14s\n",
      "Best val Acc: 0.950600\n"
     ]
    }
   ],
   "source": [
    "deit_copy1 = copy.deepcopy(deit)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "deit_copy1.to(device)\n",
    "\n",
    "if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "    model_ft_org = torch.nn.DataParallel(deit_copy1)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "base_optimizer = optim.SGD\n",
    "optimizer = PUGDX(deit_copy1.parameters(),\n",
    "                base_optimizer,\n",
    "                lr=0.0005,\n",
    "                momentum=args.momentum,\n",
    "                weight_decay=args.wd,\n",
    "                dampening=0,   # 必须设置为0才能完全固定 \n",
    "                nesterov=False # 禁用Nesterov动量 \n",
    "                 )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ft_epochs)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "deit_copy1, metrics0 = train_model(deit_copy1, criterion, optimizer, scheduler, ft_epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "model_path = \"./model/lfs/\"+args.datasets+\"/deit_pugd\" + str(ft_epochs) + \".pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': deit_copy1.state_dict(), \n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, model_path) \n",
    "\n",
    "name = \"./results/\"+args.datasets+\"/deit_pugd_\" + str(ft_epochs) + \".json\"\n",
    "with open(name,  'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
