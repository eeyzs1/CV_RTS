{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c297172",
   "metadata": {},
   "source": [
    "Multi-level Perturbed Unit Gradient Descent, MPUGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d2b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data  import Subset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models  as models\n",
    "from types import SimpleNamespace \n",
    "import matplotlib.pyplot  as plt\n",
    "import numpy as np \n",
    "\n",
    "from optimizers import *\n",
    "from upanets import UPANets\n",
    "from torchsummary import summary\n",
    "import time, copy,timm\n",
    "import json\n",
    "import random \n",
    "import os\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaa60ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args = SimpleNamespace(\n",
    "    datasets='cifar_100',\n",
    "    batch_size = 500,\n",
    "    seed = 42,\n",
    "    lr=0.1, \n",
    "    momentum=0.9,\n",
    "    wd = 0.0005,\n",
    "    blocks = 1,\n",
    "    filters = 16,\n",
    "    epochs = 400,\n",
    "    start_epochs = 8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bdda934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed=42):\n",
    "    # Python原生随机 \n",
    "    random.seed(seed) \n",
    "    # NumPy随机 \n",
    "    np.random.seed(seed) \n",
    "    # PyTorch随机 \n",
    "    torch.manual_seed(seed) \n",
    "    # CUDA随机（GPU相关）\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    # CUDNN确定性模式 \n",
    "    torch.backends.cudnn.deterministic  = True \n",
    "    torch.backends.cudnn.benchmark  = False \n",
    " \n",
    "set_all_seeds(args.seed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2f2d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "557e50ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 50000, 'valid': 10000}\n"
     ]
    }
   ],
   "source": [
    "img_size = 32 # default image size for Cifar-10\n",
    "im_dimention = 32\n",
    "cifar_10_mean = [0.4914, 0.4822, 0.4465] \n",
    "cifar_10_std = [0.2023, 0.1994, 0.2010]\n",
    "cifar_100_mean = [0.5071, 0.4867, 0.4408]\n",
    "cifar_100_std = [0.2673, 0.2564, 0.2762]\n",
    "\n",
    "if args.datasets == 'cifar_10':\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((im_dimention,im_dimention)),\n",
    "            transforms.RandomRotation(15,),\n",
    "            transforms.RandomCrop(im_dimention),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cifar_10_mean, std=cifar_10_std)\n",
    "            # transforms.Lambda(lambda x: x.to(torch.float16))    # 最终输出FP16\n",
    "        ]),\n",
    "        # 'valid': transforms.Compose([\n",
    "        #     transforms.Resize((im_dimention,im_dimention)),\n",
    "        #     transforms.ToTensor(),\n",
    "        #     transforms.Normalize(mean=cifar_10_mean, std=cifar_10_std)\n",
    "        # ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((im_dimention,im_dimention)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cifar_10_mean, std=cifar_10_std)\n",
    "        ]),\n",
    "    }\n",
    " \n",
    "    full_trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data/cifar_10', train=True, download=True, transform=data_transforms['train'])\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data/cifar_10', train=False, download=True, transform=data_transforms['test'])\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "    Num_class = 10\n",
    "\n",
    "if args.datasets == 'cifar_100':\n",
    "    data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((im_dimention,im_dimention)),\n",
    "        transforms.RandomRotation(15,),\n",
    "        transforms.RandomCrop(im_dimention),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar_100_mean, std=cifar_100_std)\n",
    "    ]),\n",
    "    # 'valid': transforms.Compose([\n",
    "    #     transforms.Resize((im_dimention,im_dimention)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=cifar_100_mean, std=cifar_100_std)\n",
    "    # ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((im_dimention,im_dimention)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar_100_mean, std=cifar_100_std)\n",
    "    ]),\n",
    "    }\n",
    "    full_trainset = torchvision.datasets.CIFAR100(\n",
    "        root='./data/cifar_100', train=True, download=True, transform=data_transforms['train'])\n",
    "    testset = torchvision.datasets.CIFAR100(\n",
    "        root='./data/cifar_100', train=False, download=True, transform=data_transforms['test'])\n",
    "    testloader = DataLoader(\n",
    "        testset, batch_size=args.batch_size, shuffle=False,sampler=torch.utils.data.SequentialSampler(testset),  num_workers=0)\n",
    "    Num_class = 100\n",
    "\n",
    "# # 获取所有样本的标签 \n",
    "# labels = [full_trainset[i][1] for i in range(len(full_trainset))]\n",
    "\n",
    "# # 分层划分（stratify参数确保比例）\n",
    "# train_idx, val_idx = train_test_split(\n",
    "#     range(len(full_trainset)),\n",
    "#     test_size=0.2,\n",
    "#     shuffle=True,\n",
    "#     stratify=labels,\n",
    "#     random_state=args.seed  \n",
    "# )\n",
    "\n",
    "# train_data = np.stack([full_trainset.data[i]  for i in train_idx]) \n",
    "# train_targets = [full_trainset.targets[i] for i in train_idx] \n",
    "# val_data = np.stack([full_trainset.data[i]  for i in val_idx]) \n",
    "# val_targets = [full_trainset.targets[i] for i in val_idx] \n",
    "\n",
    "# valset = full_trainset\n",
    "# valset.data = val_data\n",
    "# valset.targets = val_targets\n",
    "# valset.transform = data_transforms['valid']\n",
    "\n",
    "# trainset = copy.deepcopy(valset)\n",
    "# trainset.data = train_data\n",
    "# trainset.targets = train_targets\n",
    "# trainset.transform = data_transforms['train']\n",
    "\n",
    "# trainloader = {\n",
    "#     'train':DataLoader(\n",
    "#     trainset, batch_size=args.batch_size, shuffle=False, sampler=torch.utils.data.SequentialSampler(trainset), num_workers=0),\n",
    "#     'valid':DataLoader(\n",
    "#     valset, batch_size=args.batch_size, shuffle=False, sampler=torch.utils.data.SequentialSampler(valset), num_workers=0)}\n",
    "\n",
    "# dataset_sizes = {\n",
    "#     'train': len(trainset),\n",
    "#     'valid': len(valset),            \n",
    "                #  }\n",
    "\n",
    "trainloader = {\n",
    "    'train':DataLoader(\n",
    "    full_trainset, batch_size=args.batch_size, shuffle=False, sampler=torch.utils.data.SequentialSampler(full_trainset), num_workers=0),\n",
    "    'valid':testloader\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(full_trainset),\n",
    "    'valid': len(testset),      \n",
    "}\n",
    "print(dataset_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "840b5037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vit_tiny_patch16_224']\n"
     ]
    }
   ],
   "source": [
    "print(timm.list_models('*vit_tiny_patch16_224*')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab5b31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from torchvision.transforms.functional  import to_pil_image \n",
    "# 反归一化转换（需与transform中的参数对应）\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.4914/0.2023, -0.4822/0.1994, -0.4465/0.2010],\n",
    "    std=[1/0.2023, 1/0.1994, 1/0.2010]\n",
    ")\n",
    " \n",
    "def show_images(loader, num_images=4):\n",
    "    # 获取一个batch的数据 \n",
    "    # images, labels = next(iter(loader))\n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        if batch_idx == 0:  # 只取第一个batch \n",
    "            break \n",
    "\n",
    "    # 创建子图 \n",
    "    fig, axes = plt.subplots(1,  num_images, figsize=(15, 3))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # 反归一化+通道顺序调整 \n",
    "        img = inv_normalize(images[i]).permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img,  0, 1)  # 处理浮点误差 \n",
    "        \n",
    "        # 显示图像及标签 \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Label: {full_trainset.classes[labels[i]]}\") \n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show() \n",
    " \n",
    "show_images(trainloader['train'])\n",
    "show_images(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5aa47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/\" + args.datasets + \"/benchmark.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    net_benckmark_data = torch.load(model_path,  map_location='cpu')\n",
    "    benckmark_state_dict = net_benckmark_data['model_state_dict'] \n",
    "else:\n",
    "    net_benchmark = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "    torch.save({\n",
    "        'model_state_dict': net_benchmark.state_dict()\n",
    "    }, model_path)\n",
    "    benckmark_state_dict = net_benchmark.state_dict()\n",
    "\n",
    "def tensor_to_serializable(obj):\n",
    "    if isinstance(obj, (np.float32,  np.float64)):   # 处理NumPy浮点数\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.integer):               # 处理NumPy整数 \n",
    "        return int(obj)\n",
    "    elif isinstance(obj, torch.Tensor):            # 处理PyTorch Tensor \n",
    "        return obj.item()  if obj.numel()  == 1 else obj.tolist() \n",
    "    elif isinstance(obj, (np.ndarray)):             # 处理NumPy数组 \n",
    "        return obj.tolist() \n",
    "    elif hasattr(obj, '__dict__'):                 # 处理自定义对象（可选）\n",
    "        return obj.__dict__\n",
    "    return obj \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ec9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdt_delta = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugdt_delta.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdt_delta.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugdt_delta = torch.nn.DataParallel(net_pugdt_delta)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXT(net_pugdt_delta.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugdt_delta, metricst_delta = train_model_timing_delta(net_pugdt_delta, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes, int(args.epochs/10), 0.01, 10) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdt_delta_\" + str(args.epochs) + \"xi\" + str(args.epochs/10) + \"mu0.01_t10.pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdt_delta.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdt_delta_\" + str(args.epochs) + \"xi\" + str(args.epochs/10) + \"mu0.01_t10.json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricst_delta,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb925b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdt_var = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugdt_var.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdt_var.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugdt_var = torch.nn.DataParallel(net_pugdt_var)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXT(net_pugdt_var.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugdt_var, metricst_var = train_model_timing_var(net_pugdt_var, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes, int(args.epochs/10), 0.015, 10) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdt_var_\" + str(args.epochs) + \"init_t\" + str(args.epochs/10) + \"gamma0.015_k10.pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdt_var.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdt_var_\" + str(args.epochs) + \"init_t\" + str(args.epochs/10) + \"gamma0.015_k10.json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricst_var,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a42d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "# net.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net = torch.nn.DataParallel(net)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(),\n",
    "#                 lr=args.lr,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net, metrics_org = train_model_org(net, criterion, optimizer, scheduler, args.epochs * 2, trainloader, device, dataset_sizes) \n",
    "\n",
    "# # 保存模型架构+参数+优化器状态（完整恢复训练）\n",
    "# model_path = \"./model/\"+args.datasets+\"/org\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/org_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metrics_org, f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n",
    " \n",
    "# # 加载 \n",
    "\n",
    "# # checkpoint = torch.load('full_model_checkpoint.pth',  map_location='cpu')  # 先加载到CPU避免设备冲突 \n",
    "# # 模型结构需提前定义（需与保存时一致）\n",
    "# # model = YourModelClass()  \n",
    "# # model.load_state_dict(checkpoint['model_state_dict']) \n",
    " \n",
    "# # # 恢复优化器和训练状态 \n",
    "# # optimizer = torch.optim.Adam(model.parameters())  \n",
    "# # optimizer.load_state_dict(checkpoint['optimizer_state_dict']) \n",
    "# # with open('data.json',  'r', encoding='utf-8') as f:\n",
    "# #     loaded_dict = json.load(f) \n",
    "\n",
    "\n",
    "# # summary(net, (3, img_size, img_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db0a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdr_sin = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "# net_pugdr_sin.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdr_sin.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugdr_sin)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(net_pugdr_sin.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.2, \n",
    "#                 max_beta = 2.0, \n",
    "#                 method = 'sin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugdr_sin, metricsr_sin = train_model_alpha(net_pugdr_sin, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdr_sin\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdr_sin.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdr_sin_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsr_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c82cb0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdr_cos = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "# net_pugdr_cos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdr_cos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugdr_cos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(net_pugdr_cos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.001, \n",
    "#                 max_beta = 2.0, \n",
    "#                 method = 'cos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugdr_cos, metricsr_sin = train_model_alpha(net_pugdr_cos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdr_cos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdr_cos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdr_cos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsr_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db4abe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     if metricsr_sin['bast_acc'] < 0.7234:\n",
    "#         net_pugdr_cos = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "#         net_pugdr_cos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "#         criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#         net_pugdr_cos.to(device)\n",
    "\n",
    "#         if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#             model_ft_org = torch.nn.DataParallel(net_pugdr_cos)\n",
    "#             cudnn.benchmark = True\n",
    "\n",
    "#         base_optimizer = optim.SGD\n",
    "#         optimizer = PUGDXR(net_pugdr_cos.parameters(),\n",
    "#                         base_optimizer,\n",
    "#                         lr=args.lr,\n",
    "#                         max_epochs= args.epochs,\n",
    "#                         momentum=args.momentum,\n",
    "#                         weight_decay=args.wd,\n",
    "#                         min_beta = 0.0, \n",
    "#                         max_beta = 2.0, \n",
    "#                         method = 'cos',\n",
    "#                         dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                         nesterov=False # 禁用Nesterov动量 \n",
    "#                         )\n",
    "\n",
    "#         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "#         # Decay LR by a factor of 0.1 every 7 epochs\n",
    "#         # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "#         net_pugdr_cos, metricsr_sin = train_model_alpha(net_pugdr_cos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "#         model_path = \"./model/\"+args.datasets+\"/pugdr_cos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "#         torch.save({\n",
    "#             'model_state_dict': net_pugdr_cos.state_dict(), \n",
    "#             'optimizer_state_dict': optimizer.state_dict()\n",
    "#         }, model_path) \n",
    "\n",
    "#         name = \"./results/\"+args.datasets+\"/pugdr_cos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "#         with open(name,  'w', encoding='utf-8') as f:\n",
    "#             json.dump(metricsr_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a514712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdr_cos = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "# net_pugdr_cos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdr_cos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugdr_cos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(net_pugdr_cos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.0, \n",
    "#                 max_beta = 3.0, \n",
    "#                 method = 'cos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugdr_cos, metricsr_sin = train_model_alpha(net_pugdr_cos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdr_cos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdr_cos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdr_cos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsr_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be3e5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugds_cos = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugds_cos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugds_cos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugds_cos = torch.nn.DataParallel(net_pugds_cos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXS(net_pugds_cos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 1.2, \n",
    "#                 max_beta = 3.0,\n",
    "#                 method = 'cos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugds_cos, metricss_cos = train_model_alpha(net_pugds_cos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugds_cos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugds_cos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugds_cos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricss_cos,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14e79fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugds_sin = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugds_sin.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugds_sin.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugds_sin = torch.nn.DataParallel(net_pugds_sin)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXS(net_pugds_sin.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 1.8, \n",
    "#                 max_beta = 2.0,\n",
    "#                 method = 'sin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugds_sin, metricss_sin = train_model_alpha(net_pugds_sin, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugds_sin\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugds_sin.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugds_sin_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricss_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d2ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugd = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugd.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugd.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugd)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDX(net_pugd.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugd, metrics0 = train_model(net_pugd, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugd\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugd.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugd_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c195537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdr_cos = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugdr_cos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdr_cos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugdr_cos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(net_pugdr_cos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.3, \n",
    "#                 max_beta = 1, \n",
    "#                 method = 'icos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugdr_cos, metricsr_cos = train_model_alpha(net_pugdr_cos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdr_icos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdr_cos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdr_icos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsr_cos,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10cf6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdr_sin = UPANets(args.filters, Num_class, args.blocks, img_size)\n",
    "# net_pugdr_sin.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdr_sin.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(net_pugdr_sin)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(net_pugdr_sin.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.1, \n",
    "#                 max_beta = 1, \n",
    "#                 method = 'isin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                 )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugdr_sin, metricsr_sin = train_model_alpha(net_pugdr_sin, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdr_isin\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdr_sin.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdr_isin_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsr_sin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6b645d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugds_icos = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugds_icos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugds_icos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugds_icos = torch.nn.DataParallel(net_pugds_icos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXS(net_pugds_icos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.01, \n",
    "#                 max_beta = 0.5, \n",
    "#                 method = 'icos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugds_icos, metricss_icos = train_model_alpha(net_pugds_icos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugds_icos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugds_icos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugds_icos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricss_icos,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9575fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugds_icos = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugds_icos.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugds_icos.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugds_icos = torch.nn.DataParallel(net_pugds_icos)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXS(net_pugds_icos.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.05, \n",
    "#                 max_beta = 1.0, \n",
    "#                 method = 'icos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugds_icos, metricss_icos = train_model_alpha(net_pugds_icos, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugds_icos\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugds_icos.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugds_icos_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricss_icos,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "214741d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugds_isin = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugds_isin.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugds_isin.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugds_isin = torch.nn.DataParallel(net_pugds_isin)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXS(net_pugds_isin.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.1, \n",
    "#                 max_beta = 2,\n",
    "#                 method = 'isin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# net_pugds_isin, metricss_isin = train_model_alpha(net_pugds_isin, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugds_isin\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugds_isin.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugds_isin_\" + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricss_isin,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "156b0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdrs = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugdrs.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdrs.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugdrs = torch.nn.DataParallel(net_pugdrs)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXRS(net_pugdrs.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta_r = 0.01, \n",
    "#                 max_beta_r = 2, \n",
    "#                 method_r = 'isin',\n",
    "#                 min_beta_s = 0, \n",
    "#                 max_beta_s = 1.5,\n",
    "#                 method_s = 'cos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugdrs, metricsrs = train_model_alpha(net_pugdrs, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdrs_\" + str(optimizer.method_r) + str(optimizer.max_beta_r) + \"_\" + str(optimizer.min_beta_r) + \"_\" + str(optimizer.method_s) + str(optimizer.max_beta_s) + \"_\" + str(optimizer.min_beta_s) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdrs.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdrs_\" + str(optimizer.method_r) + str(optimizer.max_beta_r) + \"_\" + str(optimizer.min_beta_r) + \"_\" + str(optimizer.method_s) + str(optimizer.max_beta_s) + \"_\" + str(optimizer.min_beta_s) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsrs,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f20f6280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_pugdrs = UPANets(args.filters, Num_class, args.blocks, img_size)  \n",
    "# net_pugdrs.load_state_dict(copy.deepcopy(benckmark_state_dict))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# net_pugdrs.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     net_pugdrs = torch.nn.DataParallel(net_pugdrs)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXRS(net_pugdrs.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=args.lr,\n",
    "#                 max_epochs= args.epochs,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta_r = 0.0,\n",
    "#                 max_beta_r = 2.0,\n",
    "#                 method_r = 'sin',\n",
    "#                 min_beta_s = 0.0, \n",
    "#                 max_beta_s = 2.0,\n",
    "#                 method_s = 'sin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "# net_pugdrs, metricsrs = train_model_alpha(net_pugdrs, criterion, optimizer, scheduler, args.epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/\"+args.datasets+\"/pugdrs_\" + str(optimizer.method_r) + str(optimizer.max_beta_r) + \"_\" + str(optimizer.min_beta_r) + \"_\" + str(optimizer.method_s) + str(optimizer.max_beta_s) + \"_\" + str(optimizer.min_beta_s) + \"_\" + str(args.epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': net_pugdrs.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/pugdrs_\" + str(optimizer.method_r) + str(optimizer.max_beta_r) + \"_\" + str(optimizer.min_beta_r) + \"_\" + str(optimizer.method_s) + str(optimizer.max_beta_s) + \"_\" + str(optimizer.min_beta_s) + \"_\" + str(args.epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metricsrs,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "672b77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG-16 ResNet-18 DenseNet-121* growth rate in 16 UPANet-16 Overall Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "088d3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## finetune\n",
    "\n",
    "# from transformers import ViTForImageClassification, DeiTForImageClassification \n",
    " \n",
    "# # 加载预训练模型 \n",
    "# vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", num_labels=10, ignore_mismatched_sizes=True, device_map=\"auto\", resume_download=True) \n",
    "# deit_model = DeiTForImageClassification.from_pretrained(\"facebook/deit-base-patch16-224\", num_labels=10, ignore_mismatched_sizes=True, device_map=\"auto\", resume_download=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd6b0f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
    "#     optimizer,  # 绑定的优化器对象 \n",
    "#     mode='max',  # 监测指标模式 \n",
    "#     factor=0.5,  # 学习率衰减系数 \n",
    "#     patience=3   # 等待周期数 \n",
    "# )\n",
    "# for name, param in vit_model.named_parameters(): \n",
    "#     if 'encoder.layer.0'  in name:  # 冻结前N层 \n",
    "#         param.requires_grad  = False \n",
    "# # 修改分类头以适应CIFAR-10的10类 \n",
    "# vit_model.classifier  = torch.nn.Linear(vit_model.config.hidden_size,  10)\n",
    "# deit_model.classifier  = torch.nn.Linear(deit_model.config.hidden_size,  10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8923cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(new_head.__dict__)\n",
    "# print(vars(new_head))\n",
    "# import optimizers\n",
    "# import importlib \n",
    "# importlib.reload(optimizers)   \n",
    "# from optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04c21966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft2 = timm.create_model('mobilenetv3_small_100', pretrained=True, num_classes=Num_class)\n",
    "# original_head = model_ft2.classifier   # MobileNetV3的分类头名为classifier \n",
    "# new_head = nn.Linear(original_head.in_features,  Num_class)\n",
    "# model_ft2 = model_ft2.to(device)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# base_optimizer2 = optim.SGD\n",
    "# optimizer2 = PUGD2(model_ft2.parameters(),\n",
    "#                  base_optimizer2,\n",
    "#                  lr=args.lr,\n",
    "#                  momentum=args.momentum,\n",
    "#                  weight_decay=args.wd,\n",
    "#                  )\n",
    "\n",
    "\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer2, step_size=7, gamma=0.1)\n",
    "\n",
    "# model_ft2 = train_model2(model_ft2, criterion, optimizer2, exp_lr_scheduler, num_epochs=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bd8ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_epochs = 300\n",
    "# os.environ.update({ \n",
    "#     \"HF_ENDPOINT\": \"https://hf-mirror.com\", \n",
    "#     \"HF_HUB_OFFLINE\": \"0\",  # 确保非离线模式 \n",
    "#     \"HF_HUB_DISABLE_TELEMETRY\": \"1\",  # 禁用检测请求 \n",
    "#     \"HF_CDN_DOMAIN\": \"hf-mirror.com\", \n",
    "#     \"HF_S3_ENDPOINT\": \"hf-mirror.com\", \n",
    "#     \"TRANSFORMERS_OFFLINE\": \"0\",       # transformers专用设置 \n",
    "#     \"HF_DATASETS_OFFLINE\": \"0\"         # datasets专用设置 \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0696aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests \n",
    "# endpoints = [\"https://hf-mirror.com\",  \"https://huggingface.co\"] \n",
    "# for url in endpoints:\n",
    "#     try:\n",
    "#         r = requests.get(f\"{url}/api/models\",  timeout=5)\n",
    "#         print(f\"{url} 响应状态: {r.status_code}\") \n",
    "#     except Exception as e:\n",
    "#         print(f\"{url} 连接失败: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01616389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# ft_epochs = 300\n",
    "# 加载预训练ResNet18\n",
    "resnet18 = models.resnet18(weights=(\"pretrained\", models.ResNet18_Weights.IMAGENET1K_V1))\n",
    "\n",
    "# 冻结所有卷积层\n",
    "# for name, param in resnet18.named_parameters(): \n",
    "#     if 'fc' not in name:  # 排除最后的全连接层\n",
    "#         param.requires_grad  = False\n",
    "\n",
    "resnet18.fc  = torch.nn.Linear(resnet18.fc.in_features,  Num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9179dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_epochs = 200\n",
    "# resnet18_copy1 = copy.deepcopy(resnet18)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# resnet18_copy1.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(resnet18_copy1)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDX(resnet18_copy1.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=0.01,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ft_epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# resnet18_copy1, metrics0 = train_model(resnet18_copy1, criterion, optimizer, scheduler, ft_epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/lfs/\"+args.datasets+\"/resnet18_pugd\" + str(ft_epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': resnet18_copy1.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/resnet18_pugd_\" + str(ft_epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "86811ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "7.9486987590789795\n",
      "Epoch 1/199\n",
      "7.718895673751831\n",
      "Epoch 2/199\n",
      "7.610345363616943\n",
      "Epoch 3/199\n",
      "7.641237497329712\n",
      "Epoch 4/199\n",
      "7.762565612792969\n",
      "Epoch 5/199\n",
      "7.585090637207031\n",
      "Epoch 6/199\n",
      "7.547120094299316\n",
      "Epoch 7/199\n",
      "7.5596230030059814\n",
      "Epoch 8/199\n",
      "7.580765247344971\n",
      "Epoch 9/199\n",
      "7.517432928085327\n",
      "Epoch 10/199\n",
      "7.677819490432739\n",
      "Epoch 11/199\n",
      "7.545671224594116\n",
      "Epoch 12/199\n",
      "7.477651596069336\n",
      "Epoch 13/199\n",
      "7.516470670700073\n",
      "Epoch 14/199\n",
      "7.668240070343018\n",
      "Epoch 15/199\n",
      "7.642920017242432\n",
      "Epoch 16/199\n",
      "7.567986249923706\n",
      "Epoch 17/199\n",
      "7.747403621673584\n",
      "Epoch 18/199\n",
      "7.5731425285339355\n",
      "Epoch 19/199\n",
      "7.626915454864502\n",
      "Epoch 20/199\n",
      "7.711003303527832\n",
      "Epoch 21/199\n",
      "7.617855787277222\n",
      "Epoch 22/199\n",
      "7.660419225692749\n",
      "Epoch 23/199\n",
      "7.707245588302612\n",
      "Epoch 24/199\n",
      "7.6093223094940186\n",
      "Epoch 25/199\n",
      "7.852248191833496\n",
      "Epoch 26/199\n",
      "7.664438009262085\n",
      "Epoch 27/199\n",
      "7.582745552062988\n",
      "Epoch 28/199\n",
      "7.60415244102478\n",
      "Epoch 29/199\n",
      "7.571258306503296\n",
      "Epoch 30/199\n",
      "7.720505237579346\n",
      "Epoch 31/199\n",
      "7.6521315574646\n",
      "Epoch 32/199\n",
      "7.656255006790161\n",
      "Epoch 33/199\n",
      "7.634959697723389\n",
      "Epoch 34/199\n",
      "7.572678804397583\n",
      "Epoch 35/199\n",
      "7.597740411758423\n",
      "Epoch 36/199\n",
      "7.642399311065674\n",
      "Epoch 37/199\n",
      "7.82746148109436\n",
      "Epoch 38/199\n",
      "7.651800155639648\n",
      "Epoch 39/199\n",
      "7.627037286758423\n",
      "Epoch 40/199\n",
      "7.553174018859863\n",
      "Epoch 41/199\n",
      "7.561457872390747\n",
      "Epoch 42/199\n",
      "7.638269424438477\n",
      "Epoch 43/199\n",
      "7.797018527984619\n",
      "Epoch 44/199\n",
      "7.669379234313965\n",
      "Epoch 45/199\n",
      "7.641804456710815\n",
      "Epoch 46/199\n",
      "7.646132469177246\n",
      "Epoch 47/199\n",
      "7.625871181488037\n",
      "Epoch 48/199\n",
      "7.701638698577881\n",
      "Epoch 49/199\n",
      "7.689711570739746\n",
      "Epoch 50/199\n",
      "7.782562494277954\n",
      "Epoch 51/199\n",
      "7.632803678512573\n",
      "Epoch 52/199\n",
      "7.664702892303467\n",
      "Epoch 53/199\n",
      "7.6111626625061035\n",
      "Epoch 54/199\n",
      "7.586091995239258\n",
      "Epoch 55/199\n",
      "7.56037163734436\n",
      "Epoch 56/199\n",
      "7.660145282745361\n",
      "Epoch 57/199\n",
      "7.522663116455078\n",
      "Epoch 58/199\n",
      "8.103789567947388\n",
      "Epoch 59/199\n",
      "8.611554861068726\n",
      "Epoch 60/199\n",
      "8.583909511566162\n",
      "Epoch 61/199\n",
      "8.61980676651001\n",
      "Epoch 62/199\n",
      "8.591627597808838\n",
      "Epoch 63/199\n",
      "8.7138671875\n",
      "Epoch 64/199\n",
      "8.573951244354248\n",
      "Epoch 65/199\n",
      "8.766091346740723\n",
      "Epoch 66/199\n",
      "9.980558395385742\n",
      "Epoch 67/199\n",
      "9.39876651763916\n",
      "Epoch 68/199\n",
      "8.60404658317566\n",
      "Epoch 69/199\n",
      "8.800539493560791\n",
      "Epoch 70/199\n",
      "8.645386695861816\n",
      "Epoch 71/199\n",
      "8.675529479980469\n",
      "Epoch 72/199\n",
      "8.605525493621826\n",
      "Epoch 73/199\n",
      "8.677147150039673\n",
      "Epoch 74/199\n",
      "8.718072175979614\n",
      "Epoch 75/199\n",
      "8.58628225326538\n",
      "Epoch 76/199\n",
      "8.708846092224121\n",
      "Epoch 77/199\n",
      "8.548877477645874\n",
      "Epoch 78/199\n",
      "8.430103063583374\n",
      "Epoch 79/199\n",
      "8.407737970352173\n",
      "Epoch 80/199\n",
      "8.540912389755249\n",
      "Epoch 81/199\n",
      "8.557659387588501\n",
      "Epoch 82/199\n",
      "8.611100673675537\n",
      "Epoch 83/199\n",
      "8.69995927810669\n",
      "Epoch 84/199\n",
      "8.555399179458618\n",
      "Epoch 85/199\n",
      "8.499468088150024\n",
      "Epoch 86/199\n",
      "8.507892370223999\n",
      "Epoch 87/199\n",
      "8.566322803497314\n",
      "Epoch 88/199\n",
      "8.417264223098755\n",
      "Epoch 89/199\n",
      "8.65933632850647\n",
      "Epoch 90/199\n",
      "8.4258713722229\n",
      "Epoch 91/199\n",
      "8.444270849227905\n",
      "Epoch 92/199\n",
      "8.391901731491089\n",
      "Epoch 93/199\n",
      "8.401133298873901\n",
      "Epoch 94/199\n",
      "8.44955563545227\n",
      "Epoch 95/199\n",
      "8.451650619506836\n",
      "Epoch 96/199\n",
      "8.582944869995117\n",
      "Epoch 97/199\n",
      "8.452090740203857\n",
      "Epoch 98/199\n",
      "8.39652156829834\n",
      "Epoch 99/199\n",
      "8.366471767425537\n",
      "Epoch 100/199\n",
      "8.555997848510742\n",
      "Epoch 101/199\n",
      "8.459478616714478\n",
      "Epoch 102/199\n",
      "8.549712419509888\n",
      "Epoch 103/199\n",
      "8.451847791671753\n",
      "Epoch 104/199\n",
      "8.422375202178955\n",
      "Epoch 105/199\n",
      "8.414360761642456\n",
      "Epoch 106/199\n",
      "8.434656620025635\n",
      "Epoch 107/199\n",
      "8.460158586502075\n",
      "Epoch 108/199\n",
      "8.446070909500122\n",
      "Epoch 109/199\n",
      "8.574734449386597\n",
      "Epoch 110/199\n",
      "8.445111751556396\n",
      "Epoch 111/199\n",
      "8.463045358657837\n",
      "Epoch 112/199\n",
      "8.434025526046753\n",
      "Epoch 113/199\n",
      "8.43657922744751\n",
      "Epoch 114/199\n",
      "8.411459684371948\n",
      "Epoch 115/199\n",
      "8.596938610076904\n",
      "Epoch 116/199\n",
      "8.443912506103516\n",
      "Epoch 117/199\n",
      "8.446547746658325\n",
      "Epoch 118/199\n",
      "8.415081262588501\n",
      "Epoch 119/199\n",
      "8.4320969581604\n",
      "Epoch 120/199\n",
      "8.437007427215576\n",
      "Epoch 121/199\n",
      "8.475458860397339\n",
      "Epoch 122/199\n",
      "8.581064939498901\n",
      "Epoch 123/199\n",
      "8.468690872192383\n",
      "Epoch 124/199\n",
      "8.483392477035522\n",
      "Epoch 125/199\n",
      "8.409825325012207\n",
      "Epoch 126/199\n",
      "8.421265363693237\n",
      "Epoch 127/199\n",
      "8.45963740348816\n",
      "Epoch 128/199\n",
      "8.478801488876343\n",
      "Epoch 129/199\n",
      "8.60693645477295\n",
      "Epoch 130/199\n",
      "8.38189697265625\n",
      "Epoch 131/199\n",
      "8.443892002105713\n",
      "Epoch 132/199\n",
      "8.46066403388977\n",
      "Epoch 133/199\n",
      "8.48691201210022\n",
      "Epoch 134/199\n",
      "8.45881986618042\n",
      "Epoch 135/199\n",
      "8.542632818222046\n",
      "Epoch 136/199\n",
      "8.453117370605469\n",
      "Epoch 137/199\n",
      "8.445948362350464\n",
      "Epoch 138/199\n",
      "8.513916015625\n",
      "Epoch 139/199\n",
      "8.455013275146484\n",
      "Epoch 140/199\n",
      "8.40561580657959\n",
      "Epoch 141/199\n",
      "8.411129236221313\n",
      "Epoch 142/199\n",
      "8.567543745040894\n",
      "Epoch 143/199\n",
      "8.352638721466064\n",
      "Epoch 144/199\n",
      "8.390801906585693\n",
      "Epoch 145/199\n",
      "8.473467350006104\n",
      "Epoch 146/199\n",
      "8.455249309539795\n",
      "Epoch 147/199\n",
      "8.372874736785889\n",
      "Epoch 148/199\n",
      "8.551731586456299\n",
      "Epoch 149/199\n",
      "8.501481533050537\n",
      "Epoch 150/199\n",
      "8.461398363113403\n",
      "Epoch 151/199\n",
      "8.432355880737305\n",
      "Epoch 152/199\n",
      "8.488833904266357\n",
      "Epoch 153/199\n",
      "8.462848424911499\n",
      "Epoch 154/199\n",
      "8.400970458984375\n",
      "Epoch 155/199\n",
      "8.524590015411377\n",
      "Epoch 156/199\n",
      "8.406390905380249\n",
      "Epoch 157/199\n",
      "8.392243146896362\n",
      "Epoch 158/199\n",
      "8.436854839324951\n",
      "Epoch 159/199\n",
      "8.416931629180908\n",
      "Epoch 160/199\n",
      "8.42059326171875\n",
      "Epoch 161/199\n",
      "8.579981803894043\n",
      "Epoch 162/199\n",
      "8.46106219291687\n",
      "Epoch 163/199\n",
      "8.415457010269165\n",
      "Epoch 164/199\n",
      "8.376436948776245\n",
      "Epoch 165/199\n",
      "8.421191930770874\n",
      "Epoch 166/199\n",
      "8.422545671463013\n",
      "Epoch 167/199\n",
      "8.473563432693481\n",
      "Epoch 168/199\n",
      "8.587241411209106\n",
      "Epoch 169/199\n",
      "8.514767408370972\n",
      "Epoch 170/199\n",
      "8.396247625350952\n",
      "Epoch 171/199\n",
      "8.374934196472168\n",
      "Epoch 172/199\n",
      "8.400694370269775\n",
      "Epoch 173/199\n",
      "8.432822227478027\n",
      "Epoch 174/199\n",
      "8.412118911743164\n",
      "Epoch 175/199\n",
      "8.577237844467163\n",
      "Epoch 176/199\n",
      "8.373270273208618\n",
      "Epoch 177/199\n",
      "8.382606029510498\n",
      "Epoch 178/199\n",
      "8.424829959869385\n",
      "Epoch 179/199\n",
      "8.443701982498169\n",
      "Epoch 180/199\n",
      "8.392117977142334\n",
      "Epoch 181/199\n",
      "8.5670006275177\n",
      "Epoch 182/199\n",
      "8.507914066314697\n",
      "Epoch 183/199\n",
      "8.37766981124878\n",
      "Epoch 184/199\n",
      "8.426298141479492\n",
      "Epoch 185/199\n",
      "8.442342758178711\n",
      "Epoch 186/199\n",
      "8.46317172050476\n",
      "Epoch 187/199\n",
      "8.425986051559448\n",
      "Epoch 188/199\n",
      "8.620370626449585\n",
      "Epoch 189/199\n",
      "8.45082712173462\n",
      "Epoch 190/199\n",
      "8.312755346298218\n",
      "Epoch 191/199\n",
      "8.34675908088684\n",
      "Epoch 192/199\n",
      "8.376136064529419\n",
      "Epoch 193/199\n",
      "8.432214498519897\n",
      "Epoch 194/199\n",
      "8.601913213729858\n",
      "Epoch 195/199\n",
      "8.499768733978271\n",
      "Epoch 196/199\n",
      "8.355583667755127\n",
      "Epoch 197/199\n",
      "8.397804021835327\n",
      "Epoch 198/199\n",
      "8.424575328826904\n",
      "Epoch 199/199\n",
      "8.426179647445679\n",
      "Training complete in 27m 30s\n",
      "Best val Acc: 0.344200\n"
     ]
    }
   ],
   "source": [
    "ft_epochs = 200\n",
    "resnet18_copy1 = copy.deepcopy(resnet18)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "resnet18_copy1.to(device)\n",
    "\n",
    "if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "    model_ft_org = torch.nn.DataParallel(resnet18_copy1)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "base_optimizer = optim.SGD\n",
    "optimizer = PUGDXR(resnet18_copy1.parameters(),\n",
    "                base_optimizer,\n",
    "                lr=0.01,\n",
    "                max_epochs= args.epochs,\n",
    "                momentum=args.momentum,\n",
    "                weight_decay=args.wd,\n",
    "                min_beta = 8.0, \n",
    "                max_beta = 10,\n",
    "                method = 'sin',\n",
    "                dampening=0,   # 必须设置为0才能完全固定 \n",
    "                nesterov=False # 禁用Nesterov动量 \n",
    "                )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ft_epochs)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "resnet18_copy1, metrics0 = train_model(resnet18_copy1, criterion, optimizer, scheduler, ft_epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "model_path = \"./model/lfs/\"+args.datasets+\"/resnet18_pugdr_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(ft_epochs) + \".pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': resnet18_copy1.state_dict(), \n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, model_path) \n",
    "\n",
    "name = \"./results/\"+args.datasets+\"/resnet18_pugdr_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" +  str(ft_epochs) + \".json\"\n",
    "with open(name,  'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5026da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "8.548946857452393\n",
      "Epoch 1/199\n",
      "8.308665037155151\n",
      "Epoch 2/199\n",
      "8.276022911071777\n",
      "Epoch 3/199\n",
      "8.233057737350464\n",
      "Epoch 4/199\n",
      "8.509968996047974\n",
      "Epoch 5/199\n",
      "8.235347986221313\n",
      "Epoch 6/199\n",
      "8.270201444625854\n",
      "Epoch 7/199\n",
      "8.270421266555786\n",
      "Epoch 8/199\n",
      "8.259424686431885\n",
      "Epoch 9/199\n",
      "8.337769031524658\n",
      "Epoch 10/199\n",
      "8.267012119293213\n",
      "Epoch 11/199\n",
      "8.49018931388855\n",
      "Epoch 12/199\n",
      "8.328078031539917\n",
      "Epoch 13/199\n",
      "8.297568321228027\n",
      "Epoch 14/199\n",
      "8.33276104927063\n",
      "Epoch 15/199\n",
      "8.28247880935669\n",
      "Epoch 16/199\n",
      "8.250569820404053\n",
      "Epoch 17/199\n",
      "8.367702007293701\n",
      "Epoch 18/199\n",
      "8.340120792388916\n",
      "Epoch 19/199\n",
      "8.352041482925415\n",
      "Epoch 20/199\n",
      "8.286262273788452\n",
      "Epoch 21/199\n",
      "8.255422353744507\n",
      "Epoch 22/199\n",
      "8.388095140457153\n",
      "Epoch 23/199\n",
      "8.311641216278076\n",
      "Epoch 24/199\n",
      "8.519598245620728\n",
      "Epoch 25/199\n",
      "8.297900199890137\n",
      "Epoch 26/199\n",
      "8.290802001953125\n",
      "Epoch 27/199\n",
      "8.291116714477539\n",
      "Epoch 28/199\n",
      "8.329510927200317\n",
      "Epoch 29/199\n",
      "8.295830726623535\n",
      "Epoch 30/199\n",
      "8.41019868850708\n",
      "Epoch 31/199\n",
      "8.292865753173828\n",
      "Epoch 32/199\n",
      "8.306671857833862\n",
      "Epoch 33/199\n",
      "8.303105115890503\n",
      "Epoch 34/199\n",
      "8.278512239456177\n",
      "Epoch 35/199\n",
      "8.279516458511353\n",
      "Epoch 36/199\n",
      "8.297034740447998\n",
      "Epoch 37/199\n",
      "8.508018255233765\n",
      "Epoch 38/199\n",
      "8.305493116378784\n",
      "Epoch 39/199\n",
      "8.336890459060669\n",
      "Epoch 40/199\n",
      "8.328289031982422\n",
      "Epoch 41/199\n",
      "8.326951026916504\n",
      "Epoch 42/199\n",
      "8.334794044494629\n",
      "Epoch 43/199\n",
      "8.427163362503052\n",
      "Epoch 44/199\n",
      "8.34814453125\n",
      "Epoch 45/199\n",
      "8.35645866394043\n",
      "Epoch 46/199\n",
      "8.334250450134277\n",
      "Epoch 47/199\n",
      "8.290900230407715\n",
      "Epoch 48/199\n",
      "8.288495302200317\n",
      "Epoch 49/199\n",
      "8.317378997802734\n",
      "Epoch 50/199\n",
      "8.559122562408447\n",
      "Epoch 51/199\n",
      "8.3798246383667\n",
      "Epoch 52/199\n",
      "8.336641311645508\n",
      "Epoch 53/199\n",
      "8.306468963623047\n",
      "Epoch 54/199\n",
      "8.308212518692017\n",
      "Epoch 55/199\n",
      "8.315134525299072\n",
      "Epoch 56/199\n",
      "8.314874649047852\n",
      "Epoch 57/199\n",
      "8.437792539596558\n",
      "Epoch 58/199\n",
      "8.393093824386597\n",
      "Epoch 59/199\n",
      "8.353759050369263\n",
      "Epoch 60/199\n",
      "8.30286693572998\n",
      "Epoch 61/199\n",
      "8.37356162071228\n",
      "Epoch 62/199\n",
      "8.23661994934082\n",
      "Epoch 63/199\n",
      "8.324804544448853\n",
      "Epoch 64/199\n",
      "8.331979513168335\n",
      "Epoch 65/199\n",
      "8.32191777229309\n",
      "Epoch 66/199\n",
      "8.354347229003906\n",
      "Epoch 67/199\n",
      "8.28943920135498\n",
      "Epoch 68/199\n",
      "8.38097858428955\n",
      "Epoch 69/199\n",
      "8.342765808105469\n",
      "Epoch 70/199\n",
      "8.428234577178955\n",
      "Epoch 71/199\n",
      "8.314963817596436\n",
      "Epoch 72/199\n",
      "8.293996572494507\n",
      "Epoch 73/199\n",
      "8.320376873016357\n",
      "Epoch 74/199\n",
      "8.35466742515564\n",
      "Epoch 75/199\n",
      "8.324860572814941\n",
      "Epoch 76/199\n",
      "8.460288047790527\n",
      "Epoch 77/199\n",
      "8.303956508636475\n",
      "Epoch 78/199\n",
      "8.367995738983154\n",
      "Epoch 79/199\n",
      "8.344437599182129\n",
      "Epoch 80/199\n",
      "8.354136943817139\n",
      "Epoch 81/199\n",
      "8.337260723114014\n",
      "Epoch 82/199\n",
      "8.348695755004883\n",
      "Epoch 83/199\n",
      "8.4743173122406\n",
      "Epoch 84/199\n",
      "8.328318357467651\n",
      "Epoch 85/199\n",
      "8.327247619628906\n",
      "Epoch 86/199\n",
      "8.333333969116211\n",
      "Epoch 87/199\n",
      "8.279990911483765\n",
      "Epoch 88/199\n",
      "8.373249292373657\n",
      "Epoch 89/199\n",
      "8.428706169128418\n",
      "Epoch 90/199\n",
      "8.316095352172852\n",
      "Epoch 91/199\n",
      "8.33650803565979\n",
      "Epoch 92/199\n",
      "8.461989641189575\n",
      "Epoch 93/199\n",
      "8.338058233261108\n",
      "Epoch 94/199\n",
      "8.356575012207031\n",
      "Epoch 95/199\n",
      "8.336138010025024\n",
      "Epoch 96/199\n",
      "8.469772100448608\n",
      "Epoch 97/199\n",
      "8.312931776046753\n",
      "Epoch 98/199\n",
      "8.327372074127197\n",
      "Epoch 99/199\n",
      "8.321717500686646\n",
      "Epoch 100/199\n",
      "8.31208086013794\n",
      "Epoch 101/199\n",
      "8.316880941390991\n",
      "Epoch 102/199\n",
      "8.44944715499878\n",
      "Epoch 103/199\n",
      "8.328729152679443\n",
      "Epoch 104/199\n",
      "8.309350967407227\n",
      "Epoch 105/199\n",
      "8.340990781784058\n",
      "Epoch 106/199\n",
      "8.380534172058105\n",
      "Epoch 107/199\n",
      "8.318424701690674\n",
      "Epoch 108/199\n",
      "8.379610538482666\n",
      "Epoch 109/199\n",
      "8.513201713562012\n",
      "Epoch 110/199\n",
      "8.42783236503601\n",
      "Epoch 111/199\n",
      "8.498906373977661\n",
      "Epoch 112/199\n",
      "8.360227346420288\n",
      "Epoch 113/199\n",
      "8.285143375396729\n",
      "Epoch 114/199\n",
      "8.3629150390625\n",
      "Epoch 115/199\n",
      "8.479809761047363\n",
      "Epoch 116/199\n",
      "8.313649415969849\n",
      "Epoch 117/199\n",
      "8.324656248092651\n",
      "Epoch 118/199\n",
      "8.364086151123047\n",
      "Epoch 119/199\n",
      "8.34276533126831\n",
      "Epoch 120/199\n",
      "8.26771879196167\n",
      "Epoch 121/199\n",
      "8.306814670562744\n",
      "Epoch 122/199\n",
      "8.445045232772827\n",
      "Epoch 123/199\n",
      "8.315980672836304\n",
      "Epoch 124/199\n",
      "8.292350053787231\n",
      "Epoch 125/199\n",
      "8.272505760192871\n",
      "Epoch 126/199\n",
      "8.30775499343872\n",
      "Epoch 127/199\n",
      "8.326387166976929\n",
      "Epoch 128/199\n",
      "8.323131084442139\n",
      "Epoch 129/199\n",
      "8.462582349777222\n",
      "Epoch 130/199\n",
      "8.27889084815979\n",
      "Epoch 131/199\n",
      "8.28913950920105\n",
      "Epoch 132/199\n",
      "8.275294780731201\n",
      "Epoch 133/199\n",
      "8.348351240158081\n",
      "Epoch 134/199\n",
      "8.21851372718811\n",
      "Epoch 135/199\n",
      "8.364858865737915\n",
      "Epoch 136/199\n",
      "8.297407388687134\n",
      "Epoch 137/199\n",
      "8.32668685913086\n",
      "Epoch 138/199\n",
      "8.336178302764893\n",
      "Epoch 139/199\n",
      "8.316236734390259\n",
      "Epoch 140/199\n",
      "8.26027250289917\n",
      "Epoch 141/199\n",
      "8.304881572723389\n",
      "Epoch 142/199\n",
      "8.436110258102417\n",
      "Epoch 143/199\n",
      "8.378878831863403\n",
      "Epoch 144/199\n",
      "8.34999704360962\n",
      "Epoch 145/199\n",
      "8.322342157363892\n",
      "Epoch 146/199\n",
      "8.218117475509644\n",
      "Epoch 147/199\n",
      "8.221741676330566\n",
      "Epoch 148/199\n",
      "8.307114124298096\n",
      "Epoch 149/199\n",
      "8.240538358688354\n",
      "Epoch 150/199\n",
      "8.337193489074707\n",
      "Epoch 151/199\n",
      "8.358237504959106\n",
      "Epoch 152/199\n",
      "8.33055067062378\n",
      "Epoch 153/199\n",
      "8.317917823791504\n",
      "Epoch 154/199\n",
      "8.282870769500732\n",
      "Epoch 155/199\n",
      "8.387262105941772\n",
      "Epoch 156/199\n",
      "8.28003716468811\n",
      "Epoch 157/199\n",
      "8.329182147979736\n",
      "Epoch 158/199\n",
      "8.334031105041504\n",
      "Epoch 159/199\n",
      "8.364331722259521\n",
      "Epoch 160/199\n",
      "8.353351831436157\n",
      "Epoch 161/199\n",
      "8.328809976577759\n",
      "Epoch 162/199\n",
      "8.472034215927124\n",
      "Epoch 163/199\n",
      "8.297499179840088\n",
      "Epoch 164/199\n",
      "8.331144094467163\n",
      "Epoch 165/199\n",
      "8.365429878234863\n",
      "Epoch 166/199\n",
      "8.336947679519653\n",
      "Epoch 167/199\n",
      "8.296111106872559\n",
      "Epoch 168/199\n",
      "8.46268343925476\n",
      "Epoch 169/199\n",
      "8.33730959892273\n",
      "Epoch 170/199\n",
      "8.31646466255188\n",
      "Epoch 171/199\n",
      "8.32715106010437\n",
      "Epoch 172/199\n",
      "8.28373908996582\n",
      "Epoch 173/199\n",
      "8.339701414108276\n",
      "Epoch 174/199\n",
      "8.294489860534668\n",
      "Epoch 175/199\n",
      "8.429085731506348\n",
      "Epoch 176/199\n",
      "8.31011152267456\n",
      "Epoch 177/199\n",
      "8.296727180480957\n",
      "Epoch 178/199\n",
      "8.27794861793518\n",
      "Epoch 179/199\n",
      "8.350597620010376\n",
      "Epoch 180/199\n",
      "8.2838716506958\n",
      "Epoch 181/199\n",
      "8.509835481643677\n",
      "Epoch 182/199\n",
      "8.285062551498413\n",
      "Epoch 183/199\n",
      "8.26602816581726\n",
      "Epoch 184/199\n",
      "8.296029567718506\n",
      "Epoch 185/199\n",
      "8.315427303314209\n",
      "Epoch 186/199\n",
      "8.32392144203186\n",
      "Epoch 187/199\n",
      "8.290450811386108\n",
      "Epoch 188/199\n",
      "8.453890085220337\n",
      "Epoch 189/199\n",
      "8.248587846755981\n",
      "Epoch 190/199\n",
      "8.360361337661743\n",
      "Epoch 191/199\n",
      "8.3395254611969\n",
      "Epoch 192/199\n",
      "8.280665397644043\n",
      "Epoch 193/199\n",
      "8.324029684066772\n",
      "Epoch 194/199\n",
      "8.455472230911255\n",
      "Epoch 195/199\n",
      "8.197776317596436\n",
      "Epoch 196/199\n",
      "8.25514006614685\n",
      "Epoch 197/199\n",
      "8.328797578811646\n",
      "Epoch 198/199\n",
      "8.423105478286743\n",
      "Epoch 199/199\n",
      "8.28508186340332\n",
      "Training complete in 27m 48s\n",
      "Best val Acc: 0.576600\n"
     ]
    }
   ],
   "source": [
    "ft_epochs = 200\n",
    "resnet18_copy1 = copy.deepcopy(resnet18)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "resnet18_copy1.to(device)\n",
    "\n",
    "if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "    model_ft_org = torch.nn.DataParallel(resnet18_copy1)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "base_optimizer = optim.SGD\n",
    "optimizer = PUGDXR(resnet18_copy1.parameters(),\n",
    "                base_optimizer,\n",
    "                lr=0.01,\n",
    "                max_epochs= args.epochs,\n",
    "                momentum=args.momentum,\n",
    "                weight_decay=args.wd,\n",
    "                min_beta = 3.0, \n",
    "                max_beta = 10,\n",
    "                method = 'sin',\n",
    "                dampening=0,   # 必须设置为0才能完全固定 \n",
    "                nesterov=False # 禁用Nesterov动量 \n",
    "                )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ft_epochs)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "resnet18_copy1, metrics0 = train_model(resnet18_copy1, criterion, optimizer, scheduler, ft_epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "model_path = \"./model/lfs/\"+args.datasets+\"/resnet18_pugdr_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(ft_epochs) + \".pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': resnet18_copy1.state_dict(), \n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, model_path) \n",
    "\n",
    "name = \"./results/\"+args.datasets+\"/resnet18_pugdr_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" +  str(ft_epochs) + \".json\"\n",
    "with open(name,  'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1eab5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "# ft_epochs = 200\n",
    "# 加载预训练ViT-B/16\n",
    "vit = timm.create_model('google/vit_base_patch16_224',  pretrained=True, \n",
    "                        img_size = 32, patch_size = 8, num_classes = Num_class)\n",
    "\n",
    "# vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224',\n",
    "#                                                 img_size = 32, patch_size = 8, num_classes = Num_class)\n",
    "# 冻结除head外的所有层\n",
    "# for name, param in vit.named_parameters(): \n",
    "#     if 'heads.head'  not in name:  # 仅保留分类头可训练\n",
    "#         param.requires_grad  = False\n",
    "\n",
    "# vit.heads.head  = torch.nn.Linear(vit.heads.head.in_features,  Num_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f1b21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_epochs = 200\n",
    "# vit_copy1 = copy.deepcopy(vit)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# vit_copy1.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(vit_copy1)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDX(vit_copy1.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=0.001,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ft_epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# vit_copy1, metrics0 = train_model(vit_copy1, criterion, optimizer, scheduler, ft_epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/lfs/\"+args.datasets+\"/vit_pugd\" + str(ft_epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': vit_copy1.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/vit_pugd_\" + str(ft_epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "87.52443957328796\n",
      "Epoch 1/199\n",
      "87.64813780784607\n",
      "Epoch 2/199\n",
      "87.85969877243042\n",
      "Epoch 3/199\n",
      "87.91674661636353\n",
      "Epoch 4/199\n",
      "88.03079319000244\n",
      "Epoch 5/199\n",
      "88.0077178478241\n",
      "Epoch 6/199\n",
      "88.14463138580322\n",
      "Epoch 7/199\n",
      "88.13892984390259\n",
      "Epoch 8/199\n",
      "88.05786466598511\n",
      "Epoch 9/199\n",
      "88.08397722244263\n",
      "Epoch 10/199\n",
      "87.71901392936707\n",
      "Epoch 11/199\n",
      "87.54135656356812\n",
      "Epoch 12/199\n",
      "87.53413414955139\n",
      "Epoch 13/199\n",
      "87.49227046966553\n",
      "Epoch 14/199\n",
      "87.39002585411072\n",
      "Epoch 15/199\n",
      "87.31794786453247\n",
      "Epoch 16/199\n",
      "87.40021300315857\n",
      "Epoch 17/199\n",
      "87.21010255813599\n",
      "Epoch 18/199\n",
      "87.1718647480011\n",
      "Epoch 19/199\n",
      "87.29301738739014\n",
      "Epoch 20/199\n",
      "87.22142457962036\n",
      "Epoch 21/199\n",
      "87.14562630653381\n",
      "Epoch 22/199\n",
      "87.1909716129303\n",
      "Epoch 23/199\n",
      "87.10944104194641\n",
      "Epoch 24/199\n",
      "87.18378496170044\n",
      "Epoch 25/199\n",
      "87.22574663162231\n",
      "Epoch 26/199\n",
      "87.03789138793945\n",
      "Epoch 27/199\n",
      "87.06304979324341\n",
      "Epoch 28/199\n",
      "87.09196758270264\n",
      "Epoch 29/199\n",
      "87.12662148475647\n",
      "Epoch 30/199\n",
      "87.09062719345093\n",
      "Epoch 31/199\n",
      "87.29795742034912\n",
      "Epoch 32/199\n",
      "87.10976719856262\n",
      "Epoch 33/199\n",
      "87.08235287666321\n",
      "Epoch 34/199\n",
      "87.03283476829529\n",
      "Epoch 35/199\n",
      "87.01351284980774\n",
      "Epoch 36/199\n",
      "87.10957741737366\n",
      "Epoch 37/199\n",
      "87.22311902046204\n",
      "Epoch 38/199\n",
      "86.97917914390564\n",
      "Epoch 39/199\n",
      "87.07256436347961\n",
      "Epoch 40/199\n",
      "87.08959674835205\n",
      "Epoch 41/199\n",
      "87.11988615989685\n",
      "Epoch 42/199\n",
      "87.11461234092712\n",
      "Epoch 43/199\n",
      "87.18505907058716\n",
      "Epoch 44/199\n",
      "87.15808701515198\n",
      "Epoch 45/199\n",
      "87.03218507766724\n",
      "Epoch 46/199\n",
      "87.12647438049316\n",
      "Epoch 47/199\n",
      "87.27344107627869\n",
      "Epoch 48/199\n",
      "87.43262720108032\n",
      "Epoch 49/199\n",
      "87.84804177284241\n",
      "Epoch 50/199\n",
      "87.7036988735199\n",
      "Epoch 51/199\n",
      "87.7529399394989\n",
      "Epoch 52/199\n",
      "87.92923402786255\n",
      "Epoch 53/199\n",
      "87.83518862724304\n",
      "Epoch 54/199\n",
      "88.00349712371826\n",
      "Epoch 55/199\n",
      "88.06847953796387\n",
      "Epoch 56/199\n",
      "88.1346447467804\n",
      "Epoch 57/199\n",
      "88.0763680934906\n",
      "Epoch 58/199\n",
      "88.13480377197266\n",
      "Epoch 59/199\n",
      "88.11989092826843\n",
      "Epoch 60/199\n",
      "88.14425778388977\n",
      "Epoch 61/199\n",
      "88.1720552444458\n",
      "Epoch 62/199\n",
      "88.38527059555054\n",
      "Epoch 63/199\n",
      "88.11071586608887\n",
      "Epoch 64/199\n",
      "88.12210702896118\n",
      "Epoch 65/199\n",
      "88.23503422737122\n",
      "Epoch 66/199\n",
      "88.22247624397278\n",
      "Epoch 67/199\n",
      "88.27139210700989\n",
      "Epoch 68/199\n",
      "88.44927406311035\n",
      "Epoch 69/199\n",
      "88.23173499107361\n",
      "Epoch 70/199\n",
      "88.22972202301025\n",
      "Epoch 71/199\n",
      "88.1552734375\n",
      "Epoch 72/199\n",
      "88.17188692092896\n",
      "Epoch 73/199\n",
      "88.20573616027832\n",
      "Epoch 74/199\n",
      "88.37442231178284\n",
      "Epoch 75/199\n",
      "88.27335381507874\n",
      "Epoch 76/199\n",
      "88.20407032966614\n",
      "Epoch 77/199\n",
      "88.20423007011414\n",
      "Epoch 78/199\n",
      "88.0095419883728\n",
      "Epoch 79/199\n",
      "87.76091051101685\n",
      "Epoch 80/199\n",
      "87.70790767669678\n",
      "Epoch 81/199\n",
      "87.63356590270996\n",
      "Epoch 82/199\n",
      "87.53798604011536\n",
      "Epoch 83/199\n",
      "87.48229837417603\n",
      "Epoch 84/199\n",
      "87.32829284667969\n",
      "Epoch 85/199\n",
      "87.37054872512817\n",
      "Epoch 86/199\n",
      "87.40139484405518\n",
      "Epoch 87/199\n",
      "87.16110610961914\n",
      "Epoch 88/199\n",
      "87.12494587898254\n",
      "Epoch 89/199\n",
      "87.20658898353577\n",
      "Epoch 90/199\n",
      "87.12952041625977\n",
      "Epoch 91/199\n",
      "87.08459997177124\n",
      "Epoch 92/199\n",
      "87.09939646720886\n",
      "Epoch 93/199\n",
      "87.2646312713623\n",
      "Epoch 94/199\n",
      "87.12643814086914\n",
      "Epoch 95/199\n",
      "87.05658435821533\n",
      "Epoch 96/199\n",
      "87.12890982627869\n",
      "Epoch 97/199\n",
      "87.10936188697815\n",
      "Epoch 98/199\n",
      "87.1847562789917\n",
      "Epoch 99/199\n",
      "87.3553695678711\n",
      "Epoch 100/199\n",
      "87.19459533691406\n",
      "Epoch 101/199\n",
      "87.13107132911682\n",
      "Epoch 102/199\n",
      "87.122873544693\n",
      "Epoch 103/199\n",
      "87.31784772872925\n",
      "Epoch 104/199\n",
      "87.51941347122192\n",
      "Epoch 105/199\n",
      "87.71335291862488\n",
      "Epoch 106/199\n",
      "87.68042850494385\n",
      "Epoch 107/199\n",
      "87.82415890693665\n",
      "Epoch 108/199\n",
      "87.963862657547\n",
      "Epoch 109/199\n",
      "87.85078144073486\n",
      "Epoch 110/199\n",
      "88.00937509536743\n",
      "Epoch 111/199\n",
      "88.224050283432\n",
      "Epoch 112/199\n",
      "88.0697979927063\n",
      "Epoch 113/199\n",
      "88.11577916145325\n",
      "Epoch 114/199\n",
      "88.05143356323242\n",
      "Epoch 115/199\n",
      "88.12521362304688\n",
      "Epoch 116/199\n",
      "88.1996865272522\n",
      "Epoch 117/199\n",
      "88.26352548599243\n",
      "Epoch 118/199\n",
      "88.09097790718079\n",
      "Epoch 119/199\n",
      "88.19735908508301\n",
      "Epoch 120/199\n",
      "88.21412420272827\n",
      "Epoch 121/199\n",
      "88.24130201339722\n",
      "Epoch 122/199\n",
      "88.1744704246521\n",
      "Epoch 123/199\n",
      "88.23523998260498\n",
      "Epoch 124/199\n",
      "88.29634141921997\n",
      "Epoch 125/199\n",
      "88.31239104270935\n",
      "Epoch 126/199\n",
      "88.24561595916748\n",
      "Epoch 127/199\n",
      "88.20490169525146\n",
      "Epoch 128/199\n",
      "88.24193263053894\n",
      "Epoch 129/199\n",
      "88.30527520179749\n",
      "Epoch 130/199\n",
      "88.38467764854431\n",
      "Epoch 131/199\n",
      "88.30525612831116\n",
      "Epoch 132/199\n",
      "88.23394894599915\n",
      "Epoch 133/199\n",
      "88.25737047195435\n",
      "Epoch 134/199\n",
      "88.25615620613098\n",
      "Epoch 135/199\n",
      "88.29567551612854\n",
      "Epoch 136/199\n",
      "88.44041967391968\n",
      "Epoch 137/199\n",
      "88.27567625045776\n",
      "Epoch 138/199\n",
      "88.31137776374817\n",
      "Epoch 139/199\n",
      "88.32402920722961\n",
      "Epoch 140/199\n",
      "88.39405012130737\n",
      "Epoch 141/199\n",
      "88.34196424484253\n",
      "Epoch 142/199\n",
      "88.45957827568054\n",
      "Epoch 143/199\n",
      "88.2110493183136\n",
      "Epoch 144/199\n",
      "88.3639132976532\n",
      "Epoch 145/199\n",
      "88.26891088485718\n",
      "Epoch 146/199\n",
      "88.35869812965393\n",
      "Epoch 147/199\n",
      "88.3750331401825\n",
      "Epoch 148/199\n",
      "88.48258090019226\n",
      "Epoch 149/199\n",
      "88.34892058372498\n",
      "Epoch 150/199\n",
      "88.34586882591248\n",
      "Epoch 151/199\n",
      "88.29168701171875\n",
      "Epoch 152/199\n",
      "88.23170709609985\n",
      "Epoch 153/199\n",
      "88.26237535476685\n",
      "Epoch 154/199\n",
      "88.26198863983154\n",
      "Epoch 155/199\n",
      "88.54247665405273\n",
      "Epoch 156/199\n",
      "88.22176790237427\n",
      "Epoch 157/199\n",
      "88.3167028427124\n",
      "Epoch 158/199\n",
      "88.37290239334106\n",
      "Epoch 159/199\n",
      "88.38273477554321\n",
      "Epoch 160/199\n",
      "88.3241319656372\n",
      "Epoch 161/199\n",
      "88.49181723594666\n",
      "Epoch 162/199\n",
      "88.30506134033203\n",
      "Epoch 163/199\n",
      "88.29925799369812\n",
      "Epoch 164/199\n",
      "88.30316233634949\n",
      "Epoch 165/199\n",
      "88.3762686252594\n",
      "Epoch 166/199\n",
      "88.34687805175781\n",
      "Epoch 167/199\n",
      "88.52553820610046\n",
      "Epoch 168/199\n",
      "88.35351610183716\n",
      "Epoch 169/199\n",
      "88.36799025535583\n",
      "Epoch 170/199\n",
      "88.47403502464294\n",
      "Epoch 171/199\n",
      "88.28691124916077\n",
      "Epoch 172/199\n",
      "88.4073896408081\n",
      "Epoch 173/199\n",
      "88.47256803512573\n",
      "Epoch 174/199\n",
      "88.44086384773254\n",
      "Epoch 175/199\n",
      "88.39425230026245\n",
      "Epoch 176/199\n",
      "88.49564957618713\n",
      "Epoch 177/199\n",
      "88.4804515838623\n",
      "Epoch 178/199\n",
      "88.46555614471436\n",
      "Epoch 179/199\n",
      "88.6223738193512\n",
      "Epoch 180/199\n",
      "88.54327511787415\n",
      "Epoch 181/199\n",
      "88.37915229797363\n",
      "Epoch 182/199\n",
      "88.39110207557678\n",
      "Epoch 183/199\n",
      "88.3778567314148\n",
      "Epoch 184/199\n",
      "88.41498947143555\n",
      "Epoch 185/199\n",
      "88.60730957984924\n",
      "Epoch 186/199\n",
      "88.45551419258118\n",
      "Epoch 187/199\n",
      "88.43801617622375\n",
      "Epoch 188/199\n",
      "88.44294738769531\n",
      "Epoch 189/199\n",
      "88.46090197563171\n",
      "Epoch 190/199\n",
      "88.51996326446533\n",
      "Epoch 191/199\n",
      "88.52210879325867\n",
      "Epoch 192/199\n",
      "88.68050646781921\n",
      "Epoch 193/199\n",
      "88.45903944969177\n",
      "Epoch 194/199\n",
      "88.36785316467285\n",
      "Epoch 195/199\n",
      "88.4470808506012\n",
      "Epoch 196/199\n",
      "88.47199726104736\n",
      "Epoch 197/199\n",
      "88.61026453971863\n",
      "Epoch 198/199\n",
      "88.4758951663971\n",
      "Epoch 199/199\n",
      "88.39393496513367\n",
      "Training complete in 293m 4s\n",
      "Best val Acc: 0.867800\n"
     ]
    }
   ],
   "source": [
    "# ft_epochs = 200\n",
    "# vit_copy1 = copy.deepcopy(vit)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# vit_copy1.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(vit_copy1)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(vit_copy1.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=0.001,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 0.2, \n",
    "#                 max_beta = 1.0, \n",
    "#                 method = 'icos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ft_epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# vit_copy1, metrics0 = train_model(vit_copy1, criterion, optimizer, scheduler, ft_epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/lfs/\"+args.datasets+\"/vit_pugdr_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(ft_epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': vit_copy1.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/vit_pugdr_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\" + str(ft_epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b1a7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 加载预训练DeiT-B/16\n",
    "deit = timm.create_model('deit_base_patch16_224', pretrained=True, \n",
    "                        img_size = 32, patch_size = 8, num_classes = Num_class)\n",
    "# # 冻结所有层除了分类token和head\n",
    "# for name, param in deit.named_parameters(): \n",
    "#     if not name.startswith(('cls_token',  'pos_embed', 'head')):\n",
    "#         param.requires_grad  = False\n",
    "\n",
    "# # 修改分类头（20类任务示例）\n",
    "# deit.head  = torch.nn.Linear(deit.head.in_features,  Num_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bff32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_epochs = 200\n",
    "# deit_copy1 = copy.deepcopy(deit)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# deit_copy1.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(deit_copy1)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDX(deit_copy1.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=0.0005,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ft_epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# deit_copy1, metrics0 = train_model(deit_copy1, criterion, optimizer, scheduler, ft_epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/lfs/\"+args.datasets+\"/deit_pugd\" + str(ft_epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': deit_copy1.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/deit_pugd_\" + str(ft_epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f7516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "86.04767322540283\n",
      "Epoch 1/199\n",
      "86.55287265777588\n",
      "Epoch 2/199\n",
      "86.85405659675598\n",
      "Epoch 3/199\n",
      "87.78455400466919\n",
      "Epoch 4/199\n",
      "87.73693823814392\n",
      "Epoch 5/199\n",
      "87.81835579872131\n",
      "Epoch 6/199\n",
      "87.86040329933167\n",
      "Epoch 7/199\n",
      "87.98902416229248\n",
      "Epoch 8/199\n",
      "87.98691248893738\n",
      "Epoch 9/199\n",
      "88.15820002555847\n",
      "Epoch 10/199\n",
      "88.50795817375183\n",
      "Epoch 11/199\n",
      "88.45399212837219\n",
      "Epoch 12/199\n",
      "88.91228461265564\n",
      "Epoch 13/199\n",
      "88.84910726547241\n",
      "Epoch 14/199\n",
      "88.96572160720825\n",
      "Epoch 15/199\n",
      "88.99120712280273\n",
      "Epoch 16/199\n",
      "88.71271848678589\n",
      "Epoch 17/199\n",
      "88.63783001899719\n",
      "Epoch 18/199\n",
      "88.8223295211792\n",
      "Epoch 19/199\n",
      "88.77399277687073\n",
      "Epoch 20/199\n",
      "88.63983988761902\n",
      "Epoch 21/199\n",
      "88.75344896316528\n",
      "Epoch 22/199\n",
      "88.71014189720154\n",
      "Epoch 23/199\n",
      "88.57431960105896\n",
      "Epoch 24/199\n",
      "88.50974893569946\n",
      "Epoch 25/199\n",
      "89.04600811004639\n",
      "Epoch 26/199\n",
      "88.87741780281067\n",
      "Epoch 27/199\n",
      "88.80967283248901\n",
      "Epoch 28/199\n",
      "88.90060544013977\n",
      "Epoch 29/199\n",
      "88.65234112739563\n",
      "Epoch 30/199\n",
      "88.79205870628357\n",
      "Epoch 31/199\n",
      "89.05069994926453\n",
      "Epoch 32/199\n",
      "88.76165437698364\n",
      "Epoch 33/199\n",
      "88.91933941841125\n",
      "Epoch 34/199\n",
      "88.80628991127014\n",
      "Epoch 35/199\n",
      "88.68189358711243\n",
      "Epoch 36/199\n",
      "88.6657145023346\n",
      "Epoch 37/199\n",
      "88.72477412223816\n",
      "Epoch 38/199\n",
      "88.7279794216156\n",
      "Epoch 39/199\n",
      "88.6339385509491\n",
      "Epoch 40/199\n",
      "88.92425107955933\n",
      "Epoch 41/199\n",
      "88.93890357017517\n",
      "Epoch 42/199\n",
      "88.89339184761047\n",
      "Epoch 43/199\n",
      "88.50754356384277\n",
      "Epoch 44/199\n",
      "88.92005443572998\n",
      "Epoch 45/199\n",
      "88.57636547088623\n",
      "Epoch 46/199\n",
      "88.7222809791565\n",
      "Epoch 47/199\n",
      "88.93759274482727\n",
      "Epoch 48/199\n",
      "88.80676555633545\n",
      "Epoch 49/199\n",
      "89.13350224494934\n",
      "Epoch 50/199\n",
      "89.0685703754425\n",
      "Epoch 51/199\n",
      "89.0783679485321\n",
      "Epoch 52/199\n",
      "89.22726821899414\n",
      "Epoch 53/199\n",
      "89.06547331809998\n",
      "Epoch 54/199\n",
      "88.924067735672\n",
      "Epoch 55/199\n",
      "89.20259952545166\n",
      "Epoch 56/199\n",
      "88.8800642490387\n",
      "Epoch 57/199\n",
      "89.03986406326294\n",
      "Epoch 58/199\n",
      "88.3950126171112\n",
      "Epoch 59/199\n",
      "88.18533110618591\n",
      "Epoch 60/199\n",
      "87.65286636352539\n",
      "Epoch 61/199\n",
      "87.67477011680603\n",
      "Epoch 62/199\n",
      "87.45668578147888\n",
      "Epoch 63/199\n",
      "87.4923164844513\n",
      "Epoch 64/199\n",
      "87.42644190788269\n",
      "Epoch 65/199\n",
      "87.42922306060791\n",
      "Epoch 66/199\n",
      "87.42258381843567\n",
      "Epoch 67/199\n",
      "87.9996726512909\n",
      "Epoch 68/199\n",
      "88.42681694030762\n",
      "Epoch 69/199\n",
      "88.60102939605713\n",
      "Epoch 70/199\n",
      "88.51579570770264\n",
      "Epoch 71/199\n",
      "88.8147828578949\n",
      "Epoch 72/199\n",
      "88.678546667099\n",
      "Epoch 73/199\n",
      "88.6354432106018\n",
      "Epoch 74/199\n",
      "88.42633605003357\n",
      "Epoch 75/199\n",
      "88.66278862953186\n",
      "Epoch 76/199\n",
      "88.65636253356934\n",
      "Epoch 77/199\n",
      "88.83359122276306\n",
      "Epoch 78/199\n",
      "88.66306948661804\n",
      "Epoch 79/199\n",
      "88.69842314720154\n",
      "Epoch 80/199\n",
      "88.45990586280823\n",
      "Epoch 81/199\n",
      "88.52356958389282\n",
      "Epoch 82/199\n",
      "88.12344598770142\n",
      "Epoch 83/199\n",
      "88.13743877410889\n",
      "Epoch 84/199\n",
      "88.1143798828125\n",
      "Epoch 85/199\n",
      "88.05277824401855\n",
      "Epoch 86/199\n",
      "88.08200526237488\n",
      "Epoch 87/199\n",
      "88.01618790626526\n",
      "Epoch 88/199\n",
      "88.11073732376099\n",
      "Epoch 89/199\n",
      "88.18832898139954\n",
      "Epoch 90/199\n",
      "88.15425491333008\n",
      "Epoch 91/199\n",
      "87.8442771434784\n",
      "Epoch 92/199\n",
      "87.6931164264679\n",
      "Epoch 93/199\n",
      "87.59935140609741\n",
      "Epoch 94/199\n",
      "87.93198609352112\n",
      "Epoch 95/199\n",
      "88.35556721687317\n",
      "Epoch 96/199\n",
      "88.27829146385193\n",
      "Epoch 97/199\n",
      "88.60828375816345\n",
      "Epoch 98/199\n",
      "88.68801927566528\n",
      "Epoch 99/199\n",
      "88.72392129898071\n",
      "Epoch 100/199\n",
      "88.59222388267517\n",
      "Epoch 101/199\n",
      "88.60156798362732\n",
      "Epoch 102/199\n",
      "88.63823366165161\n",
      "Epoch 103/199\n",
      "88.74915146827698\n",
      "Epoch 104/199\n",
      "88.62860536575317\n",
      "Epoch 105/199\n",
      "88.71277499198914\n",
      "Epoch 106/199\n",
      "88.81498265266418\n",
      "Epoch 107/199\n",
      "88.3121988773346\n",
      "Epoch 108/199\n",
      "88.83412647247314\n",
      "Epoch 109/199\n",
      "88.95366287231445\n",
      "Epoch 110/199\n",
      "88.61593675613403\n",
      "Epoch 111/199\n",
      "89.02783679962158\n",
      "Epoch 112/199\n",
      "88.60213351249695\n",
      "Epoch 113/199\n",
      "88.70384693145752\n",
      "Epoch 114/199\n",
      "89.09276366233826\n",
      "Epoch 115/199\n",
      "88.76833844184875\n",
      "Epoch 116/199\n",
      "88.65203619003296\n",
      "Epoch 117/199\n",
      "88.59634280204773\n",
      "Epoch 118/199\n",
      "88.89876985549927\n",
      "Epoch 119/199\n",
      "89.01991391181946\n",
      "Epoch 120/199\n",
      "89.06772422790527\n",
      "Epoch 121/199\n",
      "88.81125283241272\n",
      "Epoch 122/199\n",
      "88.66510009765625\n",
      "Epoch 123/199\n",
      "88.57660698890686\n",
      "Epoch 124/199\n",
      "88.34518575668335\n",
      "Epoch 125/199\n",
      "88.33124160766602\n",
      "Epoch 126/199\n",
      "88.68723011016846\n",
      "Epoch 127/199\n",
      "88.7760636806488\n",
      "Epoch 128/199\n",
      "88.78168773651123\n",
      "Epoch 129/199\n",
      "88.84816384315491\n",
      "Epoch 130/199\n",
      "88.59472846984863\n",
      "Epoch 131/199\n",
      "88.60968041419983\n",
      "Epoch 132/199\n",
      "88.68620777130127\n",
      "Epoch 133/199\n",
      "88.53107166290283\n",
      "Epoch 134/199\n",
      "88.61029362678528\n",
      "Epoch 135/199\n",
      "88.64560675621033\n",
      "Epoch 136/199\n",
      "88.51092433929443\n",
      "Epoch 137/199\n",
      "88.4746744632721\n",
      "Epoch 138/199\n",
      "88.44442534446716\n",
      "Epoch 139/199\n",
      "89.00111365318298\n",
      "Epoch 140/199\n",
      "88.88196802139282\n",
      "Epoch 141/199\n",
      "88.55738544464111\n",
      "Epoch 142/199\n",
      "88.52188992500305\n",
      "Epoch 143/199\n",
      "88.4089949131012\n",
      "Epoch 144/199\n",
      "88.61770677566528\n",
      "Epoch 145/199\n",
      "89.02369689941406\n",
      "Epoch 146/199\n",
      "88.81068706512451\n",
      "Epoch 147/199\n",
      "88.90027236938477\n",
      "Epoch 148/199\n",
      "88.95916080474854\n",
      "Epoch 149/199\n",
      "88.67458534240723\n",
      "Epoch 150/199\n",
      "88.58778667449951\n",
      "Epoch 151/199\n",
      "89.1326801776886\n",
      "Epoch 152/199\n",
      "88.89429140090942\n",
      "Epoch 153/199\n",
      "88.79483270645142\n",
      "Epoch 154/199\n",
      "88.828289270401\n",
      "Epoch 155/199\n",
      "88.4782657623291\n",
      "Epoch 156/199\n",
      "88.27619075775146\n",
      "Epoch 157/199\n",
      "88.58282279968262\n",
      "Epoch 158/199\n",
      "88.63504028320312\n",
      "Epoch 159/199\n",
      "88.76724934577942\n",
      "Epoch 160/199\n",
      "88.68122744560242\n",
      "Epoch 161/199\n",
      "88.69294381141663\n",
      "Epoch 162/199\n",
      "88.88967490196228\n",
      "Epoch 163/199\n",
      "88.95487594604492\n",
      "Epoch 164/199\n",
      "88.93395972251892\n",
      "Epoch 165/199\n",
      "88.89735245704651\n",
      "Epoch 166/199\n",
      "88.82348704338074\n",
      "Epoch 167/199\n",
      "88.94864153862\n",
      "Epoch 168/199\n",
      "88.82978463172913\n",
      "Epoch 169/199\n",
      "88.66437888145447\n",
      "Epoch 170/199\n",
      "88.74851942062378\n",
      "Epoch 171/199\n",
      "88.60058236122131\n",
      "Epoch 172/199\n",
      "88.76182508468628\n",
      "Epoch 173/199\n",
      "88.990243434906\n",
      "Epoch 174/199\n",
      "88.81703996658325\n",
      "Epoch 175/199\n",
      "89.06532716751099\n",
      "Epoch 176/199\n",
      "89.12792468070984\n",
      "Epoch 177/199\n",
      "88.86552619934082\n",
      "Epoch 178/199\n",
      "88.9107518196106\n",
      "Epoch 179/199\n",
      "88.73577070236206\n",
      "Epoch 180/199\n",
      "88.6144187450409\n",
      "Epoch 181/199\n",
      "88.98767399787903\n",
      "Epoch 182/199\n",
      "89.10558795928955\n",
      "Epoch 183/199\n",
      "88.76793360710144\n",
      "Epoch 184/199\n",
      "88.70270562171936\n",
      "Epoch 185/199\n",
      "88.67519664764404\n",
      "Epoch 186/199\n",
      "88.46670532226562\n",
      "Epoch 187/199\n",
      "88.93278908729553\n",
      "Epoch 188/199\n",
      "88.9580237865448\n",
      "Epoch 189/199\n",
      "88.70213150978088\n",
      "Epoch 190/199\n",
      "88.74499440193176\n",
      "Epoch 191/199\n",
      "88.7198793888092\n",
      "Epoch 192/199\n",
      "88.33526420593262\n",
      "Epoch 193/199\n",
      "88.5781307220459\n",
      "Epoch 194/199\n",
      "89.13449764251709\n",
      "Epoch 195/199\n",
      "88.91200995445251\n",
      "Epoch 196/199\n",
      "88.59244012832642\n",
      "Epoch 197/199\n",
      "88.47572588920593\n",
      "Epoch 198/199\n",
      "88.80421209335327\n",
      "Epoch 199/199\n",
      "88.9985888004303\n",
      "Training complete in 295m 17s\n",
      "Best val Acc: 0.793700\n"
     ]
    }
   ],
   "source": [
    "# ft_epochs = 200\n",
    "# deit_copy1 = copy.deepcopy(deit)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# deit_copy1.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(deit_copy1)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(deit_copy1.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=0.0005,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 2.0, \n",
    "#                 max_beta = 3, \n",
    "#                 method = 'isin',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ft_epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# deit_copy1, metrics0 = train_model(deit_copy1, criterion, optimizer, scheduler, ft_epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/lfs/\"+args.datasets+\"/deit_pugdr_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\"  + str(optimizer.min_beta) + \"_\" + str(ft_epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': deit_copy1.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/deit_pugdr_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\"  + str(ft_epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f514caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.76422595977783\n",
      "Epoch 1/199\n",
      "89.10661554336548\n",
      "Epoch 2/199\n",
      "89.24078559875488\n",
      "Epoch 3/199\n",
      "88.26227951049805\n",
      "Epoch 4/199\n",
      "88.67666673660278\n",
      "Epoch 5/199\n",
      "88.85479831695557\n",
      "Epoch 6/199\n",
      "88.85738682746887\n",
      "Epoch 7/199\n",
      "88.81713342666626\n",
      "Epoch 8/199\n",
      "89.05789422988892\n",
      "Epoch 9/199\n",
      "88.9247887134552\n",
      "Epoch 10/199\n",
      "89.36668419837952\n",
      "Epoch 11/199\n",
      "89.24304008483887\n",
      "Epoch 12/199\n",
      "89.05508804321289\n",
      "Epoch 13/199\n",
      "88.50157833099365\n",
      "Epoch 14/199\n",
      "88.67002296447754\n",
      "Epoch 15/199\n",
      "89.14885759353638\n",
      "Epoch 16/199\n",
      "88.70243763923645\n",
      "Epoch 17/199\n",
      "89.01074552536011\n",
      "Epoch 18/199\n",
      "88.92478680610657\n",
      "Epoch 19/199\n",
      "88.96434426307678\n",
      "Epoch 20/199\n",
      "88.8775143623352\n",
      "Epoch 21/199\n",
      "89.09601521492004\n",
      "Epoch 22/199\n",
      "89.16467547416687\n",
      "Epoch 23/199\n",
      "88.93419814109802\n",
      "Epoch 24/199\n",
      "88.56355810165405\n",
      "Epoch 25/199\n",
      "89.35416650772095\n",
      "Epoch 26/199\n",
      "88.9465811252594\n",
      "Epoch 27/199\n",
      "88.91129803657532\n",
      "Epoch 28/199\n",
      "89.08080768585205\n",
      "Epoch 29/199\n",
      "88.682870388031\n",
      "Epoch 30/199\n",
      "88.5936028957367\n",
      "Epoch 31/199\n",
      "88.94569945335388\n",
      "Epoch 32/199\n",
      "89.2173969745636\n",
      "Epoch 33/199\n",
      "89.07467937469482\n",
      "Epoch 34/199\n",
      "89.16620373725891\n",
      "Epoch 35/199\n",
      "88.87099123001099\n",
      "Epoch 36/199\n",
      "88.79894351959229\n",
      "Epoch 37/199\n",
      "89.17926287651062\n",
      "Epoch 38/199\n",
      "88.89414668083191\n",
      "Epoch 39/199\n",
      "89.00317740440369\n",
      "Epoch 40/199\n",
      "89.1012716293335\n",
      "Epoch 41/199\n",
      "88.38746786117554\n",
      "Epoch 42/199\n",
      "88.64872884750366\n",
      "Epoch 43/199\n",
      "88.98265290260315\n",
      "Epoch 44/199\n",
      "88.26247358322144\n",
      "Epoch 45/199\n",
      "88.79731321334839\n",
      "Epoch 46/199\n",
      "88.92856764793396\n",
      "Epoch 47/199\n",
      "89.14249730110168\n",
      "Epoch 48/199\n",
      "89.23819017410278\n",
      "Epoch 49/199\n",
      "89.36683177947998\n",
      "Epoch 50/199\n",
      "89.05013108253479\n",
      "Epoch 51/199\n",
      "88.99761080741882\n",
      "Epoch 52/199\n",
      "88.33746671676636\n",
      "Epoch 53/199\n",
      "88.24870920181274\n",
      "Epoch 54/199\n",
      "88.53550052642822\n",
      "Epoch 55/199\n",
      "88.58415102958679\n",
      "Epoch 56/199\n",
      "89.18279695510864\n",
      "Epoch 57/199\n",
      "89.09050512313843\n",
      "Epoch 58/199\n",
      "88.59541535377502\n",
      "Epoch 59/199\n",
      "89.24694633483887\n",
      "Epoch 60/199\n",
      "89.25924396514893\n",
      "Epoch 61/199\n",
      "89.01854920387268\n",
      "Epoch 62/199\n",
      "89.04816389083862\n",
      "Epoch 63/199\n",
      "89.0605354309082\n",
      "Epoch 64/199\n",
      "88.96904063224792\n",
      "Epoch 65/199\n",
      "89.04021883010864\n",
      "Epoch 66/199\n",
      "89.10231590270996\n",
      "Epoch 67/199\n",
      "89.1737847328186\n",
      "Epoch 68/199\n",
      "89.25233340263367\n",
      "Epoch 69/199\n",
      "89.01056933403015\n",
      "Epoch 70/199\n",
      "88.42485523223877\n",
      "Epoch 71/199\n",
      "88.69725370407104\n",
      "Epoch 72/199\n",
      "88.8231201171875\n",
      "Epoch 73/199\n",
      "88.4717788696289\n",
      "Epoch 74/199\n",
      "89.29070496559143\n",
      "Epoch 75/199\n",
      "89.26344132423401\n",
      "Epoch 76/199\n",
      "89.21687841415405\n",
      "Epoch 77/199\n",
      "89.265789270401\n",
      "Epoch 78/199\n",
      "89.12908816337585\n",
      "Epoch 79/199\n",
      "89.34230756759644\n",
      "Epoch 80/199\n",
      "88.81429529190063\n",
      "Epoch 81/199\n",
      "88.40967082977295\n",
      "Epoch 82/199\n",
      "88.67334294319153\n",
      "Epoch 83/199\n",
      "88.71951270103455\n",
      "Epoch 84/199\n",
      "89.1180055141449\n",
      "Epoch 85/199\n",
      "88.99823474884033\n",
      "Epoch 86/199\n",
      "88.63286852836609\n",
      "Epoch 87/199\n",
      "88.82496452331543\n",
      "Epoch 88/199\n",
      "88.86650824546814\n",
      "Epoch 89/199\n",
      "88.64175510406494\n",
      "Epoch 90/199\n",
      "89.15472388267517\n",
      "Epoch 91/199\n",
      "89.08587384223938\n",
      "Epoch 92/199\n",
      "89.28902626037598\n",
      "Epoch 93/199\n",
      "89.15679121017456\n",
      "Epoch 94/199\n",
      "88.72751593589783\n",
      "Epoch 95/199\n",
      "89.162930727005\n",
      "Epoch 96/199\n",
      "88.92462158203125\n",
      "Epoch 97/199\n",
      "88.85666871070862\n",
      "Epoch 98/199\n",
      "88.7848711013794\n",
      "Epoch 99/199\n",
      "88.49375247955322\n",
      "Epoch 100/199\n",
      "88.56657791137695\n",
      "Epoch 101/199\n",
      "88.28329277038574\n",
      "Epoch 102/199\n",
      "88.3305037021637\n",
      "Epoch 103/199\n",
      "87.98523449897766\n",
      "Epoch 104/199\n",
      "87.9072961807251\n",
      "Epoch 105/199\n",
      "88.4990656375885\n",
      "Epoch 106/199\n",
      "88.21708130836487\n",
      "Epoch 107/199\n",
      "88.2155077457428\n",
      "Epoch 108/199\n",
      "88.3149242401123\n",
      "Epoch 109/199\n",
      "87.79737877845764\n",
      "Epoch 110/199\n",
      "88.36204123497009\n",
      "Epoch 111/199\n",
      "87.65129399299622\n",
      "Epoch 112/199\n",
      "88.03433465957642\n",
      "Epoch 113/199\n",
      "88.56714010238647\n",
      "Epoch 114/199\n",
      "88.78206515312195\n",
      "Epoch 115/199\n",
      "88.76837635040283\n",
      "Epoch 116/199\n",
      "88.86450505256653\n",
      "Epoch 117/199\n",
      "89.02926754951477\n",
      "Epoch 118/199\n",
      "88.98620700836182\n",
      "Epoch 119/199\n",
      "87.9566547870636\n",
      "Epoch 120/199\n",
      "87.85321688652039\n",
      "Epoch 121/199\n",
      "87.58499121665955\n",
      "Epoch 122/199\n",
      "87.63922810554504\n",
      "Epoch 123/199\n",
      "87.58482694625854\n",
      "Epoch 124/199\n",
      "87.31138491630554\n",
      "Epoch 125/199\n",
      "87.26379776000977\n",
      "Epoch 126/199\n",
      "87.99922394752502\n",
      "Epoch 127/199\n",
      "87.42335081100464\n",
      "Epoch 128/199\n",
      "88.20265865325928\n",
      "Epoch 129/199\n",
      "88.56768274307251\n",
      "Epoch 130/199\n",
      "88.52638149261475\n",
      "Epoch 131/199\n",
      "88.22382020950317\n",
      "Epoch 132/199\n",
      "88.35938596725464\n",
      "Epoch 133/199\n",
      "88.80355834960938\n",
      "Epoch 134/199\n",
      "88.90557909011841\n",
      "Epoch 135/199\n",
      "88.62129139900208\n",
      "Epoch 136/199\n",
      "88.52812623977661\n",
      "Epoch 137/199\n",
      "88.35242891311646\n",
      "Epoch 138/199\n",
      "88.74845957756042\n",
      "Epoch 139/199\n",
      "89.08090209960938\n",
      "Epoch 140/199\n",
      "88.53257894515991\n",
      "Epoch 141/199\n",
      "87.95870280265808\n",
      "Epoch 142/199\n",
      "87.88903737068176\n",
      "Epoch 143/199\n",
      "87.6872079372406\n",
      "Epoch 144/199\n",
      "87.51858115196228\n",
      "Epoch 145/199\n",
      "87.48710775375366\n",
      "Epoch 146/199\n",
      "87.34782481193542\n",
      "Epoch 147/199\n",
      "87.68925309181213\n",
      "Epoch 148/199\n",
      "88.95996499061584\n",
      "Epoch 149/199\n",
      "88.42073845863342\n",
      "Epoch 150/199\n",
      "88.89757871627808\n",
      "Epoch 151/199\n",
      "88.63986897468567\n",
      "Epoch 152/199\n",
      "88.61974215507507\n",
      "Epoch 153/199\n",
      "88.88794994354248\n",
      "Epoch 154/199\n",
      "88.35358095169067\n",
      "Epoch 155/199\n",
      "88.49038124084473\n",
      "Epoch 156/199\n",
      "88.26683163642883\n",
      "Epoch 157/199\n",
      "88.83222389221191\n",
      "Epoch 158/199\n",
      "88.63222908973694\n",
      "Epoch 159/199\n",
      "88.76021552085876\n",
      "Epoch 160/199\n",
      "88.56171774864197\n",
      "Epoch 161/199\n",
      "88.38001465797424\n",
      "Epoch 162/199\n",
      "88.70318412780762\n",
      "Epoch 163/199\n",
      "89.19439554214478\n",
      "Epoch 164/199\n",
      "89.00500464439392\n",
      "Epoch 165/199\n",
      "88.83387541770935\n",
      "Epoch 166/199\n",
      "88.79244017601013\n",
      "Epoch 167/199\n",
      "88.76718020439148\n",
      "Epoch 168/199\n",
      "89.26482224464417\n",
      "Epoch 169/199\n",
      "89.20445775985718\n",
      "Epoch 170/199\n",
      "89.2158613204956\n",
      "Epoch 171/199\n",
      "88.74816131591797\n",
      "Epoch 172/199\n",
      "88.78762555122375\n",
      "Epoch 173/199\n",
      "89.12947130203247\n",
      "Epoch 174/199\n",
      "89.17969799041748\n",
      "Epoch 175/199\n",
      "88.93546438217163\n",
      "Epoch 176/199\n",
      "89.05247664451599\n",
      "Epoch 177/199\n",
      "88.78940343856812\n",
      "Epoch 178/199\n",
      "88.85535597801208\n",
      "Epoch 179/199\n",
      "88.90857124328613\n",
      "Epoch 180/199\n",
      "88.60815262794495\n",
      "Epoch 181/199\n",
      "88.59859800338745\n",
      "Epoch 182/199\n",
      "89.00184440612793\n",
      "Epoch 183/199\n",
      "89.07228755950928\n",
      "Epoch 184/199\n",
      "89.30173802375793\n",
      "Epoch 185/199\n",
      "89.26579403877258\n",
      "Epoch 186/199\n",
      "88.80840730667114\n",
      "Epoch 187/199\n",
      "88.66384530067444\n",
      "Epoch 188/199\n",
      "88.50999426841736\n",
      "Epoch 189/199\n",
      "88.24744057655334\n",
      "Epoch 190/199\n",
      "88.24703526496887\n",
      "Epoch 191/199\n",
      "88.87362051010132\n",
      "Epoch 192/199\n",
      "89.09136319160461\n",
      "Epoch 193/199\n",
      "89.24669289588928\n",
      "Epoch 194/199\n",
      "89.18427848815918\n",
      "Epoch 195/199\n",
      "88.87017011642456\n",
      "Epoch 196/199\n",
      "88.28506994247437\n",
      "Epoch 197/199\n",
      "88.71816754341125\n",
      "Epoch 198/199\n",
      "88.72919154167175\n",
      "Epoch 199/199\n",
      "88.75608849525452\n",
      "Training complete in 295m 43s\n",
      "Best val Acc: 0.791200\n"
     ]
    }
   ],
   "source": [
    "# ft_epochs = 200\n",
    "# deit_copy1 = copy.deepcopy(deit)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# deit_copy1.to(device)\n",
    "\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     model_ft_org = torch.nn.DataParallel(deit_copy1)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# base_optimizer = optim.SGD\n",
    "# optimizer = PUGDXR(deit_copy1.parameters(),\n",
    "#                 base_optimizer,\n",
    "#                 lr=0.0005,\n",
    "#                 momentum=args.momentum,\n",
    "#                 weight_decay=args.wd,\n",
    "#                 min_beta = 2.5,\n",
    "#                 max_beta = 3,\n",
    "#                 method = 'icos',\n",
    "#                 dampening=0,   # 必须设置为0才能完全固定 \n",
    "#                 nesterov=False # 禁用Nesterov动量 \n",
    "#                  )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ft_epochs)\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# deit_copy1, metrics0 = train_model(deit_copy1, criterion, optimizer, scheduler, ft_epochs, trainloader, device, dataset_sizes) \n",
    "\n",
    "# model_path = \"./model/lfs/\"+args.datasets+\"/deit_pugdr_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\"  + str(optimizer.min_beta) + \"_\" + str(ft_epochs) + \".pth\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': deit_copy1.state_dict(), \n",
    "#     'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, model_path) \n",
    "\n",
    "# name = \"./results/\"+args.datasets+\"/deit_pugdr_\" + str(optimizer.method) + str(optimizer.max_beta) + \"_\" + str(optimizer.min_beta) + \"_\"  + str(ft_epochs) + \".json\"\n",
    "# with open(name,  'w', encoding='utf-8') as f:\n",
    "#     json.dump(metrics0,  f, default=tensor_to_serializable, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
