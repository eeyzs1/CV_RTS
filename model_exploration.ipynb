{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchinfo import summary \n",
    " \n",
    "model = torch.hub.load('pytorch/vision',  'resnet50', pretrained=True)\n",
    " \n",
    "# 显示完整结构（含参数形状）\n",
    "summary(model, input_size=(1, 3, 224, 224), depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm \n",
    "model = timm.create_model('vit_large_patch16_224',  num_classes=1000)\n",
    " \n",
    "# 冻结策略示例（保留最后3层为任务特定层）\n",
    "for name, param in model.named_parameters(): \n",
    "    if not name.startswith(('blocks.21',  'blocks.22', 'blocks.23', 'head')):\n",
    "        param.requires_grad  = False \n",
    " \n",
    "# 验证冻结效果 \n",
    "print([n for n,p in model.named_parameters()  if p.requires_grad]) \n",
    "# 输出：['blocks.21...', 'blocks.22...', 'blocks.23...', 'head.weight',  'head.bias'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_ratio = 0.5  # 冻结50%的层 \n",
    "total_layers = len(model.blocks) \n",
    "for i, block in enumerate(model.blocks): \n",
    "    if i < int(total_layers * freeze_ratio):\n",
    "        for param in block.parameters(): \n",
    "            param.requires_grad  = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_layers(epoch):\n",
    "    if epoch == 5:\n",
    "        for param in model.blocks[4:6].parameters():   # 第5个epoch解冻第5-6块 \n",
    "            param.requires_grad  = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分阶段解冻（示例调度器）\n",
    "def unfreeze_scheduler(epoch):\n",
    "    if epoch == 5:\n",
    "        for name, param in model.named_parameters(): \n",
    "            if name.startswith('blocks.18'): \n",
    "                param.requires_grad  = True \n",
    "    elif epoch == 10:\n",
    "        for name, param in model.named_parameters(): \n",
    "            if name.startswith('blocks.15'): \n",
    "                param.requires_grad  = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历所有层 \n",
    "for name, module in model.named_modules(): \n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        print(f\"\\nLayer: {name}\")\n",
    "        print(f\"Weight shape: {module.weight.shape}\") \n",
    "        print(f\"Bias shape: {module.bias.shape  if module.bias  is not None else 'None'}\")\n",
    "        print(f\"Initial mean: {module.weight.data.mean():.4f}  ± {module.weight.data.std():.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot  as plt \n",
    " \n",
    "# 绘制某层权重直方图 \n",
    "plt.hist(model.layer1[0].conv1.weight.data.cpu().numpy().flatten(),  bins=50)\n",
    "plt.title(\"Layer1  Conv1 Weight Distribution\")\n",
    "plt.xlabel(\"Value\") \n",
    "plt.ylabel(\"Frequency\") \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查初始化方法 \n",
    "print(model._modules['conv1'].weight._initializer)  # 显示初始化函数 \n",
    "print(model._modules['fc'].bias.initial_value)       # 查看bias初始值 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display  import display \n",
    "import pandas as pd \n",
    " \n",
    "params_data = []\n",
    "for name, param in model.named_parameters(): \n",
    "    params_data.append({ \n",
    "        \"Name\": name,\n",
    "        \"Shape\": str(tuple(param.shape)), \n",
    "        \"Dtype\": str(param.dtype), \n",
    "        \"Requires_grad\": param.requires_grad  \n",
    "    })\n",
    "    \n",
    "display(pd.DataFrame(params_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 递归遍历工具\n",
    "def print_structure(module, prefix=\"\"):\n",
    "    for name, child in module.named_children(): \n",
    "        print(f\"{prefix}{name}: {type(child).__name__}\")\n",
    "        print_structure(child, prefix + \"    \")\n",
    " \n",
    "print_structure(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graph TD \n",
    "    A[Input] --> B[分支1]\n",
    "    A --> C[分支2]\n",
    "    B --> D[卷积层]\n",
    "    D --> E[归一化层]\n",
    "    C --> F[全连接层]\n",
    "    E --> G[输出合并]\n",
    "    F --> G \n",
    "    G --> H[Final Output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 安装可视化工具 \n",
    "# pip install torchviz \n",
    "# from torchviz import make_dot \n",
    " \n",
    "# make_dot(model(torch.randn(1,3,224,224)),  \n",
    "#          params=dict(model.named_parameters()), \n",
    "#          show_attrs=True).render(\"model\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特性\tnamed_parameters()\tnamed_modules()\n",
    "# 返回内容\t所有可训练参数（Parameter对象）\t所有模块实例（包含子模块）\n",
    "# 遍历粒度\t参数级（叶子节点）\t模块级（树形结构）\n",
    "# 典型用途\t参数初始化/梯度裁剪\t模型结构修改/层冻结\n",
    "# 包含缓冲区(Buffer)\t❌ 仅包含nn.Parameter\t❌ 但可通过named_children()访问\n",
    "# 递归控制\t自动递归所有子模块\t可通过depth参数控制递归深度\n",
    "\n",
    "# 维度\tnamed_modules()\tnamed_parameters()\n",
    "# 遍历对象\t模块（如Conv2d、Linear）\t参数（如weight、bias）\n",
    "# 是否包含子模块\t是（递归所有层级）\t仅参数，不返回模块结构\n",
    "# 是否包含不可训练参数\t是（返回模块实例）\t否（仅含requires_grad=True的参数）\n",
    "# 典型用途\t模型结构分析/层替换\t参数初始化/梯度监控\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一、named_parameters()的参数来源\n",
    "# 1. 参数注册时机\n",
    "# 定义阶段：当继承nn.Module的类在__init__中调用self.param  = nn.Parameter()或使用nn.Linear等内置模块时，参数会自动注册\n",
    "# 构建阶段：通过register_parameter(name, param)方法手动注册\n",
    "# 加载预训练：load_state_dict()会将参数合并到命名空间中\n",
    "# 2. 典型存储流程\n",
    "# mermaid\n",
    "# 复制\n",
    "# sequenceDiagram \n",
    "#     participant User \n",
    "#     participant Module \n",
    "#     participant Parameter \n",
    "#     User->>Module: 定义nn.Linear(10,20)\n",
    "#     Module->>Parameter: 创建weight/bias Parameter \n",
    "#     Parameter->>Module: 自动注册到_parameters字典 \n",
    "#     User->>Module: 调用named_parameters()\n",
    "#     Module->>User: 返回_parameters的键值对 \n",
    "# 二、可能遗漏的参数类型\n",
    "# 1. 非标准存储的参数\n",
    "# 类型\t示例\t是否在named_parameters()中\n",
    "# 手动张量\tself.tensor  = torch.randn(10)\t❌\n",
    "# 缓冲区(Buffer)\tself.register_buffer('running_mean',  torch.zeros(10))\t❌（需用named_buffers()）\n",
    "# 计算中间量\tself.cache  = None\t❌\n",
    "# 非Parameter属性\tself.scale  = 1.0\t❌\n",
    "# 2. 特殊模块例外\n",
    "# 量化模型：伪量化参数可能存储在_qparams字典中\n",
    "# 第三方扩展：如FairSeq的Adam优化器参数可能单独存储\n",
    "# 三、参数完整性验证方法\n",
    "# 1. 交叉检查工具\n",
    "# python\n",
    "# 复制\n",
    "# def check_parameters(model):\n",
    "#     # 检查所有可训练参数 \n",
    "#     trainable_params = set(name for name, _ in model.named_parameters()) \n",
    "    \n",
    "#     # 检查所有Parameter实例 \n",
    "#     all_params = set()\n",
    "#     for name, module in model.named_modules(): \n",
    "#         if hasattr(module, '_parameters'):\n",
    "#             all_params.update(f\"{name}.{k}\"  for k in module._parameters.keys()) \n",
    "    \n",
    "#     # 打印差异 \n",
    "#     print(\"Missing in named_parameters():\", all_params - trainable_params)\n",
    "#     print(\"Extra in named_parameters():\", trainable_params - all_params)\n",
    " \n",
    "# check_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
