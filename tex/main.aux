\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\citation{Tseng_2022}
\citation{4308316}
\citation{kawaguchi2016deeplearningpoorlocal}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{ruder2017overviewgradientdescentoptimization}
\citation{Tseng_2022}
\citation{Tseng_2022}
\citation{4308316}
\citation{loshchilov2017sgdrstochasticgradientdescent}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\@writefile{brf}{\backcite{Tseng_2022}{{1}{(document)}{Doc-Start}}}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.~Introduction}{1}{section.1}\protected@file@percent }
\@writefile{brf}{\backcite{4308316}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{kawaguchi2016deeplearningpoorlocal}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{ruder2017overviewgradientdescentoptimization}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{Tseng_2022}{{1}{1}{section.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training histories of SGD and PUGD \textbf  {Top row}: Loss and accuracy of both optimizers (400 epochs) \textbf  {Bottom row}: SGD (800 epochs) vs PUGD (400 epochs) }}{1}{figure.1}\protected@file@percent }
\newlabel{fig:SGDvsPUGD}{{1}{1}{Training histories of SGD and PUGD \textbf {Top row}: Loss and accuracy of both optimizers (400 epochs) \textbf {Bottom row}: SGD (800 epochs) vs PUGD (400 epochs)}{figure.1}{}}
\newlabel{fig:SGDvsPUGD@cref}{{[figure][1][]1}{[1][1][]1}}
\@writefile{brf}{\backcite{Tseng_2022}{{1}{1}{figure.1}}}
\@writefile{brf}{\backcite{4308316}{{1}{1}{figure.1}}}
\citation{4308316}
\citation{keskar2017largebatchtrainingdeeplearning}
\citation{ji2020directionalconvergencealignmentdeep}
\citation{neyshabur2015pathsgdpathnormalizedoptimizationdeep}
\citation{jiang2019fantasticgeneralizationmeasures}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{murray2018revisitingnormalizedgradientdescent}
\citation{zheng2021regularizingneuralnetworksadversarial}
\citation{Tseng_2022}
\@writefile{brf}{\backcite{loshchilov2017sgdrstochasticgradientdescent}{{2}{1}{figure.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.~Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:2}{{2}{2}{\hskip -1em.~Related Work}{section.2}{}}
\newlabel{sec:2@cref}{{[section][2][]2}{[1][2][]2}}
\@writefile{brf}{\backcite{4308316}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{keskar2017largebatchtrainingdeeplearning}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{ji2020directionalconvergencealignmentdeep}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{neyshabur2015pathsgdpathnormalizedoptimizationdeep}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{jiang2019fantasticgeneralizationmeasures}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{murray2018revisitingnormalizedgradientdescent}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{zheng2021regularizingneuralnetworksadversarial}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{Tseng_2022}{{2}{2}{section.2}}}
\newlabel{eq:1}{{1}{2}{\hskip -1em.~Related Work}{equation.2.1}{}}
\newlabel{eq:1@cref}{{[equation][1][]1}{[1][2][]2}}
\newlabel{eq:2}{{2}{2}{\hskip -1em.~Related Work}{equation.2.2}{}}
\newlabel{eq:2@cref}{{[equation][2][]2}{[1][2][]2}}
\newlabel{eq:3}{{3}{2}{\hskip -1em.~Related Work}{equation.2.3}{}}
\newlabel{eq:3@cref}{{[equation][3][]3}{[1][2][]2}}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{kwon2021asamadaptivesharpnessawareminimization}
\citation{Tseng_2022}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.~Radius-Timing Scale(RTS)}{3}{section.3}\protected@file@percent }
\newlabel{sec:3}{{3}{3}{\hskip -1em.~Radius-Timing Scale(RTS)}{section.3}{}}
\newlabel{sec:3@cref}{{[section][3][]3}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.~Limitations of PUGD}{3}{subsection.3.1}\protected@file@percent }
\newlabel{subsec:3.1}{{3.1}{3}{\hskip -1em.~Limitations of PUGD}{subsection.3.1}{}}
\newlabel{subsec:3.1@cref}{{[subsection][1][3]3.1}{[1][3][]3}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{3}{3.1}{subsection.3.1}}}
\@writefile{brf}{\backcite{kwon2021asamadaptivesharpnessawareminimization}{{3}{3.1}{subsection.3.1}}}
\@writefile{brf}{\backcite{Tseng_2022}{{3}{3.1}{subsection.3.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.~Perturbation Radius Tuning}{3}{subsection.3.2}\protected@file@percent }
\newlabel{subsec:3.2}{{3.2}{3}{\hskip -1em.~Perturbation Radius Tuning}{subsection.3.2}{}}
\newlabel{subsec:3.2@cref}{{[subsection][2][3]3.2}{[1][3][]3}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{3}{3.2}{subsection.3.2}}}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{kwon2021asamadaptivesharpnessawareminimization}
\citation{russakovsky2015imagenet}
\citation{szegedy2015going}
\citation{krizhevsky2012imagenet}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{4}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{kwon2021asamadaptivesharpnessawareminimization}{{4}{3.2}{subsection.3.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Loss landscape of SGD }}{4}{figure.2}\protected@file@percent }
\newlabel{fig:SGD_LL}{{2}{4}{Loss landscape of SGD}{figure.2}{}}
\newlabel{fig:SGD_LL@cref}{{[figure][2][]2}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.~Timing of application for PUGD}{4}{subsection.3.3}\protected@file@percent }
\newlabel{subsec:3.3}{{3.3}{4}{\hskip -1em.~Timing of application for PUGD}{subsection.3.3}{}}
\newlabel{subsec:3.3@cref}{{[subsection][3][3]3.3}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.~Scale of gradient Tunning}{4}{subsection.3.4}\protected@file@percent }
\newlabel{subsec:3.4}{{3.4}{4}{\hskip -1em.~Scale of gradient Tunning}{subsection.3.4}{}}
\newlabel{subsec:3.4@cref}{{[subsection][4][3]3.4}{[1][4][]4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.~Experiments}{4}{section.4}\protected@file@percent }
\newlabel{sec:4}{{4}{4}{\hskip -1em.~Experiments}{section.4}{}}
\newlabel{sec:4@cref}{{[section][4][]4}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.~Perturbation radius}{4}{subsection.4.1}\protected@file@percent }
\newlabel{subsec:4.1}{{4.1}{4}{\hskip -1em.~Perturbation radius}{subsection.4.1}{}}
\newlabel{subsec:4.1@cref}{{[subsection][1][4]4.1}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.~Timing of application}{4}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:4.2}{{4.2}{4}{\hskip -1em.~Timing of application}{subsection.4.2}{}}
\newlabel{subsec:4.2@cref}{{[subsection][2][4]4.2}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\hskip -1em.~Radius-Timing Scale(RTS)}{4}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:4.3}{{4.3}{4}{\hskip -1em.~Radius-Timing Scale(RTS)}{subsection.4.3}{}}
\newlabel{subsec:4.3@cref}{{[subsection][3][4]4.3}{[1][4][]4}}
\@writefile{brf}{\backcite{russakovsky2015imagenet}{{4}{4.3}{subsection.4.3}}}
\@writefile{brf}{\backcite{szegedy2015going}{{4}{4.3}{subsection.4.3}}}
\@writefile{brf}{\backcite{krizhevsky2012imagenet}{{4}{4.3}{subsection.4.3}}}
\citation{Khosla2011dogs}
\citation{KrauseStarkDengFei-Fei_3DRR2013}
\citation{nilsback2008automated}
\citation{simonyan2014very}
\citation{huang2017densely}
\citation{sandler2018mobilenetv2}
\citation{tan2019efficientnet}
\citation{tan2019mnasnet}
\citation{simonyan2014very}
\citation{huang2017densely}
\citation{chollet2017xception}
\citation{tan2019efficientnet}
\citation{tan2019mnasnet}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces MobileNets on ImageNet. BSConv\xspace  -U\xspace  is used for MobileNetV1, and BSConv\xspace  -S\xspace  is used for MobileNetV2/V3. Note that BSConv\xspace  does not introduce additional parameters. }}{5}{table.1}\protected@file@percent }
\newlabel{tab:ImageNet-Mobilenet}{{1}{5}{MobileNets on ImageNet. \DCCKU is used for MobileNetV1, and \DCCKS is used for MobileNetV2/V3. Note that \DCCK does not introduce additional parameters}{table.1}{}}
\newlabel{tab:ImageNet-Mobilenet@cref}{{[table][1][]1}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}\hskip -1em.~Fine-grained Recognition}{5}{subsection.4.4}\protected@file@percent }
\@writefile{brf}{\backcite{Khosla2011dogs}{{5}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{KrauseStarkDengFei-Fei_3DRR2013}{{5}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{nilsback2008automated}{{5}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{simonyan2014very}{{5}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{huang2017densely}{{5}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{sandler2018mobilenetv2}{{5}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{tan2019efficientnet}{{5}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{tan2019mnasnet}{{5}{4.4}{subsection.4.4}}}
\bibstyle{ieee_fullname}
\bibdata{main}
\bibcite{foret2021sharpnessawareminimizationefficientlyimproving}{1}
\bibcite{ji2020directionalconvergencealignmentdeep}{2}
\bibcite{jiang2019fantasticgeneralizationmeasures}{3}
\bibcite{kawaguchi2016deeplearningpoorlocal}{4}
\bibcite{keskar2017largebatchtrainingdeeplearning}{5}
\bibcite{kwon2021asamadaptivesharpnessawareminimization}{6}
\bibcite{loshchilov2017sgdrstochasticgradientdescent}{7}
\bibcite{murray2018revisitingnormalizedgradientdescent}{8}
\bibcite{neyshabur2015pathsgdpathnormalizedoptimizationdeep}{9}
\bibcite{ruder2017overviewgradientdescentoptimization}{10}
\bibcite{4308316}{11}
\bibcite{Tseng_2022}{12}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces ResNets on ImageNet. For the baseline models, we use ResNet-10/18/26. The BSConv\xspace  variants are ResNet-10/18/34/68/102. }}{6}{figure.3}\protected@file@percent }
\newlabel{fig:resnetsImagenetParamsAndFlops}{{3}{6}{ResNets on ImageNet. For the baseline models, we use ResNet-10/18/26. The \DCCK variants are ResNet-10/18/34/68/102}{figure.3}{}}
\newlabel{fig:resnetsImagenetParamsAndFlops@cref}{{[figure][3][]3}{[1][5][]6}}
\@writefile{brf}{\backcite{simonyan2014very}{{6}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{huang2017densely}{{6}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{chollet2017xception}{{6}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{tan2019efficientnet}{{6}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{tan2019mnasnet}{{6}{4.4}{subsection.4.4}}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results of various architectures and their BSConv\xspace  counterparts for the Stanford Dogs dataset. BSConv\xspace  -U\xspace  CNNs have fewer parameters and a smaller computational complexity compared to their baseline models. BSConv\xspace  -S\xspace  CNNs have the same parameter count and computational complexity as their counterparts. * Commonly used implementation based on DSCs. }}{6}{table.2}\protected@file@percent }
\newlabel{tab:dogsOthers}{{2}{6}{Results of various architectures and their \DCCK counterparts for the Stanford Dogs dataset. \DCCKU CNNs have fewer parameters and a smaller computational complexity compared to their baseline models. \DCCKS CNNs have the same parameter count and computational complexity as their counterparts. * Commonly used implementation based on DSCs}{table.2}{}}
\newlabel{tab:dogsOthers@cref}{{[table][2][]2}{[1][5][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Influence of the orthonormal regularization loss on the accuracy for the BSConv\xspace  -S\xspace  variant of MobileNetV3-large (red solid line) on Stanford Dogs. The baseline MobileNetV3-large model without BSConv\xspace  -S\xspace  is indicated by the black dashed line. }}{6}{figure.4}\protected@file@percent }
\newlabel{fig:dcckV2Alpha}{{4}{6}{Influence of the orthonormal regularization loss on the accuracy for the \DCCKS variant of MobileNetV3-large (red solid line) on Stanford Dogs. The baseline MobileNetV3-large model without \DCCKS is indicated by the black dashed line}{figure.4}{}}
\newlabel{fig:dcckV2Alpha@cref}{{[figure][4][]4}{[1][5][]6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.~Conclusions}{6}{section.5}\protected@file@percent }
\newlabel{sec:5}{{5}{6}{\hskip -1em.~Conclusions}{section.5}{}}
\newlabel{sec:5@cref}{{[section][5][]5}{[1][6][]6}}
\bibcite{zheng2021regularizingneuralnetworksadversarial}{13}
\gdef \@abspage@last{7}
