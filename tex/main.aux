\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\citation{Tseng_2022}
\citation{4308316}
\citation{kawaguchi2016deeplearningpoorlocal}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{ruder2017overviewgradientdescentoptimization}
\citation{Tseng_2022}
\citation{Tseng_2022}
\citation{4308316}
\citation{loshchilov2017sgdrstochasticgradientdescent}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\@writefile{brf}{\backcite{Tseng_2022}{{1}{(document)}{Doc-Start}}}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.~Introduction}{1}{section.1}\protected@file@percent }
\@writefile{brf}{\backcite{4308316}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{kawaguchi2016deeplearningpoorlocal}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{ruder2017overviewgradientdescentoptimization}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{Tseng_2022}{{1}{1}{section.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training histories of SGD and PUGD \textbf  {Top row}: Loss and accuracy of both optimizers (400 epochs) \textbf  {Bottom row}: SGD (800 epochs) vs PUGD (400 epochs)}}{1}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:SGDvsPUGD}{{1}{1}{Training histories of SGD and PUGD \textbf {Top row}: Loss and accuracy of both optimizers (400 epochs) \textbf {Bottom row}: SGD (800 epochs) vs PUGD (400 epochs)}{figure.caption.1}{}}
\newlabel{fig:SGDvsPUGD@cref}{{[figure][1][]1}{[1][1][]1}}
\@writefile{brf}{\backcite{Tseng_2022}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{4308316}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{loshchilov2017sgdrstochasticgradientdescent}{{1}{1}{figure.caption.1}}}
\citation{4308316}
\citation{keskar2017largebatchtrainingdeeplearning}
\citation{ji2020directionalconvergencealignmentdeep}
\citation{neyshabur2015pathsgdpathnormalizedoptimizationdeep}
\citation{jiang2019fantasticgeneralizationmeasures,jastrzebski2020breakevenpointoptimizationtrajectories}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{murray2018revisitingnormalizedgradientdescent}
\citation{zheng2021regularizingneuralnetworksadversarial}
\citation{Tseng_2022}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.~Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:2}{{2}{2}{\hskip -1em.~Related Work}{section.2}{}}
\newlabel{sec:2@cref}{{[section][2][]2}{[1][2][]2}}
\@writefile{brf}{\backcite{4308316}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{keskar2017largebatchtrainingdeeplearning}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{ji2020directionalconvergencealignmentdeep}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{neyshabur2015pathsgdpathnormalizedoptimizationdeep}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{jastrzebski2020breakevenpointoptimizationtrajectories}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{jiang2019fantasticgeneralizationmeasures}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{murray2018revisitingnormalizedgradientdescent}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{zheng2021regularizingneuralnetworksadversarial}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{Tseng_2022}{{2}{2}{section.2}}}
\newlabel{eq:1}{{1}{2}{\hskip -1em.~Related Work}{equation.2.1}{}}
\newlabel{eq:1@cref}{{[equation][1][]1}{[1][2][]2}}
\newlabel{eq:2}{{2}{2}{\hskip -1em.~Related Work}{equation.2.2}{}}
\newlabel{eq:2@cref}{{[equation][2][]2}{[1][2][]2}}
\newlabel{eq:3}{{3}{2}{\hskip -1em.~Related Work}{equation.2.3}{}}
\newlabel{eq:3@cref}{{[equation][3][]3}{[1][2][]2}}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{kwon2021asamadaptivesharpnessawareminimization}
\citation{Tseng_2022}
\citation{li2018visualizinglosslandscapeneural}
\citation{chaudhari2017entropysgdbiasinggradientdescent}
\citation{li2018visualizinglosslandscapeneural}
\citation{jastrzebski2020breakevenpointoptimizationtrajectories}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.~Radius-Timing Scale(RTS)}{3}{section.3}\protected@file@percent }
\newlabel{sec:3}{{3}{3}{\hskip -1em.~Radius-Timing Scale(RTS)}{section.3}{}}
\newlabel{sec:3@cref}{{[section][3][]3}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.~Limitations of PUGD}{3}{subsection.3.1}\protected@file@percent }
\newlabel{subsec:3.1}{{3.1}{3}{\hskip -1em.~Limitations of PUGD}{subsection.3.1}{}}
\newlabel{subsec:3.1@cref}{{[subsection][1][3]3.1}{[1][3][]3}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{3}{3.1}{subsection.3.1}}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{3}{3.1}{subsection.3.1}}}
\@writefile{brf}{\backcite{kwon2021asamadaptivesharpnessawareminimization}{{3}{3.1}{subsection.3.1}}}
\@writefile{brf}{\backcite{Tseng_2022}{{3}{3.1}{subsection.3.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.~Perturbation Radius Tuning}{3}{subsection.3.2}\protected@file@percent }
\newlabel{subsec:3.2}{{3.2}{3}{\hskip -1em.~Perturbation Radius Tuning}{subsection.3.2}{}}
\newlabel{subsec:3.2@cref}{{[subsection][2][3]3.2}{[1][3][]3}}
\@writefile{brf}{\backcite{li2018visualizinglosslandscapeneural}{{3}{3.2}{figure.caption.2}}}
\@writefile{brf}{\backcite{chaudhari2017entropysgdbiasinggradientdescent}{{3}{3.2}{figure.caption.2}}}
\@writefile{brf}{\backcite{li2018visualizinglosslandscapeneural}{{3}{3.2}{figure.caption.2}}}
\@writefile{brf}{\backcite{jastrzebski2020breakevenpointoptimizationtrajectories}{{3}{3.2}{figure.caption.2}}}
\citation{maclaurin2015earlystoppingnonparametricvariational}
\citation{4308316}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Loss landscape and contour of SGD}}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:SGD_LLC}{{2}{4}{Loss landscape and contour of SGD}{figure.caption.2}{}}
\newlabel{fig:SGD_LLC@cref}{{[figure][2][]2}{[1][3][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.~Timing of application for PUGD}{4}{subsection.3.3}\protected@file@percent }
\newlabel{subsec:3.3}{{3.3}{4}{\hskip -1em.~Timing of application for PUGD}{subsection.3.3}{}}
\newlabel{subsec:3.3@cref}{{[subsection][3][3]3.3}{[1][4][]4}}
\@writefile{brf}{\backcite{maclaurin2015earlystoppingnonparametricvariational}{{4}{3.3}{subsection.3.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.~Scale of gradient Tunning}{4}{subsection.3.4}\protected@file@percent }
\newlabel{subsec:3.4}{{3.4}{4}{\hskip -1em.~Scale of gradient Tunning}{subsection.3.4}{}}
\newlabel{subsec:3.4@cref}{{[subsection][4][3]3.4}{[1][4][]4}}
\@writefile{brf}{\backcite{4308316}{{4}{3.4}{subsection.3.4}}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{4}{3.4}{subsection.3.4}}}
\citation{loshchilov2017sgdrstochasticgradientdescent}
\citation{gastpar2023fantasticgeneralizationmeasures}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}\hskip -1em.~Four tuning strategies with $\alpha $}{5}{subsection.3.5}\protected@file@percent }
\newlabel{subsec:3.5}{{3.5}{5}{\hskip -1em.~Four tuning strategies with $\alpha $}{subsection.3.5}{}}
\newlabel{subsec:3.5@cref}{{[subsection][5][3]3.5}{[1][5][]5}}
\@writefile{brf}{\backcite{loshchilov2017sgdrstochasticgradientdescent}{{5}{3.5}{subsection.3.5}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of alpha scheduling strategies \textbf  {Top subfigure}: Cosine vs inverse sine annealing \textbf  {Bottom subfigure}: Sine vs inverse cosine annealing. The annotations are the decile-sampled values}}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:AlphaSch}{{3}{5}{Comparison of alpha scheduling strategies \textbf {Top subfigure}: Cosine vs inverse sine annealing \textbf {Bottom subfigure}: Sine vs inverse cosine annealing. The annotations are the decile-sampled values}{figure.caption.3}{}}
\newlabel{fig:AlphaSch@cref}{{[figure][3][]3}{[1][5][]5}}
\@writefile{brf}{\backcite{gastpar2023fantasticgeneralizationmeasures}{{5}{3.5}{figure.caption.3}}}
\citation{loshchilov2017sgdrstochasticgradientdescent}
\citation{ishida2021needzerotrainingloss}
\citation{bai2021understandingimprovingearlystopping}
\citation{lin2018focallossdenseobject}
\citation{szegedy2015rethinkinginceptionarchitecturecomputer}
\bibstyle{ieee_fullname}
\bibdata{main}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.~Experiments}{6}{section.4}\protected@file@percent }
\newlabel{sec:4}{{4}{6}{\hskip -1em.~Experiments}{section.4}{}}
\newlabel{sec:4@cref}{{[section][4][]4}{[1][5][]6}}
\@writefile{brf}{\backcite{loshchilov2017sgdrstochasticgradientdescent}{{6}{4}{section.4}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.~Comparative analysis of optimization strategies}{6}{subsection.4.1}\protected@file@percent }
\newlabel{subsec:4.1}{{4.1}{6}{\hskip -1em.~Comparative analysis of optimization strategies}{subsection.4.1}{}}
\newlabel{subsec:4.1@cref}{{[subsection][1][4]4.1}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Perturbation radius Tuning}{6}{subsubsection.4.1.1}\protected@file@percent }
\newlabel{subsec:4.1.1}{{4.1.1}{6}{Perturbation radius Tuning}{subsubsection.4.1.1}{}}
\newlabel{subsec:4.1.1@cref}{{[subsubsection][1][4,1]4.1.1}{[1][6][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Perturbation Radius Tuning with Inverse Sine and Inverse Cosine \textbf  {(a)}: Pugd vs Pugd Radius inverse sine loss \textbf  {(b)}: Pugd vs Pugd Radius inverse sine accuracy \textbf  {(c)}: Pugd vs Pugd Radius inverse cosine loss \textbf  {(d)}: Pugd vs Pugd Radius inverse cosine accuracy \textbf  {(e)}: Pugd Radius inverse sine vs inverse cosine loss \textbf  {(f)}: Pugd Radius inverse sine vs inverse cosine accuracy. The epochs ranged from 21 to 400, $\beta _{min}$ set as 0.01 and $\beta _{max}$ set as 2 for both inverse sine and cosine.}}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:PUGDRadiusInv}{{4}{6}{Perturbation Radius Tuning with Inverse Sine and Inverse Cosine \textbf {(a)}: Pugd vs Pugd Radius inverse sine loss \textbf {(b)}: Pugd vs Pugd Radius inverse sine accuracy \textbf {(c)}: Pugd vs Pugd Radius inverse cosine loss \textbf {(d)}: Pugd vs Pugd Radius inverse cosine accuracy \textbf {(e)}: Pugd Radius inverse sine vs inverse cosine loss \textbf {(f)}: Pugd Radius inverse sine vs inverse cosine accuracy. The epochs ranged from 21 to 400, $\beta _{min}$ set as 0.01 and $\beta _{max}$ set as 2 for both inverse sine and cosine}{figure.caption.4}{}}
\newlabel{fig:PUGDRadiusInv@cref}{{[figure][4][]4}{[1][6][]6}}
\@writefile{brf}{\backcite{ishida2021needzerotrainingloss}{{6}{4.1.1}{figure.caption.4}}}
\@writefile{brf}{\backcite{bai2021understandingimprovingearlystopping}{{6}{4.1.1}{figure.caption.4}}}
\@writefile{brf}{\backcite{lin2018focallossdenseobject}{{6}{4.1.1}{figure.caption.4}}}
\@writefile{brf}{\backcite{szegedy2015rethinkinginceptionarchitecturecomputer}{{6}{4.1.1}{figure.caption.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Perturbation Radius Tuning with Sine and Cosine \textbf  {(a)}: Pugd vs Pugd Radius sine loss \textbf  {(b)}: Pugd vs Pugd Radius sine accuracy \textbf  {(c)}: Pugd vs Pugd Radius cosine loss \textbf  {(d)}: Pugd vs Pugd Radius cosine accuracy \textbf  {(e)}: Pugd Radius sine vs cosine loss \textbf  {(f)}: Pugd Radius sine vs cosine accuracy. The epochs ranged from 21 to 400, cosine used $\beta _{min}$ as 0.5 and $\beta _{max}$ as 1.5, sine used $\beta _{min}$ as 0 and $\beta _{max}$ as 2.}}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:PUGDRadius}{{5}{6}{Perturbation Radius Tuning with Sine and Cosine \textbf {(a)}: Pugd vs Pugd Radius sine loss \textbf {(b)}: Pugd vs Pugd Radius sine accuracy \textbf {(c)}: Pugd vs Pugd Radius cosine loss \textbf {(d)}: Pugd vs Pugd Radius cosine accuracy \textbf {(e)}: Pugd Radius sine vs cosine loss \textbf {(f)}: Pugd Radius sine vs cosine accuracy. The epochs ranged from 21 to 400, cosine used $\beta _{min}$ as 0.5 and $\beta _{max}$ as 1.5, sine used $\beta _{min}$ as 0 and $\beta _{max}$ as 2}{figure.caption.5}{}}
\newlabel{fig:PUGDRadius@cref}{{[figure][5][]5}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Timing of application}{6}{subsubsection.4.1.2}\protected@file@percent }
\newlabel{subsec:4.1.2}{{4.1.2}{6}{Timing of application}{subsubsection.4.1.2}{}}
\newlabel{subsec:4.1.2@cref}{{[subsubsection][2][4,1]4.1.2}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Scale of gradient Tunning}{6}{subsubsection.4.1.3}\protected@file@percent }
\newlabel{subsec:4.1.3}{{4.1.3}{6}{Scale of gradient Tunning}{subsubsection.4.1.3}{}}
\newlabel{subsec:4.1.3@cref}{{[subsubsection][3][4,1]4.1.3}{[1][6][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Perturbation Scale Tuning with Inverse Sine and Inverse Cosine \textbf  {(a)}: Pugd vs Pugd Scale inverse sine loss \textbf  {(b)}: Pugd vs Pugd Scale inverse sine accuracy \textbf  {(c)}: Pugd vs Pugd Scale inverse cosine loss \textbf  {(d)}: Pugd vs Pugd Scale inverse cosine accuracy \textbf  {(e)}: Pugd Scale inverse sine vs inverse cosine loss \textbf  {(f)}: Pugd Scale inverse sine vs inverse cosine accuracy The epochs ranged from 21 to 400, inverse cosine used $\beta _{min}$ as 1 and $\beta _{max}$ as 2, inverse sine used $\beta _{min}$ as 0.8 and $\beta _{max}$ as 3. }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:PUGDScaleInv}{{6}{6}{Perturbation Scale Tuning with Inverse Sine and Inverse Cosine \textbf {(a)}: Pugd vs Pugd Scale inverse sine loss \textbf {(b)}: Pugd vs Pugd Scale inverse sine accuracy \textbf {(c)}: Pugd vs Pugd Scale inverse cosine loss \textbf {(d)}: Pugd vs Pugd Scale inverse cosine accuracy \textbf {(e)}: Pugd Scale inverse sine vs inverse cosine loss \textbf {(f)}: Pugd Scale inverse sine vs inverse cosine accuracy The epochs ranged from 21 to 400, inverse cosine used $\beta _{min}$ as 1 and $\beta _{max}$ as 2, inverse sine used $\beta _{min}$ as 0.8 and $\beta _{max}$ as 3}{figure.caption.6}{}}
\newlabel{fig:PUGDScaleInv@cref}{{[figure][6][]6}{[1][6][]6}}
\bibcite{bai2021understandingimprovingearlystopping}{{1}{}{{}}{{}}}
\bibcite{chaudhari2017entropysgdbiasinggradientdescent}{{2}{}{{}}{{}}}
\bibcite{foret2021sharpnessawareminimizationefficientlyimproving}{{3}{}{{}}{{}}}
\bibcite{gastpar2023fantasticgeneralizationmeasures}{{4}{}{{}}{{}}}
\bibcite{ishida2021needzerotrainingloss}{{5}{}{{}}{{}}}
\bibcite{jastrzebski2020breakevenpointoptimizationtrajectories}{{6}{}{{}}{{}}}
\bibcite{ji2020directionalconvergencealignmentdeep}{{7}{}{{}}{{}}}
\bibcite{jiang2019fantasticgeneralizationmeasures}{{8}{}{{}}{{}}}
\bibcite{kawaguchi2016deeplearningpoorlocal}{{9}{}{{}}{{}}}
\bibcite{keskar2017largebatchtrainingdeeplearning}{{10}{}{{}}{{}}}
\bibcite{kwon2021asamadaptivesharpnessawareminimization}{{11}{}{{}}{{}}}
\bibcite{li2018visualizinglosslandscapeneural}{{12}{}{{}}{{}}}
\bibcite{lin2018focallossdenseobject}{{13}{}{{}}{{}}}
\bibcite{loshchilov2017sgdrstochasticgradientdescent}{{14}{}{{}}{{}}}
\bibcite{maclaurin2015earlystoppingnonparametricvariational}{{15}{}{{}}{{}}}
\bibcite{murray2018revisitingnormalizedgradientdescent}{{16}{}{{}}{{}}}
\bibcite{neyshabur2015pathsgdpathnormalizedoptimizationdeep}{{17}{}{{}}{{}}}
\bibcite{ruder2017overviewgradientdescentoptimization}{{18}{}{{}}{{}}}
\bibcite{4308316}{{19}{}{{}}{{}}}
\bibcite{szegedy2015rethinkinginceptionarchitecturecomputer}{{20}{}{{}}{{}}}
\bibcite{Tseng_2022}{{21}{}{{}}{{}}}
\bibcite{zheng2021regularizingneuralnetworksadversarial}{{22}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Perturbation Scale Tuning with Inverse Sine and Inverse Cosine \textbf  {(a)}: Pugd vs Pugd Scale inverse sine loss