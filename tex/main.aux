\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Tseng_2022}
\citation{4308316}
\citation{kawaguchi2016deeplearningpoorlocal}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{ruder2017overviewgradientdescentoptimization}
\citation{Tseng_2022}
\citation{Tseng_2022}
\citation{4308316}
\citation{loshchilov2017sgdrstochasticgradientdescent}
\providecommand \oddpage@label [2]{}
\@writefile{brf}{\backcite{Tseng_2022}{{1}{(document)}{Doc-Start}}}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.~Introduction}{1}{section.1}\protected@file@percent }
\@writefile{brf}{\backcite{4308316}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{kawaguchi2016deeplearningpoorlocal}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{1}{1}{section.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Left shows \vspace  {-0.2em} }}{1}{figure.1}\protected@file@percent }
\newlabel{fig:pugd_sgd_reproduction}{{1}{1}{Left shows \vspace {-0.2em}}{figure.1}{}}
\newlabel{fig:pugd_sgd_reproduction@cref}{{[figure][1][]1}{[1][1][]1}}
\@writefile{brf}{\backcite{ruder2017overviewgradientdescentoptimization}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{Tseng_2022}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{Tseng_2022}{{1}{1}{figure.1}}}
\@writefile{brf}{\backcite{4308316}{{1}{1}{figure.1}}}
\@writefile{brf}{\backcite{loshchilov2017sgdrstochasticgradientdescent}{{1}{1}{figure.1}}}
\citation{4308316}
\citation{keskar2017largebatchtrainingdeeplearning}
\citation{ji2020directionalconvergencealignmentdeep}
\citation{neyshabur2015pathsgdpathnormalizedoptimizationdeep}
\citation{jiang2019fantasticgeneralizationmeasures}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{murray2018revisitingnormalizedgradientdescent}
\citation{zheng2021regularizingneuralnetworksadversarial}
\citation{Tseng_2022}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.~Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:2}{{2}{2}{\hskip -1em.~Related Work}{section.2}{}}
\newlabel{sec:2@cref}{{[section][2][]2}{[1][2][]2}}
\@writefile{brf}{\backcite{4308316}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{keskar2017largebatchtrainingdeeplearning}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{ji2020directionalconvergencealignmentdeep}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{neyshabur2015pathsgdpathnormalizedoptimizationdeep}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{jiang2019fantasticgeneralizationmeasures}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{murray2018revisitingnormalizedgradientdescent}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{zheng2021regularizingneuralnetworksadversarial}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{Tseng_2022}{{2}{2}{section.2}}}
\newlabel{eq:1}{{1}{2}{\hskip -1em.~Related Work}{equation.2.1}{}}
\newlabel{eq:1@cref}{{[equation][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.~Radius-Timing Scale(RTS)}{2}{section.3}\protected@file@percent }
\newlabel{sec:3}{{3}{2}{\hskip -1em.~Radius-Timing Scale(RTS)}{section.3}{}}
\newlabel{sec:3@cref}{{[section][3][]3}{[1][2][]2}}
\citation{kwon2021asamadaptivesharpnessawareminimization}
\citation{russakovsky2015imagenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.~Limitations of PUGD}{3}{subsection.3.1}\protected@file@percent }
\newlabel{subsec:3.1}{{3.1}{3}{\hskip -1em.~Limitations of PUGD}{subsection.3.1}{}}
\newlabel{subsec:3.1@cref}{{[subsection][1][3]3.1}{[1][2][]3}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{3}{3.1}{subsection.3.1}}}
\@writefile{brf}{\backcite{kwon2021asamadaptivesharpnessawareminimization}{{3}{3.1}{subsection.3.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.~Perturbation Radius Tuning}{3}{subsection.3.2}\protected@file@percent }
\newlabel{subsec:3.2}{{3.2}{3}{\hskip -1em.~Perturbation Radius Tuning}{subsection.3.2}{}}
\newlabel{subsec:3.2@cref}{{[subsection][2][3]3.2}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.~Scale of gradient Tunning}{3}{subsection.3.3}\protected@file@percent }
\newlabel{subsec:3.3}{{3.3}{3}{\hskip -1em.~Scale of gradient Tunning}{subsection.3.3}{}}
\newlabel{subsec:3.3@cref}{{[subsection][3][3]3.3}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.~Timing of application for PUGD}{3}{subsection.3.4}\protected@file@percent }
\newlabel{subsec:3.4}{{3.4}{3}{\hskip -1em.~Timing of application for PUGD}{subsection.3.4}{}}
\newlabel{subsec:3.4@cref}{{[subsection][4][3]3.4}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.~Experiments}{3}{section.4}\protected@file@percent }
\newlabel{sec:4}{{4}{3}{\hskip -1em.~Experiments}{section.4}{}}
\newlabel{sec:4@cref}{{[section][4][]4}{[1][3][]3}}
\citation{szegedy2015going}
\citation{krizhevsky2012imagenet}
\citation{Khosla2011dogs}
\citation{KrauseStarkDengFei-Fei_3DRR2013}
\citation{nilsback2008automated}
\citation{simonyan2014very}
\citation{huang2017densely}
\citation{sandler2018mobilenetv2}
\citation{tan2019efficientnet}
\citation{tan2019mnasnet}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces MobileNets on ImageNet. BSConv\xspace  -U\xspace  is used for MobileNetV1, and BSConv\xspace  -S\xspace  is used for MobileNetV2/V3. Note that BSConv\xspace  does not introduce additional parameters. }}{4}{table.1}\protected@file@percent }
\newlabel{tab:ImageNet-Mobilenet}{{1}{4}{MobileNets on ImageNet. \DCCKU is used for MobileNetV1, and \DCCKS is used for MobileNetV2/V3. Note that \DCCK does not introduce additional parameters}{table.1}{}}
\newlabel{tab:ImageNet-Mobilenet@cref}{{[table][1][]1}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.~Perturbation radius}{4}{subsection.4.1}\protected@file@percent }
\newlabel{subsec:4.1}{{4.1}{4}{\hskip -1em.~Perturbation radius}{subsection.4.1}{}}
\newlabel{subsec:4.1@cref}{{[subsection][1][4]4.1}{[1][3][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.~Timing of application}{4}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:4.2}{{4.2}{4}{\hskip -1em.~Timing of application}{subsection.4.2}{}}
\newlabel{subsec:4.2@cref}{{[subsection][2][4]4.2}{[1][3][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\hskip -1em.~Radius-Timing Scale(RTS)}{4}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:4.3}{{4.3}{4}{\hskip -1em.~Radius-Timing Scale(RTS)}{subsection.4.3}{}}
\newlabel{subsec:4.3@cref}{{[subsection][3][4]4.3}{[1][3][]4}}
\@writefile{brf}{\backcite{russakovsky2015imagenet}{{4}{4.3}{subsection.4.3}}}
\@writefile{brf}{\backcite{szegedy2015going}{{4}{4.3}{subsection.4.3}}}
\@writefile{brf}{\backcite{krizhevsky2012imagenet}{{4}{4.3}{subsection.4.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}\hskip -1em.~Fine-grained Recognition}{4}{subsection.4.4}\protected@file@percent }
\@writefile{brf}{\backcite{Khosla2011dogs}{{4}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{KrauseStarkDengFei-Fei_3DRR2013}{{4}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{nilsback2008automated}{{4}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{simonyan2014very}{{4}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{huang2017densely}{{4}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{sandler2018mobilenetv2}{{4}{4.4}{subsection.4.4}}}
\citation{simonyan2014very}
\citation{huang2017densely}
\citation{chollet2017xception}
\citation{tan2019efficientnet}
\citation{tan2019mnasnet}
\bibstyle{ieee_fullname}
\bibdata{main}
\bibcite{foret2021sharpnessawareminimizationefficientlyimproving}{1}
\bibcite{ji2020directionalconvergencealignmentdeep}{2}
\bibcite{jiang2019fantasticgeneralizationmeasures}{3}
\bibcite{kawaguchi2016deeplearningpoorlocal}{4}
\bibcite{keskar2017largebatchtrainingdeeplearning}{5}
\bibcite{kwon2021asamadaptivesharpnessawareminimization}{6}
\bibcite{loshchilov2017sgdrstochasticgradientdescent}{7}
\bibcite{murray2018revisitingnormalizedgradientdescent}{8}
\bibcite{neyshabur2015pathsgdpathnormalizedoptimizationdeep}{9}
\bibcite{ruder2017overviewgradientdescentoptimization}{10}
\bibcite{4308316}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces ResNets on ImageNet. For the baseline models, we use ResNet-10/18/26. The BSConv\xspace  variants are ResNet-10/18/34/68/102. }}{5}{figure.2}\protected@file@percent }
\newlabel{fig:resnetsImagenetParamsAndFlops}{{2}{5}{ResNets on ImageNet. For the baseline models, we use ResNet-10/18/26. The \DCCK variants are ResNet-10/18/34/68/102}{figure.2}{}}
\newlabel{fig:resnetsImagenetParamsAndFlops@cref}{{[figure][2][]2}{[1][4][]5}}
\@writefile{brf}{\backcite{simonyan2014very}{{5}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{huang2017densely}{{5}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{chollet2017xception}{{5}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{tan2019efficientnet}{{5}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{tan2019mnasnet}{{5}{4.4}{subsection.4.4}}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results of various architectures and their BSConv\xspace  counterparts for the Stanford Dogs dataset. BSConv\xspace  -U\xspace  CNNs have fewer parameters and a smaller computational complexity compared to their baseline models. BSConv\xspace  -S\xspace  CNNs have the same parameter count and computational complexity as their counterparts. * Commonly used implementation based on DSCs. }}{5}{table.2}\protected@file@percent }
\newlabel{tab:dogsOthers}{{2}{5}{Results of various architectures and their \DCCK counterparts for the Stanford Dogs dataset. \DCCKU CNNs have fewer parameters and a smaller computational complexity compared to their baseline models. \DCCKS CNNs have the same parameter count and computational complexity as their counterparts. * Commonly used implementation based on DSCs}{table.2}{}}
\newlabel{tab:dogsOthers@cref}{{[table][2][]2}{[1][5][]5}}
\@writefile{brf}{\backcite{tan2019efficientnet}{{5}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{tan2019mnasnet}{{5}{4.4}{subsection.4.4}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.~Conclusions}{5}{section.5}\protected@file@percent }
\newlabel{sec:5}{{5}{5}{\hskip -1em.~Conclusions}{section.5}{}}
\newlabel{sec:5@cref}{{[section][5][]5}{[1][5][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Influence of the orthonormal regularization loss on the accuracy for the BSConv\xspace  -S\xspace  variant of MobileNetV3-large (red solid line) on Stanford Dogs. The baseline MobileNetV3-large model without BSConv\xspace  -S\xspace  is indicated by the black dashed line. }}{5}{figure.3}\protected@file@percent }
\newlabel{fig:dcckV2Alpha}{{3}{5}{Influence of the orthonormal regularization loss on the accuracy for the \DCCKS variant of MobileNetV3-large (red solid line) on Stanford Dogs. The baseline MobileNetV3-large model without \DCCKS is indicated by the black dashed line}{figure.3}{}}
\newlabel{fig:dcckV2Alpha@cref}{{[figure][3][]3}{[1][5][]5}}
\bibcite{Tseng_2022}{12}
\bibcite{zheng2021regularizingneuralnetworksadversarial}{13}
\gdef \@abspage@last{6}
