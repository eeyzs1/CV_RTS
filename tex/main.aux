\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\citation{Tseng_2022}
\citation{4308316}
\citation{kawaguchi2016deeplearningpoorlocal}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{ruder2017overviewgradientdescentoptimization}
\citation{Tseng_2022}
\citation{Tseng_2022}
\citation{4308316}
\citation{loshchilov2017sgdrstochasticgradientdescent}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\@writefile{brf}{\backcite{Tseng_2022}{{1}{(document)}{Doc-Start}}}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.~Introduction}{1}{section.1}\protected@file@percent }
\@writefile{brf}{\backcite{4308316}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{kawaguchi2016deeplearningpoorlocal}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{ruder2017overviewgradientdescentoptimization}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{Tseng_2022}{{1}{1}{section.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training histories of SGD and PUGD \textbf  {Top row}: Loss and accuracy of both optimizers (400 epochs) \textbf  {Bottom row}: SGD (800 epochs) vs PUGD (400 epochs)}}{1}{figure.1}\protected@file@percent }
\newlabel{fig:SGDvsPUGD}{{1}{1}{Training histories of SGD and PUGD \textbf {Top row}: Loss and accuracy of both optimizers (400 epochs) \textbf {Bottom row}: SGD (800 epochs) vs PUGD (400 epochs)}{figure.1}{}}
\newlabel{fig:SGDvsPUGD@cref}{{[figure][1][]1}{[1][1][]1}}
\@writefile{brf}{\backcite{Tseng_2022}{{1}{1}{figure.1}}}
\@writefile{brf}{\backcite{4308316}{{1}{1}{figure.1}}}
\citation{4308316}
\citation{keskar2017largebatchtrainingdeeplearning}
\citation{ji2020directionalconvergencealignmentdeep}
\citation{neyshabur2015pathsgdpathnormalizedoptimizationdeep}
\citation{jiang2019fantasticgeneralizationmeasures,jastrzebski2020breakevenpointoptimizationtrajectories}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{murray2018revisitingnormalizedgradientdescent}
\citation{zheng2021regularizingneuralnetworksadversarial}
\citation{Tseng_2022}
\@writefile{brf}{\backcite{loshchilov2017sgdrstochasticgradientdescent}{{2}{1}{figure.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.~Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:2}{{2}{2}{\hskip -1em.~Related Work}{section.2}{}}
\newlabel{sec:2@cref}{{[section][2][]2}{[1][2][]2}}
\@writefile{brf}{\backcite{4308316}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{keskar2017largebatchtrainingdeeplearning}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{ji2020directionalconvergencealignmentdeep}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{neyshabur2015pathsgdpathnormalizedoptimizationdeep}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{jastrzebski2020breakevenpointoptimizationtrajectories}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{jiang2019fantasticgeneralizationmeasures}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{murray2018revisitingnormalizedgradientdescent}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{zheng2021regularizingneuralnetworksadversarial}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{Tseng_2022}{{2}{2}{section.2}}}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{kwon2021asamadaptivesharpnessawareminimization}
\citation{Tseng_2022}
\newlabel{eq:1}{{1}{3}{\hskip -1em.~Related Work}{equation.2.1}{}}
\newlabel{eq:1@cref}{{[equation][1][]1}{[1][2][]3}}
\newlabel{eq:2}{{2}{3}{\hskip -1em.~Related Work}{equation.2.2}{}}
\newlabel{eq:2@cref}{{[equation][2][]2}{[1][2][]3}}
\newlabel{eq:3}{{3}{3}{\hskip -1em.~Related Work}{equation.2.3}{}}
\newlabel{eq:3@cref}{{[equation][3][]3}{[1][2][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.~Radius-Timing Scale(RTS)}{3}{section.3}\protected@file@percent }
\newlabel{sec:3}{{3}{3}{\hskip -1em.~Radius-Timing Scale(RTS)}{section.3}{}}
\newlabel{sec:3@cref}{{[section][3][]3}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.~Limitations of PUGD}{3}{subsection.3.1}\protected@file@percent }
\newlabel{subsec:3.1}{{3.1}{3}{\hskip -1em.~Limitations of PUGD}{subsection.3.1}{}}
\newlabel{subsec:3.1@cref}{{[subsection][1][3]3.1}{[1][3][]3}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{3}{3.1}{subsection.3.1}}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{3}{3.1}{subsection.3.1}}}
\@writefile{brf}{\backcite{kwon2021asamadaptivesharpnessawareminimization}{{3}{3.1}{subsection.3.1}}}
\@writefile{brf}{\backcite{Tseng_2022}{{3}{3.1}{subsection.3.1}}}
\citation{li2018visualizinglosslandscapeneural}
\citation{chaudhari2017entropysgdbiasinggradientdescent}
\citation{li2018visualizinglosslandscapeneural}
\citation{jastrzebski2020breakevenpointoptimizationtrajectories}
\citation{maclaurin2015earlystoppingnonparametricvariational}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.~Perturbation Radius Tuning}{4}{subsection.3.2}\protected@file@percent }
\newlabel{subsec:3.2}{{3.2}{4}{\hskip -1em.~Perturbation Radius Tuning}{subsection.3.2}{}}
\newlabel{subsec:3.2@cref}{{[subsection][2][3]3.2}{[1][4][]4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Loss landscape and contour of SGD}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:SGD_LLC}{{2}{4}{Loss landscape and contour of SGD}{figure.2}{}}
\newlabel{fig:SGD_LLC@cref}{{[figure][2][]2}{[1][4][]4}}
\@writefile{brf}{\backcite{li2018visualizinglosslandscapeneural}{{4}{3.2}{figure.2}}}
\@writefile{brf}{\backcite{chaudhari2017entropysgdbiasinggradientdescent}{{4}{3.2}{figure.2}}}
\@writefile{brf}{\backcite{li2018visualizinglosslandscapeneural}{{4}{3.2}{figure.2}}}
\@writefile{brf}{\backcite{jastrzebski2020breakevenpointoptimizationtrajectories}{{4}{3.2}{figure.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.~Timing of application for PUGD}{4}{subsection.3.3}\protected@file@percent }
\newlabel{subsec:3.3}{{3.3}{4}{\hskip -1em.~Timing of application for PUGD}{subsection.3.3}{}}
\newlabel{subsec:3.3@cref}{{[subsection][3][3]3.3}{[1][4][]4}}
\@writefile{brf}{\backcite{maclaurin2015earlystoppingnonparametricvariational}{{4}{3.3}{subsection.3.3}}}
\citation{4308316}
\citation{foret2021sharpnessawareminimizationefficientlyimproving}
\citation{loshchilov2017sgdrstochasticgradientdescent}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.~Scale of gradient Tunning}{5}{subsection.3.4}\protected@file@percent }
\newlabel{subsec:3.4}{{3.4}{5}{\hskip -1em.~Scale of gradient Tunning}{subsection.3.4}{}}
\newlabel{subsec:3.4@cref}{{[subsection][4][3]3.4}{[1][5][]5}}
\@writefile{brf}{\backcite{4308316}{{5}{3.4}{subsection.3.4}}}
\@writefile{brf}{\backcite{foret2021sharpnessawareminimizationefficientlyimproving}{{5}{3.4}{subsection.3.4}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}\hskip -1em.~Four tuning strategies with $\alpha $}{5}{subsection.3.5}\protected@file@percent }
\newlabel{subsec:3.5}{{3.5}{5}{\hskip -1em.~Four tuning strategies with $\alpha $}{subsection.3.5}{}}
\newlabel{subsec:3.5@cref}{{[subsection][5][3]3.5}{[1][5][]5}}
\@writefile{brf}{\backcite{loshchilov2017sgdrstochasticgradientdescent}{{5}{3.5}{subsection.3.5}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of alpha scheduling strategies \textbf  {Top subfigure}: Cosine vs inverse sine annealing \textbf  {Bottom row}: Sine vs inverse cosine annealing The annotations are the decile-sampled values }}{5}{figure.3}\protected@file@percent }
\newlabel{fig:AlphaSch}{{3}{5}{Comparison of alpha scheduling strategies \textbf {Top subfigure}: Cosine vs inverse sine annealing \textbf {Bottom row}: Sine vs inverse cosine annealing The annotations are the decile-sampled values}{figure.3}{}}
\newlabel{fig:AlphaSch@cref}{{[figure][3][]3}{[1][5][]5}}
\citation{gastpar2023fantasticgeneralizationmeasures}
\citation{russakovsky2015imagenet}
\citation{szegedy2015going}
\citation{krizhevsky2012imagenet}
\@writefile{brf}{\backcite{gastpar2023fantasticgeneralizationmeasures}{{6}{3.5}{figure.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.~Experiments}{6}{section.4}\protected@file@percent }
\newlabel{sec:4}{{4}{6}{\hskip -1em.~Experiments}{section.4}{}}
\newlabel{sec:4@cref}{{[section][4][]4}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.~Perturbation radius}{6}{subsection.4.1}\protected@file@percent }
\newlabel{subsec:4.1}{{4.1}{6}{\hskip -1em.~Perturbation radius}{subsection.4.1}{}}
\newlabel{subsec:4.1@cref}{{[subsection][1][4]4.1}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.~Timing of application}{6}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:4.2}{{4.2}{6}{\hskip -1em.~Timing of application}{subsection.4.2}{}}
\newlabel{subsec:4.2@cref}{{[subsection][2][4]4.2}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\hskip -1em.~Radius-Timing Scale(RTS)}{6}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:4.3}{{4.3}{6}{\hskip -1em.~Radius-Timing Scale(RTS)}{subsection.4.3}{}}
\newlabel{subsec:4.3@cref}{{[subsection][3][4]4.3}{[1][6][]6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces MobileNets on ImageNet. BSConv\xspace  -U\xspace  is used for MobileNetV1, and BSConv\xspace  -S\xspace  is used for MobileNetV2/V3. Note that BSConv\xspace  does not introduce additional parameters. }}{6}{table.1}\protected@file@percent }
\newlabel{tab:ImageNet-Mobilenet}{{1}{6}{MobileNets on ImageNet. \DCCKU is used for MobileNetV1, and \DCCKS is used for MobileNetV2/V3. Note that \DCCK does not introduce additional parameters}{table.1}{}}
\newlabel{tab:ImageNet-Mobilenet@cref}{{[table][1][]1}{[1][6][]6}}
\citation{Khosla2011dogs}
\citation{KrauseStarkDengFei-Fei_3DRR2013}
\citation{nilsback2008automated}
\citation{simonyan2014very}
\citation{huang2017densely}
\citation{sandler2018mobilenetv2}
\citation{tan2019efficientnet}
\citation{tan2019mnasnet}
\citation{simonyan2014very}
\citation{huang2017densely}
\citation{chollet2017xception}
\citation{tan2019efficientnet}
\citation{tan2019mnasnet}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces ResNets on ImageNet. For the baseline models, we use ResNet-10/18/26. The BSConv\xspace  variants are ResNet-10/18/34/68/102. }}{7}{figure.4}\protected@file@percent }
\newlabel{fig:resnetsImagenetParamsAndFlops}{{4}{7}{ResNets on ImageNet. For the baseline models, we use ResNet-10/18/26. The \DCCK variants are ResNet-10/18/34/68/102}{figure.4}{}}
\newlabel{fig:resnetsImagenetParamsAndFlops@cref}{{[figure][4][]4}{[1][6][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}\hskip -1em.~Fine-grained Recognition}{7}{subsection.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results of various architectures and their BSConv\xspace  counterparts for the Stanford Dogs dataset. BSConv\xspace  -U\xspace  CNNs have fewer parameters and a smaller computational complexity compared to their baseline models. BSConv\xspace  -S\xspace  CNNs have the same parameter count and computational complexity as their counterparts. * Commonly used implementation based on DSCs. }}{7}{table.2}\protected@file@percent }
\newlabel{tab:dogsOthers}{{2}{7}{Results of various architectures and their \DCCK counterparts for the Stanford Dogs dataset. \DCCKU CNNs have fewer parameters and a smaller computational complexity compared to their baseline models. \DCCKS CNNs have the same parameter count and computational complexity as their counterparts. * Commonly used implementation based on DSCs}{table.2}{}}
\newlabel{tab:dogsOthers@cref}{{[table][2][]2}{[1][7][]7}}
\bibstyle{ieee_fullname}
\bibdata{main}
\bibcite{chaudhari2017entropysgdbiasinggradientdescent}{{1}{}{{}}{{}}}
\bibcite{foret2021sharpnessawareminimizationefficientlyimproving}{{2}{}{{}}{{}}}
\bibcite{gastpar2023fantasticgeneralizationmeasures}{{3}{}{{}}{{}}}
\bibcite{jastrzebski2020breakevenpointoptimizationtrajectories}{{4}{}{{}}{{}}}
\bibcite{ji2020directionalconvergencealignmentdeep}{{5}{}{{}}{{}}}
\bibcite{jiang2019fantasticgeneralizationmeasures}{{6}{}{{}}{{}}}
\bibcite{kawaguchi2016deeplearningpoorlocal}{{7}{}{{}}{{}}}
\bibcite{keskar2017largebatchtrainingdeeplearning}{{8}{}{{}}{{}}}
\bibcite{kwon2021asamadaptivesharpnessawareminimization}{{9}{}{{}}{{}}}
\bibcite{li2018visualizinglosslandscapeneural}{{10}{}{{}}{{}}}
\bibcite{loshchilov2017sgdrstochasticgradientdescent}{{11}{}{{}}{{}}}
\bibcite{maclaurin2015earlystoppingnonparametricvariational}{{12}{}{{}}{{}}}
\bibcite{murray2018revisitingnormalizedgradientdescent}{{13}{}{{}}{{}}}
\bibcite{neyshabur2015pathsgdpathnormalizedoptimizationdeep}{{14}{}{{}}{{}}}
\bibcite{ruder2017overviewgradientdescentoptimization}{{15}{}{{}}{{}}}
\bibcite{4308316}{{16}{}{{}}{{}}}
\bibcite{Tseng_2022}{{17}{}{{}}{{}}}
\bibcite{zheng2021regularizingneuralnetworksadversarial}{{18}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Influence of the orthonormal regularization loss on the accuracy for the BSConv\xspace  -S\xspace  variant of MobileNetV3-large (red solid line) on Stanford Dogs. The baseline MobileNetV3-large model without BSConv\xspace  -S\xspace  is indicated by the black dashed line. }}{8}{figure.5}\protected@file@percent }
\newlabel{fig:dcckV2Alpha}{{5}{8}{Influence of the orthonormal regularization loss on the accuracy for the \DCCKS variant of MobileNetV3-large (red solid line) on Stanford Dogs. The baseline MobileNetV3-large model without \DCCKS is indicated by the black dashed line}{figure.5}{}}
\newlabel{fig:dcckV2Alpha@cref}{{[figure][5][]5}{[1][7][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.~Conclusions}{8}{section.5}\protected@file@percent }
\newlabel{sec:5}{{5}{8}{\hskip -1em.~Conclusions}{section.5}{}}
\newlabel{sec:5@cref}{{[section][5][]5}{[1][8][]8}}
\gdef \@abspage@last{8}
