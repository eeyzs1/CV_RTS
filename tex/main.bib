@misc{kawaguchi2016deeplearningpoorlocal,
      title={Deep Learning without Poor Local Minima}, 
      author={Kenji Kawaguchi},
      year={2016},
      eprint={1605.07110},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1605.07110}, 
}

@misc{ruder2017overviewgradientdescentoptimization,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1609.04747}, 
}

@misc{foret2021sharpnessawareminimizationefficientlyimproving,
      title={Sharpness-Aware Minimization for Efficiently Improving Generalization}, 
      author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
      year={2021},
      eprint={2010.01412},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2010.01412}, 
}

@inproceedings{Tseng_2022,
   title={Perturbed Gradients Updating within Unit Space for Deep Learning},
   url={http://dx.doi.org/10.1109/IJCNN55064.2022.9892245},
   DOI={10.1109/ijcnn55064.2022.9892245},
   booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
   publisher={IEEE},
   author={Tseng, Ching-Hsun and Liu, Hsueh-Cheng and Lee, Shin-Jye and Zeng, Xiaojun},
   year={2022},
   month=jul, pages={01â€“08} }


@misc{loshchilov2017sgdrstochasticgradientdescent,
      title={SGDR: Stochastic Gradient Descent with Warm Restarts}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2017},
      eprint={1608.03983},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1608.03983}, 
}

@ARTICLE{4308316,
  author={Sinha, Naresh K. and Griscik, Michael P.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
  title={A Stochastic Approximation Method}, 
  year={1971},
  volume={SMC-1},
  number={4},
  pages={338-344},
  keywords={Stochastic processes;Approximation methods;Convergence;Approximation algorithms;Statistics;Stochastic systems;Programmable control;Adaptive control;Iterative algorithms;Councils},
  doi={10.1109/TSMC.1971.4308316}}

@misc{keskar2017largebatchtrainingdeeplearning,
      title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}, 
      author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
      year={2017},
      eprint={1609.04836},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1609.04836}, 
}

@misc{ji2020directionalconvergencealignmentdeep,
      title={Directional convergence and alignment in deep learning}, 
      author={Ziwei Ji and Matus Telgarsky},
      year={2020},
      eprint={2006.06657},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.06657}, 
}

@misc{neyshabur2015pathsgdpathnormalizedoptimizationdeep,
      title={Path-SGD: Path-Normalized Optimization in Deep Neural Networks}, 
      author={Behnam Neyshabur and Ruslan Salakhutdinov and Nathan Srebro},
      year={2015},
      eprint={1506.02617},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1506.02617}, 
}

@misc{jiang2019fantasticgeneralizationmeasures,
      title={Fantastic Generalization Measures and Where to Find Them}, 
      author={Yiding Jiang and Behnam Neyshabur and Hossein Mobahi and Dilip Krishnan and Samy Bengio},
      year={2019},
      eprint={1912.02178},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.02178}, 
}

@misc{jastrzebski2020breakevenpointoptimizationtrajectories,
      title={The Break-Even Point on Optimization Trajectories of Deep Neural Networks}, 
      author={Stanislaw Jastrzebski and Maciej Szymczak and Stanislav Fort and Devansh Arpit and Jacek Tabor and Kyunghyun Cho and Krzysztof Geras},
      year={2020},
      eprint={2002.09572},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.09572}, 
}

@misc{murray2018revisitingnormalizedgradientdescent,
      title={Revisiting Normalized Gradient Descent: Fast Evasion of Saddle Points}, 
      author={Ryan Murray and Brian Swenson and Soummya Kar},
      year={2018},
      eprint={1711.05224},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/1711.05224}, 
}

@misc{zheng2021regularizingneuralnetworksadversarial,
      title={Regularizing Neural Networks via Adversarial Model Perturbation}, 
      author={Yaowei Zheng and Richong Zhang and Yongyi Mao},
      year={2021},
      eprint={2010.04925},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2010.04925}, 
}

@misc{kwon2021asamadaptivesharpnessawareminimization,
      title={ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks}, 
      author={Jungmin Kwon and Jeongseop Kim and Hyunseo Park and In Kwon Choi},
      year={2021},
      eprint={2102.11600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2102.11600}, 
}

@misc{li2018visualizinglosslandscapeneural,
      title={Visualizing the Loss Landscape of Neural Nets}, 
      author={Hao Li and Zheng Xu and Gavin Taylor and Christoph Studer and Tom Goldstein},
      year={2018},
      eprint={1712.09913},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1712.09913}, 
}

@misc{chaudhari2017entropysgdbiasinggradientdescent,
      title={Entropy-SGD: Biasing Gradient Descent Into Wide Valleys}, 
      author={Pratik Chaudhari and Anna Choromanska and Stefano Soatto and Yann LeCun and Carlo Baldassi and Christian Borgs and Jennifer Chayes and Levent Sagun and Riccardo Zecchina},
      year={2017},
      eprint={1611.01838},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.01838}, 
}

@misc{gastpar2023fantasticgeneralizationmeasures,
      title={Fantastic Generalization Measures are Nowhere to be Found}, 
      author={Michael Gastpar and Ido Nachum and Jonathan Shafer and Thomas Weinberger},
      year={2023},
      eprint={2309.13658},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.13658}, 
}

@misc{maclaurin2015earlystoppingnonparametricvariational,
      title={Early Stopping is Nonparametric Variational Inference}, 
      author={Dougal Maclaurin and David Duvenaud and Ryan P. Adams},
      year={2015},
      eprint={1504.01344},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1504.01344}, 
}

@article{Tseng_2022_net,
   title={UPANets: Learning from the Universal Pixel Attention Neworks},
   volume={24},
   ISSN={1099-4300},
   url={http://dx.doi.org/10.3390/e24091243},
   DOI={10.3390/e24091243},
   number={9},
   journal={Entropy},
   publisher={MDPI AG},
   author={Tseng, Ching-Hsun and Lee, Shin-Jye and Feng, Jianan and Mao, Shengzhong and Wu, Yu-Ping and Shang, Jia-Yu and Zeng, Xiao-Jun},
   year={2022},
   month=sep, pages={1243} }

@misc{ishida2021needzerotrainingloss,
      title={Do We Need Zero Training Loss After Achieving Zero Training Error?}, 
      author={Takashi Ishida and Ikko Yamane and Tomoya Sakai and Gang Niu and Masashi Sugiyama},
      year={2021},
      eprint={2002.08709},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.08709}, 
}
