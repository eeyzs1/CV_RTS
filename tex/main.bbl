\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{bai2021understandingimprovingearlystopping}
Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu.
\newblock Understanding and improving early stopping for learning with noisy labels, 2021.

\bibitem{chaudhari2017entropysgdbiasinggradientdescent}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys, 2017.

\bibitem{foret2021sharpnessawareminimizationefficientlyimproving}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving generalization, 2021.

\bibitem{gastpar2023fantasticgeneralizationmeasures}
Michael Gastpar, Ido Nachum, Jonathan Shafer, and Thomas Weinberger.
\newblock Fantastic generalization measures are nowhere to be found, 2023.

\bibitem{ishida2021needzerotrainingloss}
Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, and Masashi Sugiyama.
\newblock Do we need zero training loss after achieving zero training error?, 2021.

\bibitem{jastrzebski2020breakevenpointoptimizationtrajectories}
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun Cho, and Krzysztof Geras.
\newblock The break-even point on optimization trajectories of deep neural networks, 2020.

\bibitem{ji2020directionalconvergencealignmentdeep}
Ziwei Ji and Matus Telgarsky.
\newblock Directional convergence and alignment in deep learning, 2020.

\bibitem{jiang2019fantasticgeneralizationmeasures}
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio.
\newblock Fantastic generalization measures and where to find them, 2019.

\bibitem{kawaguchi2016deeplearningpoorlocal}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima, 2016.

\bibitem{keskar2017largebatchtrainingdeeplearning}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and sharp minima, 2017.

\bibitem{kwon2021asamadaptivesharpnessawareminimization}
Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In~Kwon Choi.
\newblock Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks, 2021.

\bibitem{li2018visualizinglosslandscapeneural}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets, 2018.

\bibitem{lin2018focallossdenseobject}
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.
\newblock Focal loss for dense object detection, 2018.

\bibitem{loshchilov2017sgdrstochasticgradientdescent}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts, 2017.

\bibitem{maclaurin2015earlystoppingnonparametricvariational}
Dougal Maclaurin, David Duvenaud, and Ryan~P. Adams.
\newblock Early stopping is nonparametric variational inference, 2015.

\bibitem{murray2018revisitingnormalizedgradientdescent}
Ryan Murray, Brian Swenson, and Soummya Kar.
\newblock Revisiting normalized gradient descent: Fast evasion of saddle points, 2018.

\bibitem{neyshabur2015pathsgdpathnormalizedoptimizationdeep}
Behnam Neyshabur, Ruslan Salakhutdinov, and Nathan Srebro.
\newblock Path-sgd: Path-normalized optimization in deep neural networks, 2015.

\bibitem{ruder2017overviewgradientdescentoptimization}
Sebastian Ruder.
\newblock An overview of gradient descent optimization algorithms, 2017.

\bibitem{4308316}
Naresh~K. Sinha and Michael~P. Griscik.
\newblock A stochastic approximation method.
\newblock {\em IEEE Transactions on Systems, Man, and Cybernetics}, SMC-1(4):338--344, 1971.

\bibitem{szegedy2015rethinkinginceptionarchitecturecomputer}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.
\newblock Rethinking the inception architecture for computer vision, 2015.

\bibitem{Tseng_2022}
Ching-Hsun Tseng, Hsueh-Cheng Liu, Shin-Jye Lee, and Xiaojun Zeng.
\newblock Perturbed gradients updating within unit space for deep learning.
\newblock In {\em 2022 International Joint Conference on Neural Networks (IJCNN)}, page 01–08. IEEE, July 2022.

\bibitem{zheng2021regularizingneuralnetworksadversarial}
Yaowei Zheng, Richong Zhang, and Yongyi Mao.
\newblock Regularizing neural networks via adversarial model perturbation, 2021.

\end{thebibliography}
