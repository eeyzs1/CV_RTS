{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义修改预训练模型层级的5种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接层替换\n",
    "from transformers import BertModel \n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    " \n",
    "# 替换第3个Transformer层 \n",
    "from torch import nn \n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.linear  = nn.Linear(hidden_size, hidden_size*2)\n",
    "        \n",
    "model.encoder.layer[2]  = CustomLayer(model.config.hidden_size)   # 替换第3层 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数冻结+微调\n",
    "# 冻结前6层，只训练后6层 \n",
    "for i, layer in enumerate(model.encoder.layer): \n",
    "    if i < 6:\n",
    "        for param in layer.parameters(): \n",
    "            param.requires_grad  = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入适配器模块\n",
    "import torch \n",
    "import torch.nn  as nn \n",
    "import torch.nn.functional  as F \n",
    " \n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, dim, reduction=4):\n",
    "        super().__init__()\n",
    "        self.down  = nn.Linear(dim, dim//reduction)  # 降维 \n",
    "        self.up  = nn.Linear(dim//reduction, dim)    # 升维 \n",
    "        self.act  = nn.GELU()\n",
    "        self.scale  = nn.Parameter(torch.ones(1))     # 可学习缩放系数 \n",
    " \n",
    "    def forward(self, x):\n",
    "        return x + self.scale  * self.up(self.act(self.down(x))) \n",
    "    \n",
    "from transformers import BertModel \n",
    " \n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    " \n",
    "# 为每层添加适配器 \n",
    "for layer in model.encoder.layer: \n",
    "    layer.adapter  = Adapter(model.config.hidden_size) \n",
    " \n",
    "# 冻结主干参数 \n",
    "for param in model.parameters(): \n",
    "    param.requires_grad  = False \n",
    "for layer in model.encoder.layer: \n",
    "    layer.adapter.requires_grad_(True)   # 仅训练适配器    \n",
    "\n",
    "optimizer = torch.optim.AdamW( \n",
    "    filter(lambda p: p.requires_grad,  model.parameters()),  \n",
    "    lr=1e-3,\n",
    "    weight_decay=0.01 \n",
    ")\n",
    " \n",
    "# 混合精度训练 \n",
    "scaler = torch.cuda.amp.GradScaler() \n",
    "with torch.amp.autocast(device_type='cuda'): \n",
    "    outputs = model(input_ids)\n",
    "    loss = outputs.loss  \n",
    "scaler.scale(loss).backward() \n",
    "scaler.step(optimizer) \n",
    "scaler.update() \n",
    "\n",
    "# 动态适配器路由\n",
    "class DynamicAdapter(nn.Module):\n",
    "    def __init__(self, dim, num_adapters=4):\n",
    "        super().__init__()\n",
    "        self.adapters  = nn.ModuleList([Adapter(dim) for _ in range(num_adapters)])\n",
    "        self.gate  = nn.Linear(dim, num_adapters)\n",
    " \n",
    "    def forward(self, x):\n",
    "        gate_scores = F.softmax(self.gate(x.mean(dim=1)),  dim=-1)  # [B, num_adapters]\n",
    "        return sum(score * adapter(x) for score, adapter in zip(gate_scores, self.adapters)) \n",
    "\n",
    "# 低秩适配器（LoRA） \n",
    "class LoRA(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank=8):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.randn(in_dim,  rank))\n",
    "        self.B = nn.Parameter(torch.zeros(rank,  out_dim))\n",
    "        self.scale  = 1.0 / rank \n",
    " \n",
    "    def forward(self, x):\n",
    "        return x @ (self.A @ self.B) * self.scale  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动态权重混合\n",
    "# 混合原始参数与新参数 \n",
    "original_weight = model.embeddings.word_embeddings.weight  \n",
    "custom_weight = torch.randn_like(original_weight) \n",
    "model.embeddings.word_embeddings.weight  = nn.Parameter( \n",
    "    0.3*original_weight + 0.7*custom_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结构重参数化\n",
    "import torch \n",
    "import torch.nn  as nn \n",
    "import torch.nn.functional  as F \n",
    " \n",
    "class RepBlock(nn.Module):\n",
    "    \"\"\"训练阶段的多分支结构\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        # 分支1：3x3卷积 \n",
    "        self.conv3x3  = nn.Conv2d(in_channels, in_channels, 3, padding=1, bias=False)\n",
    "        # 分支2：1x1卷积 \n",
    "        self.conv1x1  = nn.Conv2d(in_channels, in_channels, 1, bias=False)\n",
    "        # 分支3：Identity \n",
    "        self.bn  = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bn(self.conv3x3(x)  + self.conv1x1(x)  + x)  # 三路求和 \n",
    " \n",
    "    def reparameterize(self):\n",
    "        \"\"\"转换为推理结构\"\"\"\n",
    "        # 融合卷积权重 \n",
    "        fused_weight = self.conv3x3.weight  + F.pad(self.conv1x1.weight,  [1,1,1,1])\n",
    "        # 融合BN参数 \n",
    "        fused_conv = nn.Conv2d(\n",
    "            self.conv3x3.in_channels,  \n",
    "            self.conv3x3.out_channels, \n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=True \n",
    "        )\n",
    "        # 计算融合后的权重与偏置 \n",
    "        fused_conv.weight.data  = fused_weight \n",
    "        fused_conv.bias.data  = self.bn.bias  - (self.bn.weight  * self.bn.running_mean)  / torch.sqrt(self.bn.running_var  + self.bn.eps) \n",
    "        return fused_conv \n",
    " \n",
    "class RepResNet(nn.Module):\n",
    "    \"\"\"完整模型示例\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stage1  = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, stride=2, padding=3),\n",
    "            RepBlock(64),\n",
    "            RepBlock(64)\n",
    "        )\n",
    "        # ... 其他层定义 \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.stage1(x) \n",
    "        \n",
    "    def deploy(self):\n",
    "        \"\"\"转换整个模型为推理模式\"\"\"\n",
    "        for name, module in self.named_children(): \n",
    "            if isinstance(module, RepBlock):\n",
    "                setattr(self, name, module.reparameterize()) \n",
    "        return self \n",
    "    \n",
    "# 训练阶段 \n",
    "model = RepResNet()\n",
    "train_output = model(torch.randn(1,3,224,224)) \n",
    " \n",
    "# 部署阶段 \n",
    "model.eval()   # 必须先设为eval模式 \n",
    "deployed_model = model.deploy() \n",
    "deployed_output = deployed_model(torch.randn(1,3,224,224)) \n",
    " \n",
    "# 验证输出一致性 \n",
    "print(torch.allclose(train_output,  deployed_output, atol=1e-5))  # 应输出True     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
